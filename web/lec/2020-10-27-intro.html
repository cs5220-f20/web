---
title: Intro to Sparse LA
layout: slides
audio: 2020-10-27-intro
---

<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Intro to Sparse LA</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>
</section>

<section>
<h3>World of Linear Algebra</h3>
<ul>
<li>Dense methods (last week)</li>
<li>Sparse direct methods (Thurs)</li>
<li>Iterative methods (today and Thurs)</li>
</ul>
<aside class="notes">
<p>Last week was all about linear algebra. And this week is also all about linear algebra! But this week we are talking about sparse linear algebra, which arguably requires way more know-how. In this deck, we’re going to talk through the lay of the land a bit, and then we’ll talk about stationary iterations and related ideas. In Thursday’s slides, we’ll discuss Krylov subspace methods, like the method of conjugate gradients, and sparse factorization methods.</p>
</aside>
</section>

<section>
<h3>Dense methods</h3>
<pre><code>% Dense (LAPACK)
[L,U] = lu(A);
x = U\(L\b);</code></pre>
<ul>
<li>Direct representation of matrices with simple data structures (no need for indexing data structure)</li>
<li>Mostly <span class="math inline">\(O(n^3)\)</span> factorization algorithms</li>
</ul>
<p>::: Before we sketch the new, though, let’s sketch the old.</p>
<p>Dense linear algebra is all about dealing with matrices where we have to store almost every entry explicitly, and we use simple data structures to do so. The typical algorithms in this space involve factoring matrices into products of simpler matrices. For example, we saw last week that we can write the Gaussian elimination method for solving linear systems as LU factorization, or factorization of the coefficient matrices into a permutation, a lower triangular matrix, and an upper triangular matrix. Then we can solve linear systems via forward and backward substitution (also known as lower and upper triangular system solves). Gaussian elimination is an O(n^3) algorithm, and this is true for most of the algorithms in this area – cubic work and quadratic data, which is pretty good for caches. :::</p>
</section>

<section>
<h3>Software Strategies: Dense Case</h3>
<p>Assuming you want to <em>use</em> (vs develop) dense LA code:</p>
<ul>
<li>Learn enough to identify right algorithm<br />
(e.g. is it symmetric? definite? banded? etc)</li>
<li>Learn high-level organizational ideas</li>
<li>Make sure you have a good BLAS</li>
<li>Call LAPACK/ScaLAPACK!</li>
<li>For <span class="math inline">\(n\)</span> large: wait a while</li>
</ul>
<aside class="notes">
<p>If you are going to use dense linear algebra, you need to know a little bit about common structural properties so you can choose the right algorithm. And it helps to have some understanding of the high-level organization of the dense codes, so that you know about things like blocking, and can do some simple blocked method design by hand when your problem has special structure that merits it. This is the type of thing that I teach in CS 4220 and in CS 6210. But you can get a long way just knowing the names of a few standard factorizations.</p>
<p>Dense linear algebra is relatively easy in this way. There are good algorithms for these types of factorizations in the LAPACK library, which is used by languages like MATLAB, R, Julia, and NumPy/SciPy. LAPACK runs fast because we can make level-3 BLAS routines – such as matrix-matrix multiply – run fast. If we want to scale beyond the performance we can get from LAPACK with maybe some shared-memory parallelism in the BLAS, there are packages like MAGMA, PLASMA, and ScaLAPACK out there. But we will probably mostly use one of a small number of common libraries in this space.</p>
</aside>
</section>

<section>
<h3>Sparse direct methods</h3>
<pre><code>% Sparse direct (UMFPACK + COLAMD)
[L,U,P,Q] = lu(A);
x = Q*(U\(L\(P*b)));</code></pre>
<ul>
<li>Direct representation, keep only the nonzeros</li>
<li>Factorization costs depend on problem structure (1D cheap; 2D reasonable; 3D gets expensive; not easy to give a general rule, and NP hard to order for optimal sparsity)</li>
<li>Robust, but hard to scale to large 3D problems</li>
</ul>
<aside class="notes">
<p>In sparse matrices, we have relatively few nonzeros, and so we use a data structure that only explicitly keeps track of those nonzeros. We’ve already mentioned one such data structure, the compressed sparse row (or compressed sparse column) format. As you might imagine, there are others as well.</p>
<p>I want to for the moment distinguish between this type of explicit sparse representation and what we call “data sparse” matrices (which may have a lot of nonzeros, but can be written in terms of fewer than O(n^2) parameters). This is because explicit sparse representations are a natural setting for sparse direct methods. Sparse direct methods for solving linear systems involve LU factorization, just as in the dense case. But the sparse case is way more complicated. The factorization costs depend on the sparsity pattern of the matrix in a fairly subtle way. We usually talk about this in terms of the “graph” associated with a sparse matrix; we’ll talk about this later. Anyhow, two matrices with the same dimension and the same number of nonzero entries may be very different from the perspective of Gaussian elimination, with one factorization being much more expensive to compute than the other. For problems that come from 2D spatial things, like PDEs on a two-dimensional domain, a sparse direct factorization can often be the best tool to use. For 3D PDE problems, matters get much more complicated.</p>
</aside>
</section>

<section>
<h3>Software Strategies: Sparse Direct Case</h3>
<p>Assuming you want to use (vs develop) sparse LA code</p>
<ul>
<li>Identify right algorithm (mainly Cholesky vs LU)</li>
<li>Get a good solver (often from list)
<ul>
<li>You <em>don’t</em> want to roll your own!</li>
</ul></li>
<li><em>Order your unknowns</em> for sparsity
<ul>
<li>Again, good to use someone else’s software!</li>
</ul></li>
<li>For <span class="math inline">\(n\)</span> large, 3D: get lots of memory and wait</li>
</ul>
<aside class="notes">
<p>So if you want to use a sparse direct solver, what should you do? You almost surely should <em>not</em> roll your own! These are not simple codes to write correctly, let alone for good performance. The right thing to do is to choose the right algorithm – mainly Cholesky or LU – and find someone else’s software that implements the algorithm You also need to order your unknowns in order to get good sparsity; again, I recommend using someone else’s software for this! With a good sparse direct solver and appropriate choice of ordering, sparse direct solvers are often the fastest choice in practice for a lot of 2D problems, or problems that are “nearly 2D” in a sense we’ll make more clear in Thursday’s slides.</p>
</aside>
</section>

<section>
<h3>Sparse iterations</h3>
<pre><code>% Sparse iterative (PCG + incomplete Cholesky)
tol = 1e-6;
maxit = 500;
R = cholinc(A,&#39;0&#39;);
x = pcg(A,b,tol,maxit,R&#39;,R);</code></pre>
<ul>
<li>Only <em>need</em> <span class="math inline">\(y = Ax\)</span> (maybe <span class="math inline">\(y = A^T x\)</span>)</li>
<li>Produce successively better (?) approximations</li>
<li>Good convergence depends on <em>preconditioning</em></li>
<li>Best preconditioners are often hard to parallelize</li>
</ul>
<aside class="notes">
<p>Alas, for 3D PDE discretizations, problems from social networks, and a variety of other problems, we might not be able to afford a sparse factorization method. Or we might not have an explicit representation for the matrix! In this case, it is often natural to turn to iterative methods.</p>
<p>Iterative methods turn everything we’ve said about direct factorization on its head. It is not insane to go to a textbook and implement most of the standard sparse iterations out there. They often involve only a few lines of code, and while the analysis is subtle, the implementation mostly isn’t (with a few exceptions). Even if you use a standard implementation, you may find that you want to use an application-specific <em>preconditioner</em>, a transformation to the problem that converges acceleration of the iterative method. The choice of preconditioners is still as much art as science for most problems, and depends a lot on details of the application. Moreover, the convergence of these methods is often all over the place; they sometimes need a certain amount of hand-holding to get to the right solution. Worse, a preconditioner that is great for convergence may often be lousy for parallelism.</p>
<p>The good news about all of this is that there is a lot of room for creativity and there is still a lot of application-specific “low hanging fruit” when it comes to preconditioning. The bad news, if you just want to solve the problem, is that these methods often require a certain amount of hand-holding, particularly for problems that are “difficult” in some way.</p>
</aside>
</section>

<section>
<h3>Linear Algebra Software: the Wider World</h3>
<ul>
<li>Dense: LAPACK, ScaLAPACK, PLAPACK, PLASMA, MAGMA</li>
<li><a href="https://portal.nersc.gov/project/sparse/superlu/SparseDirectSurvey.pdf">Sparse direct</a>: UMFPACK, TAUCS, SuperLU, MUMPS, Pardiso, SPOOLES, ...</li>
<li>Sparse iterative: too many!
<ul>
<li><a href="https://www.mcs.anl.gov/petsc/">PETSc</a> (Argonne, object-oriented C)</li>
<li><a href="https://trilinos.github.io/">Trilinos</a> (Sandia, C++)</li>
</ul></li>
</ul>
<aside class="notes">
<p>If you are going to solve dense linear systems of equations, you’ll probably use LAPACK or some successor. Maybe you’ll use it via some other interface, like the Armadillo package in C++ or the solver interfaces in Julia and R and SciPy and MATLAB; but it’s LAPACK under the hood.</p>
<p>If you are going to do sparse direct solves, there’s a wider playing field. But it’s still a not-too-big set, mostly written by a handful of experts in the field. I’ve hyperlinked a survey article by Xiaoye Li, updated as of June 2020 – Xiaoye is the author of the SuperLU sparse solver (and is also my academic big sister).</p>
<p>For sparse iterative methods, there are too many to easily enumerate, and they all seem to have a lot of knobs. If you’re going to do things with these types of methods, I recommend getting to know either PETSc or Trilinos, two of the big parallel numerical framework libraries out there. They include implementations of many of these methods and preconditioners, and if you do something clever in this space, you probably want to use one of these frameworks to be able to compare with lots of competitors, and also to make sure that other people can benefity from your code.</p>
</aside>
</section>

<section>
<h3>Sparse Iterative Software</h3>
<p>Assuming you want to use (vs develop) sparse LA software...</p>
<ul>
<li>Identify a good algorithm (GMRES? CG?)</li>
<li>Pick a good preconditioner
<ul>
<li>Often helps to know the application</li>
<li>... <em>and</em> to know how the solvers work!</li>
</ul></li>
<li>Play with parameters, preconditioner variants, etc...</li>
<li>Swear until you get acceptable convergence?</li>
<li>Repeat for the next variation on the problem</li>
</ul>
<aside class="notes">
<p>What strategies should you use as a user of sparse iterative solvers? You probably don’t want to develop a new basic iteration: use a standard one like GMRES or CG. On the other hand, you will want to spend some time picking a good preconditioner. There are lots of default preconditioners that you can try, but if your problem is hard, you may need to go beyond the defaults. Or, at the very least, you’ll need to play with the parameters. Swearing may be involved. And passive voice may be used. Because I, of course, have never wanted to swear at an iteration that refused to converge.</p>
<p>(One of my grad school flatmates used to have a verbal running commentary when he was watching his iterations converge, or fail to do so. It was like listening to a sports commentator. It helped that he was doing earthquake simulations, so his “aah, oh no!” could either mean a convergence failure or a successful simulation that showed a catastrophic collapse of some structure. Fun times.)</p>
</aside>
</section>

<section>
<h3>Stacking Solvers</h3>
<p>(Typical) example from a bone modeling package:</p>
<ul>
<li>Outer load stepping loop</li>
<li>Newton method corrector for each load step</li>
<li>Preconditioned CG for linear system</li>
<li>Multigrid preconditioner</li>
<li>Sparse direct solver for coarse-grid solve (UMFPACK)</li>
<li>LAPACK/BLAS under that</li>
</ul>
<aside class="notes">
<p>I’ve been talking about these different types of solvers so far as if any given problem only requires one or the other. Of course, the picture is not that simple.</p>
<p>Let me tell you about a nonlinear finite element code that I wrote some years ago as a consulting gig. This was a code that simulated a hysteretic material, and so it mattered how it was loaded. So the outer loop was something that incrementally applied ever greater loads, usually up to a failure event. At each load step, we ran a Newton iteration to solve the force balance equation; and at each Newton step, we had a linear system that we solved with an iterative method (preconditioned conjugate gradients). To get reasonable convergence, we used a geometric multigrid precondiitioner that I wrote. For the “top level” coarse subproblem, I used a sparse direct solver, UMFPACK. And UMFPACK gets a lot of its performance by organizing as much as it can around dense sub-problems treated with LAPACK and level 3 BLAS subroutines. So in one code, you have the whole list of solvers that we’ve talked about so far.</p>
<p>By the way, I wrote the first three levels of the solver stack – the load stepper, the Newton iteration, and the CG solver – in the Lua scripting language. The multigrid code was in C++, and UMFPACK in C.</p>
</aside>
</section>

<section>
<h3>Overview of the week</h3>
<ul>
<li>Up next: Stationary iterations</li>
<li>Thurs: Krylov and sparse direct</li>
</ul>
<p>Reading: <a href="https://www.netlib.org/templates/templates.pdf">Templates book</a></p>
<aside class="notes">
<p>With that as setup, the next step is going to be stationary iterations. We’ll talk about those in another slide deck for Tuesday. And on Thursday, we’ll post slide decks for so-called Krylob subspace methods and sparse direct solvers.</p>
<p>This is a lot of stuff to cover in a single week. We could – and do! – spend an entire graduate course just on sparse matrix computations. But it’s useful for a lay of the land. If you want a bit more detail, I really like the “templates” book for this class – it’s pragmatic and focused on the basic properties and high-level implementation of the methods, rather than spending a lot of time on convergence analysis.</p>
</aside>
</section>

