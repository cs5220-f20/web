---
title: Particle systems
layout: slides
audio: 2020-09-29-particle
---

<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Particle Systems</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
  </aside>
</section>


<section>
  <h3>Particle systems</h3>

  <ul>
    <li>Billiards, electrons, galaxies, ...</li>
    <li>Ants, cars, agents, ...?</li>
  </ul>
</section>


<section>
  <h3>Particle simulation</h3>
  <p>Particles move via Newton (<span class="math inline">\(F = ma\)</span>), with</p>
  <ul>
    <li>External forces: ambient gravity, currents, etc.</li>
    <li>Local forces: collisions, Van der Waals (<span class="math inline">\(1/r^6\)</span>), etc.</li>
    <li>Far-field forces: gravity and electrostatics (<span class="math inline">\(1/r^2\)</span>), etc.
      <ul>
        <li>Simple approximations often apply (Saint-Venant)</li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>A forced example</h3>
  <p>Example force: <span class="math display">\[f_i = \sum_j Gm_i m_j
      \frac{(x_j-x_i)}{r_{ij}^3}
      \left(1 - \left(\frac{a}{r_{ij}}\right)^{4} \right), \qquad
      r_{ij} = \|x_i-x_j\|\]</span></p>
  <ul>
    <li>Long-range attractive force (<span class="math inline">\(r^{-2}\)</span>)</li>
    <li>Short-range repulsive force (<span class="math inline">\(r^{-6}\)</span>)</li>
    <li>Go from attraction to repulsion at radius <span class="math inline">\(a\)</span></li>
  </ul>
</section>


<section>
  <h3>A simple serial simulation</h3>
  <p>In <span class="smallcaps">Matlab</span>, we can write</p>
  <pre><code>npts = 100;
t = linspace(0, tfinal, npts);
[tout, xyv] = ode113(@fnbody, t, [x; v], [], m, g);
xout = xyv(:,1:length(x))&#39;;</code></pre>
    <p>... but I can’t call ode113 in C in parallel (or can I?)</p>
  </section>


<section>
  <h3>A simple serial simulation</h3>
  <p>Maybe a fixed step leapfrog will do?</p>
  <pre><code>npts = 100;
steps_per_pt = 10;
dt = tfinal/(steps_per_pt*(npts-1));
xout = zeros(2*n, npts);
xout(:,1) = x;
for i = 1:npts-1
  for ii = 1:steps_per_pt
    x = x + v*dt;
    a = fnbody(x, m, g);
    v = v + a*dt;
  end
  xout(:,i+1) = x;
end</code></pre>
</section>


<section>
  <h3>Plotting particles</h3>
  <figure>
    <img data-src="figs/sph-plot.png" alt="image" style="width:90.0%"
    /><figcaption>Smooth Particle Hydrodynamics (SPH) simulation</figcaption>
  </figure>
</section>


<section>
  <h3>Pondering particles</h3>
  <ul>
    <li>Where do particles <q>live</q> (distributed mem)?
      <ul>
        <li>Decompose in space? By particle number?</li>
        <li>What about clumping?</li>
    </ul></li>
    <li>How are long-range force computations organized?</li>
    <li>How are short-range force computations organized?</li>
    <li>How is force computation load balanced?</li>
    <li>What are the boundary conditions?</li>
    <li>How are potential singularities handled?</li>
    <li>What integrator is used? What step control?</li>
  </ul>
</section>


<section>
  <h3>External forces</h3>
  <p>Simplest case: no particle interactions.</p>
  <ul>
    <li>Embarrassingly parallel (like Monte Carlo)!</li>
    <li>Could just split particles evenly across processors</li>
    <li>Is it that easy?
      <ul>
        <li>Maybe some trajectories need short time steps?</li>
        <li>Even with MC, load balance may not be trivial!</li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>Local forces</h3>
  <img src="{{ "lec/figs/local-forces-dd1.svg" | relative_url }}"
       alt="Domain decomposition for local forces"
       style="background-color:white"
       width="30%"/>  
  <ul>
    <li>Simplest all-pairs check is <span class="math inline">\(O(n^2)\)</span> (expensive)</li>
    <li>Or only check close pairs (via binning, quadtrees?)</li>
    <li>Communication required for pairs checked</li>
    <li>Usual model: domain decomposition</li>
  </ul>
</section>


<section>
  <h3>Local forces: Communication</h3>
  <p>Minimize communication:</p>
  <ul>
    <li>Send particles that might affect a neighbor “soon”</li>
    <li>Trade extra computation against communication</li>
    <li>Want low surface area-to-volume ratios on domains</li>
  </ul>
</section>


<section>
  <h3>Local forces: Load balance</h3>
  <ul>
    <li>Are particles evenly distributed?</li>
    <li>Do particles remain evenly distributed?</li>
    <li>Can divide space unevenly (e.g. quadtree/octtree)</li>
  </ul>
</section>


<section>
  <h3>Far-field forces</h3>
  <ul>
    <li>Every particle affects every other particle</li>
    <li>All-to-all communication required
      <ul>
        <li>Overlap communication with computation</li>
        <li>Poor memory scaling if everyone keeps everything!</li>
    </ul></li>
    <li>Idea: pass particles in a round-robin manner</li>
  </ul>
</section>


<section>
  <h3>Passing particles (far-field forces)</h3>
  <pre>copy local particles to current buf
for phase = 1:p
  send current buf to rank+1 (mod p)
  recv next buf from rank-1 (mod p)
  interact local particles with current buf
  swap current buf with next buf
end</pre>
</section>


<section>
  <h3>Passing particles (far-field forces)</h3>
  <p>Suppose <span class="math inline">\(n = N/p\)</span> particles in buffer. At each phase <span class="math display">\[\begin{aligned}
      t_{\mathrm{comm}} &amp; \approx \alpha + \beta n \\
      t_{\mathrm{comp}} &amp; \approx \gamma n^2
      \end{aligned}\]</span> So we can mask communication with computation if <span class="math display">\[n \geq
      \frac{1}{2\gamma} \left( \beta + \sqrt{\beta^2 + 4 \alpha \gamma} \right)
      &gt; \frac{\beta}{\gamma}\]</span></p>
</section>


<section>
  <h3>Passing particles (far-field forces)</h3>
  <p>More efficient serial code<br />
    <span class="math inline">\(\implies\)</span> larger <span class="math inline">\(n\)</span> needed to mask communication!<br />
    <span class="math inline">\(\implies\)</span> worse speed-up as <span class="math inline">\(p\)</span> gets larger (fixed <span class="math inline">\(N\)</span>)<br />
    but scaled speed-up (<span class="math inline">\(n\)</span> fixed) remains unchanged.</p>
  <p>This analysis neglects overhead term in LogP.</p>
</section>


<section>
  <h3>Far-field forces: particle-mesh methods</h3>
  <p>Consider <span class="math inline">\(r^{-2}\)</span> electrostatic potential interaction</p>
  <ul>
    <li>Enough charges looks like a continuum!</li>
    <li>Poisson maps charge distribution to potential</li>
    <li>Fast Poisson for regular grids (FFT, multigrid)</li>
    <li>Approx depends on mesh and particle density</li>
    <li>Can clean up leading part of approximation error</li>
  </ul>
</section>


<section>
  <h3>Far-field forces: particle-mesh</h3>
  <ul>
    <li>Map particles to mesh points (multiple strategies)</li>
    <li>Solve potential PDE on mesh</li>
    <li>Interpolate potential to particles</li>
    <li>Add correction term – acts like local force</li>
  </ul>
</section>


<section>
  <h3>Far-field forces: tree methods</h3>
  <ul>
    <li>Distance simplifies things
      <ul>
        <li>Andromeda looks like a point mass from here?</li>
    </ul></li>
    <li>Build a tree, approx descendants at each node</li>
    <li>Variants: Barnes-Hut, FMM, Anderson’s method</li>
    <li>More on this later in the semester</li>
  </ul>
</section>


<section>
  <h3>Summary of particle example</h3>
  <ul>
    <li>Model: Continuous motion of particles
      <ul>
        <li>Could be electrons, cars, whatever...</li>
    </ul></li>
    <li>Step through discretized time</li>
  </ul>
</section>


<section>
  <h3>Summary of particle example</h3>
  <ul>
    <li>Local interactions
      <ul>
        <li>Relatively cheap</li>
        <li>Load balance a pain</li>
    </ul></li>
    <li>All-pairs interactions
      <ul>
        <li>Obvious algorithm is expensive (<span class="math inline">\(O(n^2)\)</span>)</li>
        <li>Particle-mesh and tree-based algorithms help</li>
    </ul></li>
  </ul>
  <p>An important special case of lumped/ODE models.</p>
</section>

