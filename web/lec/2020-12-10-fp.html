---
title: Impact of floating point
layout: slides
audio: 2020-12-10-fp
---

<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Impact of floating point</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>
</section>

<section>
<h3>Nothing New Under the Sun</h3>
<p><a href="https://www.microsoft.com/en-us/research/publication/pushing-the-limits-of-narrow-precision-inferencing-at-cloud-scale-with-microsoft-floating-point/">“Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point”</a><br />
(at NeurIPS 2020 this week)</p>
</section>

<section>
<h3>Von Neumann and Goldstine</h3>
<p><img data-src="figs/von-neumann.jpg" style="height:4cm" /> <img data-src="figs/goldstine.jpg" style="height:4cm" /></p>
<p>“Numerical Inverting of Matrices of High Order” (1947)</p>
<blockquote>
<p>... matrices of the orders 15, 50, 150 can usually be inverted with a (relative) precision of 8, 10, 12 decimal digits less, respectively, than the number of digits carried throughout.</p>
</blockquote>
</section>

<section>
<h3>Turing</h3>
<p><img data-src="figs/turing.jpg" style="height:5cm" /></p>
<p>“Rounding-Off Errors in Matrix Processes” (1948)</p>
<p>Carrying <span class="math inline">\(d\)</span> digits is equivalent to changing input data in the <span class="math inline">\(d\)</span>th place (backward error analysis).</p>
</section>

<section>
<h3>Wilkinson</h3>
<p><img data-src="figs/wilkinson.jpg" style="height:3cm" /></p>
<p>“Error Analysis of Direct Methods of Matrix Inv” (1961)</p>
<blockquote>
<p>For his research in numerical analysis to facilitiate the use of the high-speed digital computer, having received special recognition for his work in computations in linear algebra and “backward” error analysis. — 1970 Turing Award citation</p>
</blockquote>
</section>

<section>
<h3>Kahan</h3>
<p><img data-src="figs/kahan.jpg" style="height:3cm" /></p>
<p>IEEE-754/854 (1985, revised 2008)</p>
<blockquote>
<p>For his fundamental contributions to numerical analysis. One of the foremost experts on floating-point computations. Kahan has dedicated himself to “making the world safe for numerical computations.” — 1989 Turing Award citation</p>
</blockquote>
</section>

<section>
<h3>IEEE floating point reminder</h3>
<p>Normalized numbers: <span class="math display">\[(-1)^s \times (1.b_1 b_2 \ldots b_p)_2 \times 2^e\]</span> Have 32-bit single, 64-bit double numbers consisting of</p>
<ul>
<li>Sign <span class="math inline">\(s\)</span></li>
<li>Precision <span class="math inline">\(p\)</span> (<span class="math inline">\(p = 23\)</span> or <span class="math inline">\(52\)</span>)</li>
<li>Exponent <span class="math inline">\(e\)</span> (<span class="math inline">\(-126 \leq e \leq 126\)</span> or <span class="math inline">\(-1022 \leq e \leq 1023\)</span>)</li>
</ul>
<p>Questions:</p>
<ul>
<li>What if we can’t represent an exact result?</li>
<li>What about <span class="math inline">\(2^{e_{\max}+1} \leq x &lt; \infty\)</span> or <span class="math inline">\(0 \leq x &lt; 2^{e_{\min}}\)</span>?</li>
<li>What if we compute <span class="math inline">\(1/0\)</span>?</li>
<li>What if we compute <span class="math inline">\(\sqrt{-1}\)</span>?</li>
</ul>
</section>

<section>
<h3>Rounding</h3>
<p>Basic ops (<span class="math inline">\(+, -, \times, /, \sqrt{}\)</span>), require <em>correct rounding</em></p>
<ul>
<li>As if computed to infinite precision, then rounded.
<ul>
<li>Don’t actually need infinite precision for this!</li>
</ul></li>
<li>Different rounding rules possible:
<ul>
<li>Round to nearest even (default)</li>
<li>Round up, down, toward 0 – error bounds and intervals</li>
</ul></li>
<li>If rounded result <span class="math inline">\(\neq\)</span> exact result, have <em>inexact exception</em>
<ul>
<li>Which most people seem not to know about...</li>
<li>... and which most of us who do usually ignore</li>
</ul></li>
<li>754-2008 <em>recommends</em> (does not require) correct rounding for a few transcendentals as well (sine, cosine, etc).</li>
</ul>
</section>

<section>
<h3>Denormalization and underflow</h3>
<p>Denormalized numbers: <span class="math display">\[(-1)^s \times (0.b_1 b_2 \ldots b_p)_2 \times 2^{e_{\min}}\]</span></p>
<ul>
<li>Evenly fill in space between <span class="math inline">\(\pm 2^{e_{\min}}\)</span></li>
<li>Gradually lose bits of precision as we approach zero</li>
<li>Denormalization results in an <em>underflow exception</em>
<ul>
<li>Except when an exact zero is generated</li>
</ul></li>
</ul>
</section>

<section>
<h3>Infinity and NaN</h3>
<p>Other things can happen:</p>
<ul>
<li><span class="math inline">\(2^{e_{\max}} + 2^{e_{\max}}\)</span> generates <span class="math inline">\(\infty\)</span> (<em>overflow exception</em>)</li>
<li><span class="math inline">\(1/0\)</span> generates <span class="math inline">\(\infty\)</span> (<em>divide by zero exception</em>)
<ul>
<li>... should really be called “exact infinity” exception</li>
</ul></li>
<li><span class="math inline">\(\sqrt{-1}\)</span> generates Not-a-Number (<em>invalid exception</em>)</li>
</ul>
<p>But every basic operation produces <em>something</em> well defined.</p>
</section>

<section>
<h3>Basic rounding model</h3>
<p>Model of roundoff in a basic op: <span class="math display">\[\fl(a \odot b) = (a \odot b)(1 + \delta), \quad
|\delta| \leq \macheps.\]</span></p>
<ul>
<li>This model is <em>not</em> complete
<ul>
<li>Optimistic: misses overflow, underflow, divide by zero</li>
<li>Also too pessimistic – some things are done exactly!</li>
<li>Example: <span class="math inline">\(2x\)</span> exact, as is <span class="math inline">\(x+y\)</span> if <span class="math inline">\(x/2 \leq y \leq 2x\)</span></li>
</ul></li>
<li>But useful as a basis for backward error analysis</li>
</ul>
</section>

<section>
<h3>Example: Horner’s rule</h3>
<p>Evaluate <span class="math inline">\(p(x) = \sum_{k=0}^n c_k x^k\)</span>:</p>
<pre><code>p = c(n)
for k = n-1 downto 0
  p = x*p + c(k)</code></pre>
<p>Can show backward error result: <span class="math display">\[\fl(p) = \sum_{k=0}^n \hat{c}_k x^k\]</span> where <span class="math inline">\(|\hat{c}_k-c_k| \leq (n+1) \macheps |c_k|\)</span>.</p>
<p>Backward error + sensitivity gives forward error. Can even compute running error estimates!</p>
</section>

<section>
<h3>Hooray for the modern era!</h3>
<ul>
<li>Almost everyone implements IEEE 754 (at least 1985)
<ul>
<li>Old Cray arithmetic is essentially extinct</li>
</ul></li>
<li>We teach backward error analysis in basic classes</li>
<li>Good libraries for linear algebra, elementary functions</li>
</ul>
</section>

<section>
<h3>Back to the future?</h3>
<ul>
<li>Almost everyone implements IEEE 754 (at least 1985)
<ul>
<li>Old Cray arithmetic is essentially extinct</li>
<li>But GPUs may lack gradual underflow</li>
<li>And it’s impossible to write portable exception handlers</li>
<li>And even with C99, exception flags may be inaccessible</li>
<li>And some features might be slow</li>
<li>And the compiler might not do what you expected</li>
</ul></li>
<li>We teach backward error analysis in basic classes
<ul>
<li>... which are often no longer required!</li>
<li>And anyhow, backward error analysis isn’t everything.</li>
</ul></li>
<li>Good libraries for linear algebra, elementary functions
<ul>
<li>But people will still roll their own.</li>
</ul></li>
</ul>
</section>

<section>
<h3>Arithmetic speed</h3>
<p>Single precision is faster than double precision</p>
<ul>
<li>Actual arithmetic cost may be comparable (on CPU)</li>
<li>But GPUs generally prefer single</li>
<li>And AVX instructions do more per cycle with single</li>
<li>And memory bandwidth is lower</li>
</ul>
<p>NB: There is a half-precision type (use for storage only!)</p>
</section>

<section>
<h3>Mixed-precision arithmetic</h3>
<p>Idea: use double precision only where needed</p>
<ul>
<li>Example: iterative refinement and relatives</li>
<li>Or use double-precision arithmetic between single-precision representations (may be a good idea regardless)</li>
</ul>
</section>

<section>
<h3>Example: Mixed-precision iterative refinement</h3>
<p>̄ ̄ Factor <span class="math inline">\(A = LU\)</span> <span class="math inline">\(O(n^3)\)</span> single-precision work<br />
Solve <span class="math inline">\(x = U^{-1} (L^{-1} b)\)</span> <span class="math inline">\(O(n^2)\)</span> single-precision work<br />
<span class="math inline">\(r = b-Ax\)</span> <span class="math inline">\(O(n^2)\)</span> double-precision work<br />
While <span class="math inline">\(\|r\|\)</span> too large<br />
<span class="math inline">\(d = U^{-1} (L^{-1} r)\)</span> <span class="math inline">\(O(n^2)\)</span> single-precision work<br />
<span class="math inline">\(x = x+d\)</span> <span class="math inline">\(O(n)\)</span> single-precision work<br />
<span class="math inline">\(r = b-Ax\)</span> <span class="math inline">\(O(n^2)\)</span> double-precision work</p>
</section>

<section>
<h3>Example: Helpful extra precision</h3>
<pre><code>/*
 * Assuming all coordinates are in [1,2), check on which
 * side of the line through A and B is the point C.
 */
int check_side(float ax, float ay, float bx, float by, 
               float cx, float cy)
{
    double abx = bx-ax, aby = by-ay;
    double acx = cx-ax, acy = cy-ay;
    double det = acx*aby-abx*aby;
    if (det == 0) return  0;
    if (det &lt;  0) return -1;
    if (det &gt;  0) return  1;
}</code></pre>
<p>This is not robust if the inputs are double precision!</p>
</section>

<section>
<h3>Single or double?</h3>
<p>What to use for:</p>
<ul>
<li>Large data sets? (single for performance, if possible)</li>
<li>Local calculations? (double by default, except GPU?)</li>
<li>Physically measured inputs? (probably single)</li>
<li>Nodal coordinates? (probably single)</li>
<li>Stiffness matrices? (maybe single, maybe double)</li>
<li>Residual computations? (probably double)</li>
<li>Checking geometric predicates? (double or more)</li>
</ul>
</section>

<section>
<h3>Simulating extra precision</h3>
<p>What if we want higher precision than is fast?</p>
<ul>
<li>Double precision on a GPU?</li>
<li>Quad precision on a CPU?</li>
</ul>
<p>Can simulate extra precision. Example:</p>
<pre><code>  if abs(a) &lt; abs(b) { swap(&amp;a, &amp;b); }
  double s1 = a+b;         /* May suffer roundoff */
  double s2 = (a-s1) + b;  /* No roundoff! */</code></pre>
<p>Idea applies more broadly (Bailey, Bohlender, Dekker, Demmel, Hida, Kahan, Li, Linnainmaa, Priest, Shewchuk, ...)</p>
<ul>
<li>Used in fast extra-precision packages</li>
<li>And in robust geometric predicate code</li>
<li>And in XBLAS</li>
</ul>
</section>

<section>
<h3>Exceptional arithmetic speed</h3>
<p>Time to sum 1000 doubles on my laptop:</p>
<ul>
<li>Initialized to 1: 1.3 microseconds</li>
<li>Initialized to inf/nan: 1.3 microseconds</li>
<li>Initialized to <span class="math inline">\(10^{-312}\)</span>: 67 microseconds</li>
</ul>
<p><span class="math inline">\(50 \times\)</span> performance penalty for gradual underflow!</p>
<p>Why worry? Some GPUs don’t support gradual underflow at all!<br />
One reason:</p>
<pre><code>if (x != y)
    z = x/(x-y);</code></pre>
<p>Also limits range of simulated extra precision.</p>
</section>

<section>
<h3>Exceptional algorithms, take 2</h3>
<p>A general idea (works outside numerics, too):</p>
<ul>
<li>Try something fast but risky</li>
<li>If something breaks, retry more carefully</li>
</ul>
<p>If risky usually works and doesn’t cost too much extra, this improves performance.</p>
<p>(See Demmel and Li, and also Hull, Farfrieve, and Tang.)</p>
</section>

<section>
<h3>Parallel problems</h3>
<p>What goes wrong with floating point in parallel (or just high performance) environments?</p>
</section>

<section>
<h3>Problem 0: Mis-attributed Blame</h3>
<blockquote>
<p>To blame is human. To fix is to engineer.<br />
— Unknown</p>
</blockquote>
<p>Three variants:</p>
<ul>
<li>“I probably don’t have to worry about floating point error.”</li>
<li>“This is probably due to floating point error.”</li>
<li>“Floating point error makes this untrustworthy.”</li>
</ul>
</section>

<section>
<h3>Problem 1: Repeatability</h3>
<p>Floating point addition is <em>not</em> associative: <span class="math display">\[\fl(a + \fl(b + c)) \neq \fl(\fl(a + \b) + c)\]</span></p>
<p>So answers depends on the inputs, but also</p>
<ul>
<li>How blocking is done in multiply or other kernels</li>
<li>Maybe compiler optimizations</li>
<li>Order in which reductions are computed</li>
<li>Order in which critical sections are reached</li>
</ul>
<p>Worst case: with nontrivial probability we get an answer too bad to be useful, not bad enough for the program to barf — and garbage comes out.</p>
</section>

<section>
<h3>Problem 1: Repeatability</h3>
<p>What can we do?</p>
<ul>
<li>Apply error analysis agnostic to ordering</li>
<li>Write a slower version with specific ordering for debugging</li>
<li>Soon: Call the <em>reproducible BLAS</em></li>
</ul>
<p>Note: new two_sum operation under discussion in IEEE 754 committee should make fast reproducibility (and double-double) easier.</p>
</section>

<section>
<h3>Problem 2: Heterogeneity</h3>
<ul>
<li>Local arithmetic faster than communication</li>
<li>So be redundant about some computation</li>
<li>What if the redundant computations are on different HW?
<ul>
<li>Different nodes in the cloud?</li>
<li>GPU and CPU?</li>
</ul></li>
<li>Problem: different exception handling on different nodes</li>
<li>Problem: different branches due to different rounding</li>
</ul>
</section>

<section>
<h3>Problem 2: Heterogeneity</h3>
<p>What can we do?</p>
<ul>
<li>Avoid FP-dependent branches</li>
<li>Communicate FP results affecting branches</li>
<li>Use reproducible kernels</li>
</ul>
</section>

<section>
<h3>Recap</h3>
<p>So why care about the vagaries of floating point?</p>
<ul>
<li>Might actually care about error analysis</li>
<li>Or using single precision for speed</li>
<li>Or maybe just reproducibility</li>
<li>Or avoiding crashes from inconsistent decisions!</li>
</ul>
<p>Start with “What Every Computer Scientist Should Know About Floating Point Arithmetic” (David Goldberg, with an addendum by Doug Priest). It’s in the back of Patterson-Hennessey.</p>
</section>

