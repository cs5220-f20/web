---
title: Performance basics
layout: slides
audio: 2020-09-03-perf-basics
---


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Performance Basics</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
    Welcome to CS 5220, Applications of Parallel Computers, the Fall
    2020 edition.  I am your host, David Bindel.

    In this second technical slide deck of the semester, we will
    discuss some fundamental performance concepts.  There are a lot of
    misconceptions about computational performance, so part of the
    goal here is to convince you there is more than meets the eye when
    it comes to understanding code speed.  This is a longer slide deck
    than the last, so I'm going to use the vertical slide feature in
    reveal.js.  If you are navigating by hand rather than using
    autoplay, you can use up and down to move between slides in a
    section, and left and right to navigate between sections.
  </aside>
</section>


<section> <!-- Soap Box -->


  <section>
    <h2>Starting on the Soap Box</h2>
  </section>


  <section>
    <h3>Starting on the Soap Box</h3>

    <p>The goal is right enough, fast enough — not flop/s.</p>
  </section>


  <section>
    <h3>Starting on the Soap Box</h3>

    <p>Performance is not all that matters.</p>
    <ul>
      <li>Portability, readability, debuggability matter too!</li>
      <li>Want to make intelligent trade-offs.</li>
    </ul>
  </section>


  <section>
    <h3>Starting on the Soap Box</h3>

    <p>The road to good performance starts with a single core.</p>
    <ul>
      <li>Even single-core performance is hard.</li>
      <li>Helps to build on well-engineered libraries.</li>
    </ul>
  </section>


  <section>
    <h3>Starting on the Soap Box</h3>
    <p>Parallel efficiency is hard!</p>
    <ul>
      <li><span class="math inline">\(p\)</span> processors <span class="math inline">\(\neq\)</span> speedup of <span class="math inline">\(p\)</span></li>
      <li>Different algorithms parallelize differently.</li>
      <li>Speed vs untuned serial code is cheating!</li>
    </ul>
  </section>

</section>


<section> <!-- Peak performance -->

  <section>
    <h2>Peak performance</h2>
  </section>


  <section>
    <h3>Whence Rmax?</h3>

    <p>
      Top 500 benchmark reports:
    </p>
    <ul>
      <li>Rmax: Linpack flop/s</li>
      <li>Rpeak: Theoretical peak flop/s</li>
    </ul>
    <p>Measure the first; how do we know the second?</p>
  </section>


  <section>
    <h3>What is a float?</h3>

    <p>Start with what is floating point:</p>
    <ul>
      <li><p>(Binary) scientific notation</p></li>
      <li><p>Extras: inf, NaN, de-normalized numbers</p></li>
      <li><p>IEEE 754 standard: encodings, arithmetic rules</p></li>
    </ul>
  </section>


  <section>
    <h3>What is a float?</h3>

    <p>Common floating point formats</p>
    <ul>
      <li>64-bit double precision (DP)</li>
      <li>32-bit single precision (SP)</li>
    </ul>
    <p>Linpack results are double precision</p>
  </section>


  <section>
    <h3>What is a float?</h3>

    <p>Less common</p>
    <ul>
      <li><p>Extended precisions (often 80 bits)</p></li>
      <li><p>128-bit quad precision</p></li>
      <li><p>16-bit half precision (multiple)</p></li>
      <li><p>Decimal formats</p></li>
    </ul>
    <p>Lots of interest in 16-bit formats for ML</p>
  </section>


  <section>
    <h3>What is a flop?</h3>

    <ul>
      <li>
        <p>Basic floating point operations:
          \[
          +, -, \times, /, \sqrt{\cdot}
          \]
        </p>
      </li>
      <li>
        <p>FMA (fused multiply-add):
          \[
            d = ab + c
          \]
        </p>
      </li>
      <li><p>Costs depend on precision and op</p></li>
      <li><p>Often focus on add, multiply, FMA (<q>flams</q>)</p></li>
    </ul>

  </section>


  <section>
    <h3>Flops / cycle / core</h3>

    <p>
      Processor does more than one thing at a time.  On my laptop
      (2018 MacBook Air):
    </p>
    <ul>
      <li><p>Two vector FMAs can start in one cycle<p></li>
      <li><p>Vector FMA does four DP FMAs at once<p></li>
      <li><p>Often count an FMA as two flops</p></li>
    </ul>
  </section>


  <section>
    <h3>Flops / cycle (one core)</h3>

    <span class="math display">\[
      2 \frac{\mbox{flops}}{\mbox{FMA}} \times
      4 \frac{\mbox{FMA}}{\mbox{vector FMA}} \times
      2 \frac{\mbox{vector FMA}}{\mbox{cycle}} =
      16 \frac{\mbox{flops}}{\mbox{cycle}}\]
    </span>
  </section>


  <section>
    <h3>Flops / sec (one core)</h3>

    \[\begin{aligned}
    16 \frac{\mbox{flops}}{\mbox{cycle}} \times
    (1.6 \times 10^9) \frac{\mbox{cycle}}{\mbox{sec}}
    &= 25.6 \times 10^9 \frac{\mbox{flops}}{\mbox{sec}} \\
    &= 25.6~\mbox{Gflop/s}
    \end{aligned}\]
  </section>


  <section>
    <h3>Flops / sec</h3>

    <p><span class="math display">\[
        25.6 \frac{\mbox{Gflop/s}}{\mbox{core}} \times
        2~\mbox{cores} =
        51.2~\mbox{Gflop/s}
    \]</span></p>
    <p>
      Things get more complicated if there are different core types
      (e.g. CPU cores and GPU cores)
    </p>
  </section>


  <section>
    <h3>Some historical context</h3>


    <p>Note: <a href="https://www.top500.org/system/166997/">CM-5/1024</a>
      peak was 131 Gflop/s.</p>
    <p>This was the top machine on the first Linpack benchmark list
      (July 1993).
    </p>
  </section>


  <section>
    <h3>Sanity check</h3>

    <p>
      What is the peak flop (CPU) flop rate on your machine?
      <a href="https://stackoverflow.com/questions/15655835/flops-per-cycle-for-sandy-bridge-and-haswell-sse2-avx-avx2#:~:text=Intel%20Haswell%2FBroadwell%2FSkylake%2F,(fused%20multiply%2Dadd)%20instructions">This
      StackOverflow thread</a> might help you figure out your flops/cycle.
    </p>
  </section>


</section>


<section> <!-- The cost of computing -->


  <section>
    <h2>The Cost of Computing</h2>
    <h3>(Single core)</h3>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Consider a simple serial code:</p>
    <pre><code>
// Accumulate C += A*B for n-by-n matrices
for (i = 0; i &lt; n; ++i)
  for (j = 0; j &lt; n; ++j)
    for (k = 0; k &lt; n; ++k)
      C[i+j*n] += A[i+k*n] * B[k+j*n];
    </code></pre>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Simplest model:</p>
    <ol>
      <li>Dominant cost is <span class="math inline">\(2n^3\)</span>
        flops (adds and multiplies)</li>
      <li>One flop per clock cycle</li>
      <li>
        <p>
          Expected time is
          \[
            \mbox{Time (s)} \approx
            \frac{2n^3 \mbox{ flops}}
            {25.6 \cdot 10^9 \mbox{ flop/s}}
          \]
        </p>
      </li>
    </ol>
    <p>Problem: Model assumptions are wrong!</p>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>
      Dominant cost is <span class="math inline">\(2n^3\)</span> flops
      (adds and multiplies)?</p>
    <ul>
      <li><p>Dominant cost is often memory traffic!</p></li>
      <li><p>Special case of a <em>communication cost</em></p></li>
    </ul>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Two pieces to cost of fetching data</p>
    <dl>
      <dt>Latency</dt>
      <dd><p>Time from operation start to first result (s)</p>
      </dd>
      <dt>Bandwidth</dt>
      <dd><p>Rate at which data arrives (bytes/s)</p>
      </dd>
    </dl>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <ul>
      <li>
        <p>
          Usually latency
          <span class="math inline">\(\gg\)</span>
          bandwidth<span class="math inline">\(^{-1}\)</span>
          <span class="math inline">\(\gg\)</span>
          time per flop
        </p>
      </li>
      <li>
        <p>Latency to L3 cache is 10s of ns</p>
      </li>
      <li>
        <p>
          DRAM is <span class="math inline">\(3\)</span>–<span class="math inline">\(4 \times\)</span> slower
        </p>
      </li>
      <li><p>Partial solution: caches (to discuss next time)</p></li>
    </ul>
    <p>
      See: <a href="https://gist.github.com/jboner/2841832">Latency
        numbers every programmer should know</a>
    </p>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Makes DRAM (<span class="math inline">\(\sim 100\)</span> ns)
      look even worse:
      \[
        100 \mbox{ ns} \times
        25.6 \mbox{ Gflop/s} = 2560 \mbox{ flops}
      \]
      </span>
    </p>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <ul>
      <li><p>Lose orders of magnitude if too many memory refs</p></li>
      <li><p>And getting full vectorization is also not easy!</p></li>
      <li><p>We’ll talk more about (single-core) arch next time</p></li>
    </ul>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>
      What to take away from this example?
    </p>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Start with a simple model</p>
    <ul>
      <li><p>Simplest: asymptotic complexity (e.g. <span class="math inline">\(O(n^3)\)</span> flops)</p></li>
      <li><p>Counting <em>every</em> detail just complicates life</p></li>
      <li><p>But we want enough detail to predict something</p></li>
    </ul>
  </section>


  <section>
    <h3>The Cost of Computing</h3>

    <p>Watch out for hidden costs</p>
    <ul>
      <li><p>Flops are not the only cost!</p></li>
      <li><p>Memory/communication costs are often killers</p></li>
      <li><p>Integer computation may play a role as well</p></li>
    </ul>
  </section>


  <section>
    <h3>The Cost of Computing</h3>
    <p>Haven’t even talked about &gt; 1 core yet!</p>
  </section>

</section>


<section> <!-- Cost of parallel computing -->

  <section>
    <h2>The Cost of Computing</h2>
    <h3>(in parallel)
  </section>

  <section>
    <h3>The Cost of (Parallel) Computing</h3>
    <p>Simple model:</p>
    <ul>
      <li><p>Serial task takes time <span class="math inline">\(T\)</span> (or <span class="math inline">\(T(n)\)</span>)</p></li>
      <li><p>Deploy <span class="math inline">\(p\)</span> processors</p></li>
      <li><p>Parallel time is <span class="math inline">\(T(n)/p\)</span></p></li>
    </ul>
    <p>... and you should be suspicious by now!</p>
  </section>


  <section>
    <h3>The Cost of (Parallel) Computing</h3>
    <p>Why is parallel time not <span class="math inline">\(T/p\)</span>?</p>
    <ul>
      <li><p><strong>Overheads:</strong> Communication, synchronization, extra computation and memory overheads</p></li>
      <li><p><strong>Intrinsically serial</strong> work</p></li>
      <li><p><strong>Idle time</strong> due to synchronization</p></li>
      <li><p><strong>Contention</strong> for resources</p></li>
    </ul>
  </section>


  <section>
    <h3>Quantifying Parallel Performance</h3>
    <ul>
      <li><p>Starting point: good <em>serial</em> performance</p></li>
      <li><p>Scaling study: compare parallel to serial time as a function of number of processors (<span class="math inline">\(p\)</span>) <span class="math display">\[\begin{aligned}
            \mbox{Speedup} &amp;= \frac{\mbox{Serial time}}{\mbox{Parallel time}} \\[2mm]
            \mbox{Efficiency} &amp;= \frac{\mbox{Speedup}}{p}
            \end{aligned}\]</span></p></li>
      <li><p>Ideally, speedup = <span class="math inline">\(p\)</span>. Usually, speedup <span class="math inline">\(&lt; p\)</span>.</p></li>
      <li><p>Barriers to perfect speedup</p>
        <ul>
          <li><p>Serial work (Amdahl’s law)</p></li>
          <li><p>Parallel overheads (communication, synchronization)</p></li>
      </ul></li>
    </ul>
  </section>


  <section>
    <h3>Amdahl’s Law</h3>
    <p>Parallel scaling study where some serial code remains: <span class="math display">\[\begin{aligned}
        p = &amp; \mbox{ number of processors} \\
        s = &amp; \mbox{ fraction of work that is serial} \\
        t_s = &amp; \mbox{ serial time} \\
        t_p = &amp; \mbox{ parallel time} \geq s t_s + (1-s) t_s / p
        \end{aligned}\]</span></p>
    <p><span class="math display">\[\mbox{Speedup} =
        \frac{t_s}{t_p} = \frac{1}{s + (1-s) / p} &lt; \frac{1}{s}\]</span></p>
  </section>


  <section>
    <h3>Amdahl’s Law</h3>
    <p><span class="math display">\[\mbox{Speedup} &lt; \frac{1}{s}\]</span></p>
    <p>So <span class="math inline">\(1\%\)</span> serial work <span class="math inline">\(\implies\)</span> max speedup &lt; <span class="math inline">\(100 \times\)</span>, regardless of <span class="math inline">\(p\)</span>.</p>
  </section>


  <section>
    <h3>Strong and weak scaling</h3>
    <p>Ahmdahl looks bad! But two types of scaling studies:</p>
    <dl>
      <dt>Strong scaling</dt>
      <dd><p>Fix problem size, vary <span class="math inline">\(p\)</span></p>
      </dd>
      <dt>Weak scaling</dt>
      <dd><p>Fix work per processor, vary <span class="math inline">\(p\)</span></p>
      </dd>
    </dl>
  </section>

  <section>
    <h3>Strong and weak scaling</h3>
    <p>For weak scaling, study <em>scaled speedup</em> <span class="math display">\[S(p) =
        \frac{T_{\mbox{serial}}(n(p))}{T_{\mbox{parallel}}(n(p), p)}\]</span> Gustafson’s Law: <span class="math display">\[S(p) \leq p - \alpha(p-1)\]</span> where <span class="math inline">\(\alpha\)</span> is the fraction of work that is serial.</p>
  </section>


  <section>
    <h3>Imperfect parallelism</h3>

    <p>
      Problem is not just with purely serial work.  Distinguish between
    </p>
    <ul>
      <li>Work that cannot be parallelized (or offers limited parallelism)</li>
      <li>Work not worth parallelizing (because of coordination overheads)</li>
    </ul>
  </section>


  <section>
    <h3>Dependencies</h3>
    <p>Main pain point: <em>dependency</em> between computations</p>
    <pre><code>
        a = f(x)
        b = g(x)
        c = h(a,b)
    </code></pre>
    <p>Compute a and b in parallel, but finish both before c!<br />
      Limits amount of parallel work available.</p>
    <p>This is a true dependency (read-after-write). Also have false dependencies (write-after-read and write-after-write) that can be dealt with more easily.</p>
  </section>


  <section>
    <h3>Granularity</h3>

    <ul>
      <li>Coordination is expensive<br/>
        - including parallel start/stop!</li>
      <li>Need to do enough work to amortize parallel costs</li>
      <li>Not enough to have parallel work, need big chunks!</li>
      <li>Chunk size depends on the machine.</li>
    </ul>
  </section>

</section>

<section>

  <section>
    <h2>Patterns and Benchmarks</h2>
  </section>

  <section>
    <h3>Pleasing Parallelism</h3>
    <p>
      “Pleasingly parallel” (aka “embarrassingly parallel”)
      tasks require very little coordination, e.g.:
    </p>
    <ul>
      <li>Monte Carlo computations with independent trials</li>
      <li>Mapping many data items independently</li>
    </ul>
    <p>
      Result is “high-throughput” computing – easy to get impressive
      speedups!
    </p>
    <p>Says nothing about hard-to-parallelize tasks.</p>
  </section>


  <section>
    <h3>Patterns and Benchmarks</h3>

    <p>If your task is not pleasingly parallel, you ask:</p>
    <ul>
      <li>What is the best performance I reasonably expect?</li>
      <li>How do I get that performance?</li>
    </ul>
  </section>


  <section>
    <h3>Patterns and Benchmarks</h3>

    <p>
      Look at examples somewhat like yours – a
      <em>parallel pattern</em> – and maybe seek an informative
      benchmark. Better yet: reduce to a previously well-solved problem
      (build on tuned <em>kernels</em>).
    </p>

    <p>NB: Uninformative benchmarks will lead you astray.</p>
  </section>

</section>


<section>
  <h2>Onward!</h2>
</section>
