---
title: Parallel machines and models
layout: slides
audio: 2020-09-24-machine-model
---


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Parallel Machines and Models</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
    Welcome to another edition of 5220.  For the last few slide dekcs,
    we've been talking about single core performance, and maybe you
    thought "when do we get to the parallel case"?  Well, we start
    today.  This is an overview lecture to set the stage; we'll get
    into much more detail on the topics presented today in later
    slide decks.
  </aside>
</section>


<section>
  <h3>Parallel computer hardware</h3>
  <p>Have <em>processors</em>, <em>memory</em>, <em>interconnect</em>.</p>
  <ul>
    <li>Where is memory physically?</li>
    <li>Is it attached to processors?</li>
    <li>What is the network connectivity?</li>
  </ul>

  <aside class="notes">
    So far, our discussions of parallel machines have mostly focused
    on hardware.  We've talked about peak flop rates and power usage,
    and of course we've mentioned what types of processors these
    machines use.  But after our discussion of memory, maybe it won't
    surprise you to learn that the positioning of memory and the way
    we communicate with memory -- and with other processors -- plays
    just as big a role in performance.  In general, parallel
    architecture is about dealing with these three components: the
    individual processors, the memory, and the networks used to
    connect everything.  I'll sometimes say "network fabric" or
    "interconnect" when referring to the network; these terms
    basically all mean the same thing.  The network is often the thing
    that most distinguishes a supercomputer from a small cluster.
  </aside>
</section>


<section>
  <h3>Parallel programming model</h3>
  <p>Programming <em>model</em> through languages, libraries.</p>
  <ul>
    <li>What are the control mechanisms?</li>
    <li>What data semantics?  Private, shared?</li>
    <li>What synchronization constructs?</li>
  </ul>
  <p>For performance, need cost models!</p>

  <aside class="notes">
    This class is pretty low-level, but it's still worth
    distinguishing between the hardware and the programming model --
    that is, the abstractions we use for writing our parallel codes.
    The model tells us how we initiate and control parallel jobs,
    share data between processors, and synchronize the efforts of
    different processors.  The parallel programming models we'll
    discuss are pretty close to the way that we think about certain
    types of hardware, but they aren't identical.  We can implement
    shared memory programming abstractions even if we only have
    hardware support for passing messages around, and we can implement
    message passing on top of shared memory hardware.  Indeed, these
    can be really useful things to do!  So it is worthwhile keeping
    the programming abstraction distinct from the hardware in our
    minds.  Of course, if we want to think about performance as well
    as correctness of our parallel codes, we need to have some
    understanding of the hardware, too.  At the very least, we need to
    know enough to build cost models that will help us predict
    performance and guide us toward good implementations.
  </aside>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double dot(int n, double* x, double* y)
{
    double s = 0;
    for (int i = 0; i < n; ++i)
        s += x[i] * y[i];
    return s;
}</code></pre>
  <aside class="notes">
    Examples always help to make things concrete. For this lecture,
    our running example will be dot products. From our centroid
    example, we know this naïve implementation of the dot product
    might not be optimal on a single core; but this doesn't really
    matter for our discussion today.  What matters is that it is
    pretty obvious how to split the dot product into independent
    pieces of work that we could assign to different processors.
  </aside>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double pdot(int n, double* x, double* y)
{
    double s = 0;
    for (int p = 0; p < NUM_PROC; ++p) { // Loop to parallelize
      int i = p*n/NUM_PROC;
      int inext = (p+1)*n/NUM_PROC;
      double partial = dot(inext-i, x+i, y+i);
      s += partial;
    }
    return s;
}</code></pre>
  <aside class="notes">
    Well, let's be a little more explicit about how we might partition
    the work.  The idea is that we are going to split the big dot product
    into smaller dot products, each about size n/p.  Then we take the
    partial dot products, and accumulate them into the total sum.
    I've summarized the logic in the pdot code above, while leaving
    vague how we would actually parallelize the main loop.
  </aside>
</section>


<section>
  <h3>Simple example</h3>
  <p>How can we parallelize?</p>
  <ul>
    <li>Where do arrays <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> live? One CPU? Partitioned?</li>
    <li>Who does what work?</li>
    <li>How do we combine to get a single final result?</li>
  </ul>
  <aside class="notes">
    Actually, there is a lot that goes into thinking about parallel
    implementation for even this simple example.  We've referred to
    the arrays x and y, but in a parallel setting, there is a question
    of where they live.  Are they on the memory of a particular
    processor?  Does that concept even make sense?  Then, of course,
    there's the issue of who does what work.  We suggested one way for
    splitting up the sum, but there are other ways to do the split-up
    as well.  Finally, once each processor has done its work, we need
    to combine the partial sums, which means some type of
    communication.

    OK.  Let's turn now to a couple of different ways we might do this.
  </aside>
</section>


<section>


  <section>
    <h2>Shared memory model</h2>

    <aside class="notes">
      The first programming model we'll discuss is shared memory.
    </aside>
  </section>


  <section>
    <h3>Shared memory model</h3>
    <p>Program consists of <em>threads</em> of control.</p>
    <ul>
      <li>Can be created dynamically</li>
      <li>Each has private variables (e.g. local)</li>
      <li>Each has shared variables (e.g. heap)</li>
      <li>Communication through shared variables</li>
      <li>Coordinate by synchronizing on variables</li>
      <li>Examples: OpenMP, pthreads</li>
    </ul>

    <aside class="notes">
      In shared memory programming systems, like OpenMP or pthreads,
      there are independent "threads" of execution that communicate
      through a common memory space.  A thread has its own program
      counter and call stack, so typically would have some private
      (stack) variables as well as accessing shared space.  Threads
      may correspond to physical processors, but they don't strictly
      need to.  In some systems, threads can be dynamically added or
      removed; in others, there is a fixed pool of threads.  The
      tricky part of shared memory programming is synchronizing the
      access to the shared space.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot product</h3>
    <p>Dot product of two <span class="math inline">\(n\)</span> vectors on <span class="math inline">\(p \ll n\)</span> processors:</p>
    <ol type="1">
      <li>Each CPU: partial sum (<span class="math inline">\(n/p\)</span> elements, local)</li>
      <li>Everyone tallies partial sums</li>
    </ol>
    <p>Can we go home now?</p>

    <aside class="notes">
      In words, one shared memory or thread-based approach to dot
      products involves each CPU taking a partial dot product, and
      then adding that partial sum into a shared accumulator.
      Of course, it can't be that easy...
    </aside>
  </section>


  <section>
    <h3>Race condition</h3>
    <p>A <em>race condition</em>:</p>
    <ul>
      <li>Two threads access same variable</li>
      <li>At least one write.</li>
      <li>Access are concurrent – no ordering guarantees
        <ul>
          <li>Could happen simultaneously!</li>
      </ul></li>
    </ul>

    <aside class="notes">
      The problem here is that each thread is reading and writing to
      the same shared sum variable, but so far we haven't said
      anything about synchronization.  That's dangerous because it
      sets us up for what is called a race condition.  This is when
      two threads are accessing the same variable, with at least one
      write, concurrent access, and no ordering guarantees.  Race
      conditions lead to unpredictable variations in results.
    </aside>
  </section>


  <section>
    <h3>Race to the dot</h3>

    <p>Consider <tt>S += partial</tt> on two CPUs</p>

    <aside class="notes">
      Let's consider what could go wrong with two processors trying to
      simultaneously update the shared sum without synchronization.
    </aside>
  </section>


  <section>
    <h3>Race to the dot</h3>

    <table>
      <thead>
        <tr>
          <th><h4>P1</h4></th>
          <th><h4>P2</h4></th>
        </tr>
      </thead>
      <tbody>
        <tr><td>load S</td><td></td></tr>
        <tr><td>add partial</td><td></td></tr>
        <tr><td></td><td>load S</td></tr>
        <tr><td>store S</td><td></td></tr>
        <tr><td></td><td>add partial</td></tr>
        <tr><td></td><td>store S</td></tr>
      </tbody>
    </table>

    <aside class="notes">
      Let's consider what happens at a pseudo-assembly language
      update.  An update operation consists of three parts: first, we
      load the current sum into a register; then we update with the
      partial sum; and then we store back.  If both processors read
      before either one writes back the update, then part of the sum
      will be lost.  The thing that's really awful about this problem
      is that it won't always happen!  Whether we get the full sum, or
      one part, or the other depends on how the reads and writes
      interleave with teach other, which is pretty unpredictable and
      will vary from run to run.
    </aside>
  </section>


  <section>
    <h3>Sequential consistency</h3>

    <ul>
      <li>Idea: Looks like processors take turns, in order</li>
      <li>Convenient for thinking through correctness</li>
      <li>Really hard for performance!</li>
      <li>Will talk about <q>memory models</q> later</li>
    </ul>

    <aside class="notes">
      You might think the explanation of the race condition in the
      previous slide is unintuitive.  But even then, it's worse than
      you think.  The explanation that we gave implicitly involves the
      idea of sequential consistency: that is, the state of memory is
      consistent with some serial execution of interleaving of
      instructions from the different threads.  In reality, though,
      it's really hard to have sequential consistency and still get
      good performance on modern machines.  Some computer architects
      still try, but for the most part we have to live with weaker
      models of consistency, where even stranger things can happen
      than what we described.  We'll talk about these alternate models
      of memory later in the class.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Solution: consider S += partial_sum a <em>critical section</em></p>
    <ul>
      <li>Only one CPU at a time allowed in critical section</li>
      <li>Can violate invariants locally</li>
      <li>Enforce via a lock or mutex</li>
    </ul>
    <aside class="notes">
      How do we avoid this type of race condition?  The problem we saw
      really had to do with the fact that load, add, and store from
      one thread could interleave with the same operations from
      another thread.  We can avoid that by requiring that each thread
      get a lock, also called a mutual exclusion variable (or mutex),
      before applying the update.  This establishes what is called a
      critical section - critical sections are parts of the program
      that only one thread can enter at a time.  We will talk about
      locks and critical sections in more detail later.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Dot product with mutex:</p>
    <ol>
      <li>Create global mutex l</li>
      <li>Compute partial_sum</li>
      <li>Lock l</li>
      <li>S += partial_sum</li>
      <li>Unlock l</li>
    </ol>

    <aside class="notes">
      OK, let's sketch how this works.  We start by creating a lock or
      mutex variable.  Only one thread can "hold" or "acquire" the
      lock at a time.  So for the dot product, we compute our partial
      sum, acquire the lock, update the global sum, and release the
      lock.  This means each update is applied in sequence, without
      interleaving.  Of course, we don't know in what order the
      partial sums will be accumulated, and that matters in floating
      point, though only at the level of roundoff.  So this code,
      though correct, doesn't provide bitwise reproducibility of
      results.  As I might have already said, parallel execution is
      subtle stuff!
    </aside>
  </section>


  <section>
    <h3>A problem</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Acquire lock 1</li>
          <li>Acquire lock 2</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Acquire lock 2</li>
          <li>Acquire lock 1</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
    </div>
    <p>What if both processors execute step 1 simultaneously?</p>

    <aside class="notes">
      In the dot product example, we only need one lock, which we use
      to protect accesses to the global sum.  But what happens if we
      need more than one lock because we want to compute more than one
      thing?  Let's consider the example above.  If both threads are
      able to execute the first step simultaneously, then we run into
      trouble at the second step.  The first thread holds lock 1 and
      wants lock 2; and the second thread holds lock 2 and wants lock 1.
      Nobody can make progress!  This situation is called deadlock.
      We'll talk about this more later; it turns out that there are
      ways that we can ensure that we avoid deadlock, which those of
      you who took an OS class probably studied already (and maybe
      forgot!).  But let's now briefly mention a synchronization approach
      that will definitely not deadlock, and is really useful for lots
      of scientific codes.
    </aside>
  </section>


  <section>
    <h3>Shared memory with barriers</h3>
    <ul>
      <li>Lots of sci codes have phases (e.g. time steps)</li>
      <li>Communication only needed at end of phases</li>
      <li>Idea: synchronize on end of phase with <em>barrier</em>
        <ul>
          <li>More restrictive (less efficient?) than small locks</li>
          <li>But easier to think through! (e.g. less chance of deadlocks)</li>
        </ul>
      </li>
      <li>Sometimes called <em>bulk synchronous programming</em></li>
    </ul>

    <aside class="notes">
      In many scientific codes, and really in many codes in general,
      the computation has natural phases.  Within each phase, we can
      do independent work; for correctness, we just need to ensure
      that one phase is completely done before the next can start.  A
      barrier is a synchronization construct that does exactly this:
      every computation on every thread before the barrier has to
      finish before we start computations after the barrier.  This is
      not as flexible as fine-grain locking, but it's also less
      difficult to reason about correctness of code written with
      barriers.  This style of programming, where we have phases
      separated by barriers, is sometimes called bulk synchronous
      programming (or BSP for short).      
    </aside>
  </section>


  <section>
    <h3>Dot with barriers</h3>
    <ol>
      <li><tt>partial[threadid]</tt> = local partial sum</li>
      <li>barrier</li>
      <li>sum = sum(partial)</li>
    </ol>

    <aside class="notes">
      What does the dot product look like with barriers?  A typical
      organization might involve each thread writing a partial sum
      into an array, then a barrier, and then each thread summing the
      array to get a sum (in a private variable).  If we wanted the sum to go
      into a global space, we might have a designated thread copy the
      result out.
    </aside>
  </section>


  <section>
    <h3>Punchline</h3>
    <p>Shared memory <em>correctness</em> is hard</p>
    <ul>
      <li>Too little synchronization: races</li>
      <li>Too much synchronization: deadlock</li>
    </ul>
    <p>And this is before we talk performance!</p>

    <aside class="notes">
      So far, we've talked mostly about correctness in the shared
      memory model.  And it's not that simple!  We have to
      synchronize to avoid data races, but too much synchronization
      (done without care) might lead to deadlock.  And none of this
      has even touched on performance yet!  But to say anything about
      performance, we need to say a bit more about hardware.
    </aside>
  </section>

</section>

<section>

  <section>
    <h2>Shared memory machines</h2>
    <aside class="notes">
      So let's talk now about shared memory hardware.
    </aside>
  </section>

  <section>
    <h3>Uniform shared memory</h3>
    <ul>
      <li>Processors and memories talk through a bus</li>
      <li>Symmetric Multiprocessor (SMP)</li>
      <li>Hard to scale to lots of processors (think <span class="math inline">\(\leq 32\)</span>)
        <ul>
          <li>Bus becomes bottleneck</li>
          <li><em>Cache coherence</em> via snooping</li>
      </ul></li>
    </ul>

    <aside class="notes">
      One of the most straightforward approaches to shared memory is
      to attach processors and main memory to a common bus.  The
      caches live with the processors and not with the memory, so we
      need a way to keep them consistent (or coherent) with the main
      memory in the face of memory writes that other processors might
      commit.  A bus is a shared broadcast medium; only one core or
      memory can send on the bus at a time, but anyone can here what
      is being sent.  On the down side, the more processors and
      memories are on the bus, the harder it is to share that
      resource.  On the up side, because everyone can see the bus, it
      is possible to keep the caches consistent by "snooping" on
      memory traffic sent across the bus.
    </aside>
  </section>


  <section>
    <h3>Multithreaded processor machine</h3>
    <ul>
      <li>Maybe threads &gt; processors!</li>
      <li>Idea: Switch threads on long latency ops.</li>
      <li>Called <em>hyperthreading</em> by Intel</li>
      <li>Cray MTA was an extreme example</li>
    </ul>

    <aside class="notes">
      The shared memory interface also sometimes makes sense when
      threads don't exactly correspond to processors.  Usually, that
      means having more threads than processors.  When a thread has to
      wait for the results of a memory read, disk write, or other
      long-latency task, it can yield the processor for use by other
      threads.  Sometimes this is a purely software setup, and
      sometimes it involves hardware support.  On modern Intel chips,
      this is called hyper-threading, and we mentioned it briefly in
      our discussion of single-core architecture.  But there are some
      architectures that have gone way more extreme than what Intel
      did.  The Cray MTA machine took a really long time to access
      memory (though that time was pretty uniform), and tried to hide
      that latency with a *lot* of threads.  Needless to say, it was
      not easy to program.  I had an amusing semester in graduate
      school listening to one of my officemates swear at that
      machine.  Good times!  And better him than me.
    </aside>
  </section>


  <section>
    <h3>Distributed shared memory</h3>
    <ul>
      <li>Non-Uniform Memory Access (NUMA)</li>
      <li>Memory <em>logically</em> shared, <em>physically</em> distributed</li>
      <li>Any processor can access any address</li>
      <li>Close accesses are faster than far accesses</li>
      <li>Cache coherence is still a pain</li>
      <li>Most big modern chips are NUMA</li>
      <li>Many-core accelerators tend to be NUMA as well</li>
    </ul>

    <aside class="notes">
      When we have a lot of processors, we are generally forced to
      move away from sending all data across a single bus.  In this
      case, there is a more complex network that connects the
      processors and memories.  Often, memories are physically located
      together with processors, so each processor has "local" memory
      and "remote" memory.  These memories may all be accessed through
      the same logical address space, but it takes different amounts
      of time to read or write data depending on whether the read is
      to local or remote memory addresses.  This is called non-uniform
      memory access, or NUMA.

      NUMA systems scale to large numbers of cores much better than
      uniform access (or "symmetric") multiprocessors.  Most modern
      big chips are NUMA, as are most many-core accelerators.
      Unfortunately, it is harder to keep the caches in a NUMA chip
      consistent with each other than it is in an SMP (though there
      are mechanisms for this).
    </aside>
  </section>


  <section>
    <h3>Punchline</h3>
    <p>Shared memory is expensive!</p>
    <ul>
      <li>Uniform access means bus contention</li>
      <li>Non-uniform access scales better<br/>
        (but now access costs vary)</li>
      <li>Cache coherence is tricky</li>
      <li>May forgo sequential consistency for performance</li>
    </ul>

    <aside class="notes">
      So: shared memory hardware presents some challenges.  If we want
      uniform memory access, we need a beefy bus to connect the
      processors and memories together, and contention for that bus
      limits our performance.  Giving up on uniform access means that
      we can scale to more processors, but now the costs of accessing
      memories vary - though maybe we are OK with this, as there is
      already variation in access times due to the effect of the cache
      system.  Keeping the caches coherent with each other and with
      main memory is a challenge, particularly in the non-uniform
      case.  There are clever solutions, but for the sake of
      performance, we often don't seek to make those solutions
      perfectly maintain sequential consistency.

      Don't worry if you didn't catch all that.  We'll go into it in
      more detail when we talk about shared memory programming in
      OpenMP in a couple weeks.
    </aside>
  </section>

</section>


<section>
  <section>
    <h2>Message passing model</h2>
    <aside class="notes">
      The major alternative to shared memory programming is
      programming via message passing.
    </aside>
  </section>


  <section>
    <h3>Message-passing programming model</h3>
    <ul>
      <li>Collection of named processes</li>
      <li>Data is <em>partitioned</em></li>
      <li>Communication by send/receive of explicit message</li>
      <li>Lingua franca: MPI (Message Passing Interface)</li>
    </ul>

    <aside class="notes">
      In the message-passing model of parallel programming, we have a
      collection of named processes, each with private memory spaces.
      The data for the program is partitioned across these private
      memories; there is no global shared space.  Communication
      happens by explicitly sending and receiving messages between
      processors.  The standard approach to message-passing
      parallelism in scientific computing is the Message Passing
      Interface, or MPI.
    </aside>
  </section>


  <section>
    <h3>Message passing dot product: v1</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Partial sum s1</li>
          <li>Send s1 to P2</li>
          <li>Receive s2 from P2</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Partial sum s2</li>
          <li>Send s2 to P1</li>
          <li>Receive s1 from P1</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
    </div>
    <p>What could go wrong? Think of phones vs letters...</p>

    <aside class="notes">
      So let's talk about how parallel dot product might work with two
      processors in a message-passing model.  Each processor holds a
      part of x and a part of y in its memory.  The processor dots its
      piece, then sends the partial sum to the other processor.  Then
      the other processor receives the outside partial sum, adds it to
      the partial sum that it computed, and that gives the overall dot
      product.

      Alas, you knew it couldn't be quite that easy.  The problem is
      that sending a message *may* be less like a letter, and more
      like placing a telephone call.  You can't hang up the phone
      while you are dialing and waiting to deliver the message!  In a
      world where there are no busy signals or answering machines, if
      I call you at the same time that you call me, maybe we both
      spend forever waiting for the other side to pick up.  So this
      code is prone to deadlock.

      As it turns out, the system has buffers in it, and whether
      sending a message is phone-call-like or letter-like depends on
      the state of those buffers.  So the same code may work most of
      the time, but periodically deadlock because of the issue we
      sketched above.  This is pretty maddening to debug.
    </aside>
  </section>


  <section>
    <h3>Message passing dot product: v1</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Partial sum s1</li>
          <li>Send s1 to P2</li>
          <li>Receive s2 from P2</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Partial sum s2</li>
          <li>Receive s1 from P1</li>
          <li>Send s2 to P1</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
    </div>
    <p>Better, but what if more than two processors?</p>

    <aside class="notes">
      There's a way around the potential deadlock issue in the
      previous example.  If the first processor sends before
      receiving, and the second processor receives before sending,
      then we no longer have the possibility of deadlock.  P1 sends
      and  P2 receives; then P2 sends and P1 receives.  The order is
      nailed down.

      Of course, this business of swapping the sends and receives is
      fine for two processors communicating with each other in a
      fairly simple exchange.  What if there are more processors, and
      the communication between them is more complex?
    </aside>
  </section>


  <section>
    <h3>MPI: the de facto standard</h3>
    <ul>
      <li>Pro: <em>Portability</em></li>
      <li>Con: least-common-denominator for mid 80s</li>
    </ul>
    <p>The “assembly language” (or C?) of parallelism...<br />
      but, alas, assembly language can be high performance.</p>

    <aside class="notes">
      There are other message-passing programming environments out
      there, but the lingua franca in scientific computing is MPI: the
      Message-Passing Interface.  It can be pretty low-level, but
      there are a number of implementations that provide pretty good
      performance.
    </aside>
  </section>


  <section>
    <h3>Punchline</h3>
    <ul>
      <li>Message passing hides less than shared memory</li>
      <li>But correctness is still subtle</li>
    </ul>
    
    <aside class="notes">
      Shared memory programming sometimes seems like magic pixie dust:
      we take a serial code and sprinkle in some parallel constructs,
      and we get out immediate speedup.  Unfortunately, there are a
      lot of subtleties in both the performance and correctness of
      shared memory code.  Codes written in a message-passing style
      are often more verbose, and they have their own subtleties when
      it comes to correctness.  But it is sometimes easier to reason
      about performance, because the communication events are
      explicit, and not hidden behind an innocuous looking read from a
      variable that happens to be stored in a remote shared memory.
    </aside>
  </section>

</section>


<section>
  <section>
    <h2>Distributed memory machines</h2>

    <aside class="notes">
      All right.  We've talked about message-passing programming.  Now
      let's talk about distributed memory machines.
    </aside>
  </section>

  <section>
    <h3>Distributed memory machines</h3>
    <ul>
      <li>Each node has local memory
        <ul>
          <li>... and no direct access to memory on other nodes</li>
      </ul></li>
      <li>Nodes communicate via network interface</li>
      <li>Example: most modern clusters!</li>
    </ul>

    <aside class="notes">
      In distributed memory machines, the hardware doesn't provide
      direct support for memory on other nodes.  Instead, nodes
      communicate with each other by sending messages across a network
      via a network interface card (or NIC).  Most modern clusters are
      distributed memory between nodes, but have a shared memory
      architecture within a given node.  We can mix-and-match these
      architectural ideas.
    </aside>
  </section>


  <section>
    <h3>Back of the envelope</h3>

    <ul>
      <li>c is 3 billion m/s.</li>
      <li>One light-ns is about 0.3 m<br/>(about a foot)</li>
      <li>A big machine is often over 300 feet across</li>
      <li>May still be dominated by NIC latency (microseconds)</li>
      <li>Across a big machine will always be order(s)-of-magnitude slower
        than local memory accesses</li>
      <li>Another reason locality matters!</li>
    </ul>
  </section>

  <aside class="notes">
    Sending a message across the network can be a lot more expensive
    than retrieving data from memory, even when there is a cache miss.
    The problem is that one light nanosecond - or the distance that
    light travels in one nanosecond - is about a foot.  And big
    supercomputers often have a space footprint the size of a football
    field.  That means that even without the overheads for routing
    data through a network, simple speed-of-light delays might give us
    a delay of something like 600 ns to send data across the machine
    and back.  That's significantly worse than fetching data from
    DRAM!  On top of that, we might spend on the order of microseconds
    to get data through the NIC and onto the network.  And we can do a lot
    of flops in a couple microseconds!
  </aside>
</section>


<section>

  <section>
    <h2>Paths to Parallel Performance</h2>

    <aside class="notes">
      All right.  We've talked about shared memory programming, shared
      memory hardware, message passing, and distributed memory.
      Correctness is hard in every case we've considered; performance
      is harder.  We don't want to learn all the minutiae in order to
      write fast code with these programming models and hardware
      platforms.  So how can we do this?
    </aside>
  </section>


  <section>
    <h3>Reminder: what do we want?</h3>
    <ul>
      <li>High-level: solve big problems fast</li>
      <li>Start with good <em>serial</em> performance</li>
      <li>Given <span class="math inline">\(p\)</span> processors, could then ask for
        <ul>
          <li>Good <em>speedup</em>: <span class="math inline">\(p^{-1}\)</span> times serial time</li>
          <li>Good <em>scaled speedup</em>: <span class="math inline">\(p \times\)</span> the work, same time</li>
      </ul></li>
      <li>Easiest to get speedup from bad serial code!</li>
    </ul>

    <aside class="notes">
      First, let's be clear what we want.  We want good scaling
      in either the strong sense - where the problem stays the same
      as the processor counts vary; or in the weak sense - where the
      work per processor stays the same as the processor counts vary.
      But good scaling is a hollow victory if we start with poor
      single-core performance, so ideally we will start with a
      well-tuned serial code and then parallelize.
    </aside>
  </section>


  <section>
    <h3>The story so far</h3>

    <p>Parallel performance is limited by:</p>
    <ul>
      <li>Single-core performance</li>
      <li>Communication and synchronization costs</li>
      <li>Non-parallel work (Amdahl)</li>
    </ul>
    <p>Plan now: talk about how to overcome these limits for some types
      of scientific applications</p>

    <aside class="notes">
      What limits the performance of parallel codes?  As we just said,
      one factor is single-core performance.  But it also matters how
      much time we spend on communication and synchronization;
      and how much parallelism is in our workload.  If much of the
      workload is serial, we are not going to get great parallel speedups no
      matter how many processors we might use!

      The key to getting lots of parallelism and minimizing
      communication costs is to exploit parallelism and locality in
      the problem.  How this works varies from problem to problem, but
      there are some common patterns that appear across many
      scientific codes.
    </aside>
  </section>


  <section>
    <h3>Parallelism and locality</h3>
    <p>Can get more parallelism / locality through model</p>
    <ul>
      <li>Limited range of dependency between time steps</li>
      <li>Can neglect or approximate far-field effects</li>
    </ul>

    <aside class="notes">
      Why do independent parallel work and local communication arise
      naturally in so many simulations?  It's really because a lot of
      interactions in the physical world are local.  For example, just
      as there is a speed of light or sound in physics, there is a
      rate at which information can travel across a computational mess
      in an explicit time stepper - and that rate is usually connected
      to the rates in the physics problem.  Also, when we look at
      computations in which far-away things have influence, the
      influence of those far-away things can often be approximated in
      a pretty simple way.  When we model gravity in the solar system,
      we treat the planets as point masses; and if we are going to
      compute the influence of distant star systems, we will probably
      just treat them as a single point mass!  This is a pretty big
      simplification over modeling the gravitational attraction of the
      matter as it is truly spread over space.
    </aside>
  </section>


  <section>
    <h3>Parallelism and locality</h3>
    <p>Often get parallism at multiple levels</p>
    <ul>
      <li>Hierarchical circuit simulation</li>
      <li>Interacting models for climate</li>
      <li>Parallelizing individual experiments in MC or optimization</li>
    </ul>

    <aside class="notes">
      Moreover, there is often a natural partitioning or hierarchy in
      how we treat models, and this structure helps us when it comes
      time to find parallelism.  We might be able to use parallel
      resources to simulate parts of a circuit or different components
      of a climate model, and then combine those parts later on
      without too much communication.

      Yes, talking about this will mean that I talk about the physics
      and modeling in simulations.  So this means that all of you who
      like numerics and physics but don't know architecture get to
      enjoy the change of pace, and those of you who knew about
      computer architecture but know little of physics modeling will
      get to learn something new!
    </aside>
  </section>

</section>


<section>
  <h3>Next up</h3>

  <p>Parallel patterns in simulation</p>
  <ul>
    <li>Discrete events and particle systems</li>
    <li>Differential equations (ODEs and PDEs)</li>
  </ul>

  <aside class="notes">
    So the plan for next week is to explore locality and parallelism
    in different types of simulations.  We will probably treat
    discrete event simulations and particle systems on Tuesday, and
    ordinary and partial differential equation models on Thursday.
    But in this new world order, I can combine slide decks when it
    makes sense, so you might also get everything in a giant deck on
    Tuesday.  We will see.  Either way, until next time!
  </aside>
</section>

