---
title: Parallel machines and models
layout: slides
audio: 2020-09-24-machine-model
---


<style>
.container{
    display: flex;
}
.col{
    flex: 1;
}
</style>


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Parallel Machines and Models</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
  </aside>
</section>


<section>
  <h3>Parallel computer hardware</h3>
  <p>Have <em>processors</em>, <em>memory</em>, <em>interconnect</em>.</p>
  <ul>
    <li>Where is memory physically?</li>
    <li>Is it attached to processors?</li>
    <li>What is the network connectivity?</li>
  </ul>
</section>


<section>
  <h3>Parallel programming model</h3>
  <p>Programming <em>model</em> through languages, libraries.</p>
  <ul>
    <li>What are the control mechanisms?</li>
    <li>What data semantics?  Private, shared?</li>
    <li>What synchronization constructs?</li>
  </ul>
  <p>For performance, need cost models!</p>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double dot(int n, double* x, double* y)
{
    double s = 0;
    for (int i = 0; i < n; ++i)
        s += x[i] * y[i];
    return s;
}</code></pre>
</section>

<section>
  <h3>Simple example</h3>
  <p>How can we parallelize?</p>
  <ul>
    <li>Where do arrays <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> live? One CPU? Partitioned?</li>
    <li>Who does what work?</li>
    <li>How do we combine to get a single final result?</li>
  </ul>
</section>


<section>
  <h3>Shared memory model</h3>
  <p>Program consists of <em>threads</em> of control.</p>
  <ul>
    <li>Can be created dynamically</li>
    <li>Each has private variables (e.g. local)</li>
    <li>Each has shared variables (e.g. heap)</li>
    <li>Communication through shared variables</li>
    <li>Coordinate by synchronizing on variables</li>
    <li>Examples: OpenMP, pthreads</li>
  </ul>
</section>


<section>
  <h3>Shared memory dot product</h3>
  <p>Dot product of two <span class="math inline">\(n\)</span> vectors on <span class="math inline">\(p \ll n\)</span> processors:</p>
  <ol type="1">
    <li><p>Each CPU: partial sum (<span class="math inline">\(n/p\)</span> elements, local)</p></li>
    <li><p>Everyone tallies partial sums</p></li>
  </ol>
  <p>Can we go home now?</p>
</section>


<section>
  <h3>Race condition</h3>
  <p>A <em>race condition</em>:</p>
  <ul>
    <li>Two threads access same variable</li>
    <li>At least one write.</li>
    <li>Access are concurrent – no ordering guarantees
      <ul>
        <li>Could happen simultaneously!</li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>Race to the dot</h3>

  <p>Consider <tt>S += partial</tt> on two CPUs</p>
</section>


<section>
  <h3>Race to the dot</h3>

  <table>
    <thead>
      <tr>
        <th><h4>P1</h4></th>
        <th><h4>P2</h4></th>
      </tr>
    </thead>
    <tbody>
      <tr><td>load S</td><td></td></tr>
      <tr><td>add partial</td><td></td></tr>
      <tr><td></td><td>load S</td></tr>
      <tr><td>store S</td><td></td></tr>
      <tr><td></td><td>add partial</td></tr>
      <tr><td></td><td>store S</td></tr>
    </tbody>
  </table>
</section>


<section>
  <h3>Shared memory dot with locks</h3>
  <p>Solution: consider S += partial_sum a <em>critical section</em></p>
  <ul>
    <li><p>Only one CPU at a time allowed in critical section</p></li>
    <li><p>Can violate invariants locally</p></li>
    <li><p>Enforce via a lock or mutex</p></li>
  </ul>
</section>


<section>
  <h3>Shared memory dot with locks</h3>
  <p>Dot product with mutex:</p>
  <ol>
    <li>Create global mutex l</li>
    <li>Compute partial_sum</li>
    <li>Lock l</li>
    <li>S += partial_sum</li>
    <li>Unlock l</li>
  </ol>
</section>


<section>
  <h3>A problem</h3>
  <div class="container">
    <div class="col">
      <p>Processor 1:</p>
      <ol type="1">
        <li>Acquire lock 1</li>
        <li>Acquire lock 2</li>
        <li>Do something</li>
        <li>Release locks</li>
      </ol>
    </div>
    <div class="col">
      <p>Processor 2:</p>
      <ol type="1">
        <li>Acquire lock 2<li>
        <li>Acquire lock 1</li>
        <li>Do something</li>
        <li>Release locks</li>
      </ol>
    </div>
  </div>
  <p>What if both processors execute step 1 simultaneously?</p>
</section>


<section>
  <h3>Shared memory with barriers</h3>
  <ul>
    <li><p>Lots of sci codes have phases (e.g. time steps)</p></li>
    <li><p>Communication only needed at end of phases</p></li>
    <li><p>Idea: synchronize on end of phase with <em>barrier</em></p>
      <ul>
        <li><p>More restrictive (less efficient?) than small locks</p></li>
        <li><p>But easier to think through! (e.g. less chance of deadlocks)</p></li>
    </ul></li>
    <li><p>Sometimes called <em>bulk synchronous programming</em></p></li>
  </ul>
</section>


<section>
  <h3>Dot with barriers</h3>
  <ol>
    <li><tt>partial[threadid]</tt> = local partial sum</li>
    <li>barrier</li>
    <li>sum = sum(partial)</li>
  </ol>
</section>



<section>
  <h3>Shared memory machine model</h3>
  <ul>
    <li><p>Processors and memories talk through a bus</p></li>
    <li><p>Symmetric Multiprocessor (SMP)</p></li>
    <li><p>Hard to scale to lots of processors (think <span class="math inline">\(\leq 32\)</span>)</p>
      <ul>
        <li><p>Bus becomes bottleneck</p></li>
        <li><p><em>Cache coherence</em> is a pain</p></li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>Multithreaded processor machine</h3>
  <ul>
    <li><p>Maybe threads &gt; processors!</p></li>
    <li><p>Idea: Switch threads on long latency ops.</p></li>
    <li><p>Called <em>hyperthreading</em> by Intel</p></li>
    <li><p>Cray MTA was an extreme example</p></li>
  </ul>
</section>


<section>
  <h3>Distributed shared memory</h3>
  <ul>
    <li><p>Non-Uniform Memory Access (NUMA)</p></li>
    <li><p>Memory <em>logically</em> shared, <em>physically</em> distributed</p></li>
    <li><p>Any processor can access any address</p></li>
    <li><p>Cache coherence is still a pain</p></li>
    <li><p>Most big modern chips are NUMA</p></li>
    <li><p>Many-core accelerators tend to be NUMA as well</p></li>
  </ul>
</section>


<section>
  <h3>Message-passing programming model</h3>
  <ul>
    <li><p>Collection of named processes</p></li>
    <li><p>Data is <em>partitioned</em></p></li>
    <li><p>Communication by send/receive of explicit message</p></li>
    <li><p>Lingua franca: MPI (Message Passing Interface)</p></li>
  </ul>
</section>


<section>
  <h3>Message passing dot product: v1</h3>
  <div class="container">
    <div class="col">
      <p>Processor 1:</p>
      <ol type="1">
        <li><p>Partial sum s1</p></li>
        <li><p>Send s1 to P2</p></li>
        <li><p>Receive s2 from P2</p></li>
        <li><p>s = s1 + s2</p></li>
      </ol>
    </div>
    <div class="col">
      <p>Processor 2:</p>
      <ol type="1">
        <li><p>Partial sum s2</p></li>
        <li><p>Send s2 to P1</p></li>
        <li><p>Receive s1 from P1</p></li>
        <li><p>s = s1 + s2</p></li>
      </ol>
    </div>
  </div>
  <p>What could go wrong? Think of phones vs letters...</p>
</section>


<section>
  <h3>Message passing dot product: v1</h3>
  <div class="container">
    <div class="col">
      <p>Processor 1:</p>
      <ol type="1">
        <li><p>Partial sum s1</p></li>
        <li><p>Send s1 to P2</p></li>
        <li><p>Receive s2 from P2</p></li>
        <li><p>s = s1 + s2</p></li>
      </ol>
    </div>
    <div class="col">
      <p>Processor 2:</p>
      <ol type="1">
        <li><p>Partial sum s2</p></li>
        <li><p>Receive s1 from P1</p></li>
        <li><p>Send s2 to P1</p></li>
        <li><p>s = s1 + s2</p></li>
      </ol>
    </div>
  </div>
  <p>Better, but what if more than two processors?</p>
</section>


<section>
  <h3>MPI: the de facto standard</h3>
  <ul>
    <li><p>Pro: <em>Portability</em></p></li>
    <li><p>Con: least-common-denominator for mid 80s</p></li>
  </ul>
  <p>The “assembly language” (or C?) of parallelism...<br />
    but, alas, assembly language can be high performance.</p>
</section>


<section>
  <h3>Distributed memory machines</h3>
  <ul>
    <li><p>Each node has local memory</p>
      <ul>
        <li>... and no direct access to memory on other nodes</li>
    </ul></li>
    <li><p>Nodes communicate via network interface</p></li>
    <li><p>Example: most modern clusters!</p></li>
  </ul>
</section>


<section>
  <h3>The story so far</h3>

  <p>Good parallel performance starts with good single-core performance.</p>
  <ul>
    <li>Performance depends on architecture and memory system</li>
    <li>Need to design data structures and algorithms to use these
      features</li>
  </ul>
</section>

<section>
  <h3>The story so far</h3>

  <p>Parallelism requires communication and synchronization.</p>
  <ul>
    <li>Several models with varying subtleties</li>
    <li>Correctness is hard!</li>
    <li>Performance is hard, too!</li>
  </ul>
</section>

<section>
  <h3>The story so far</h3>

  <p>Parallel performance is limited by:</p>
  <ul>
    <li>Single-core performance</li>
    <li>Communication and synchronization costs</li>
    <li>Non-parallel work (Amdahl)</li>
  </ul>
  <p>Plan now: talk about how to overcome these limits for some types
    of scientific applications</p>
</section>


<section>
  <h3>Reminder: what do we want?</h3>
  <ul>
    <li>High-level: solve big problems fast</li>
    <li>Start with good <em>serial</em> performance</li>
    <li>Given <span class="math inline">\(p\)</span> processors, could then ask for
      <ul>
        <li>Good <em>speedup</em>: <span class="math inline">\(p^{-1}\)</span> times serial time</li>
        <li>Good <em>scaled speedup</em>: <span class="math
        inline">\(p \times\)</span> the work, same time</li>
    </ul></li>
    <li>Easiest to get speedup from bad serial code!</li>
  </ul>
</section>


<section>
  <h3>Parallelism and locality</h3>

  <p>Real world exhibits <em>parallelism</em> and <em>locality</em></p>
  <ul>
    <li>Particles, people, etc function independently</li>
    <li>Nearby objects interact more strongly than distant ones</li>
    <li>Can often simplify dependence on distant objects</li>
  </ul>
</section>


<section>
  <h3>Parallelism and locality</h3>
  <p>Can get more parallelism / locality through model</p>
  <ul>
    <li><p>Limited range of dependency between adjacent time steps</p></li>
    <li><p>Can neglect or approximate far-field effects</p></li>
  </ul>
</section>


<section>
  <h3>Parallelism and locality</h3>
  <p>Often get parallism at multiple levels</p>
  <ul>
    <li><p>Hierarchical circuit simulation</p></li>
    <li><p>Interacting models for climate</p></li>
    <li><p>Parallelizing individual experiments in MC or optimization</p></li>
  </ul>
</section>


<section>
  <h3>Basic styles of simulation</h3>
  <ul>
    <li><p>Discrete event systems (continuous or discrete time)</p>
      <ul>
        <li><p>Game of life, logic-level circuit simulation</p></li>
        <li><p>Network simulation</p></li>
    </ul></li>
    <li><p>Particle systems</p>
      <ul>
        <li><p>Billiards, electrons, galaxies, ...</p></li>
        <li><p>Ants, cars, ...?</p></li>
    </ul></li>
    <li><p>Lumped parameter models (ODEs)</p>
      <ul>
        <li>Circuits (SPICE), structures, chemical kinetics</li>
    </ul></li>
    <li><p>Distributed parameter models (PDEs / integral equations)</p>
      <ul>
        <li>Heat, elasticity, electrostatics, ...</li>
    </ul></li>
  </ul>
  <p>Often more than one type of simulation appropriate.<br />
    Sometimes more than one at a time!</p>
</section>
