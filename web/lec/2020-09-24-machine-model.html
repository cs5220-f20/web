---
title: Parallel machines and models
layout: slides
audio: 2020-09-24-machine-model
---


<style>
.container{
    display: flex;
}
.col{
    flex: 1;
}
</style>


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Parallel Machines and Models</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
    Welcome to another edition of 5220.  For the last few slide dekcs,
    we've been talking about single core performance, and maybe you
    thought "when do we get to the parallel case"?  Well, we start
    today.  This is an overview lecture to set the stage; we'll get
    into much more detail on the topics presented today in later
    slide decks.
  </aside>
</section>


<section>
  <h3>Parallel computer hardware</h3>
  <p>Have <em>processors</em>, <em>memory</em>, <em>interconnect</em>.</p>
  <ul>
    <li>Where is memory physically?</li>
    <li>Is it attached to processors?</li>
    <li>What is the network connectivity?</li>
  </ul>

  <aside class="notes">
    So far, our discussions of parallel machines have mostly focused
    on hardware.  We've talked about peak flop rates and power usage,
    and of course we've mentioned what types of processors these
    machines use.  But after our discussion of memory, maybe it won't
    surprise you to learn that the positioning of memory and the way
    we communicate with memory -- and with other processors -- plays
    just as big a role in performance.  In general, parallel
    architecture is about dealing with these three components: the
    individual processors, the memory, and the networks used to
    connect everything.  I'll sometimes say network fabric or
    interconnect when referring to the network; these terms
    basically all mean the same thing.  The network is often the thing
    that most distinguishes a supercomputer from a small cluster.
  </aside>
</section>


<section>
  <h3>Parallel programming model</h3>
  <p>Programming <em>model</em> through languages, libraries.</p>
  <ul>
    <li>What are the control mechanisms?</li>
    <li>What data semantics?  Private, shared?</li>
    <li>What synchronization constructs?</li>
  </ul>
  <p>For performance, need cost models!</p>
  
  <aside class="notes">
    This class is pretty low-level, but it's still worth
    distinguishing between the hardware and the programming model --
    that is, the abstractions we use for writing our parallel codes.
    The model tells us how we initiate and control parallel jobs,
    share data between processors, and synchronize the efforts of
    different processors.  The parallel programming models we'll
    discuss are pretty close to the way that we think about certain
    types of hardware, but they aren't identical.  We can implement
    shared memory programming abstractions even if we only have
    hardware support for passing messages around, and we can implement
    message passing on top of shared memory hardware.  Indeed, these
    can be really useful things to do!  So it is worthwhile keeping
    the programming abstraction distinct from the hardware in our
    minds.  Of course, if we want to think about performance as well
    as correctness of our parallel codes, we need to have some
    understanding of the hardware, too.  At the very least, we need to
    know enough to build cost models that will help us predict
    performance and guide us toward good implementations.
  </aside>
</section>


<section>
  <h3>Simple example</h3>
  
<pre><code>double dot(int n, double* x, double* y)
{
    double s = 0;
    for (int i = 0; i < n; ++i)
        s += x[i] * y[i];
    return s;
}</code></pre>
  <aside class="notes">
    Examples always help to make things concrete. For this lecture,
    our running example will be dot products. From our centroid
    example, we know this naïve implementation of the dot product
    might not be optimal on a single core; but this doesn't really
    matter for our discussion today.  What matters is that it is
    pretty obvious how to split the dot product into independent
    pieces of work that we could assign to different processors.
  </aside>
</section>


<section>
  <h3>Simple example</h3>

 <pre><code>double pdot(int n, double* x, double* y)
{
    double s = 0;
    for (int p = 0; p < NUM_PROC; ++p) { // Loop to parallelize
      int i = p*n/NUM_PROC;
      int inext = (p+1)*n/NUM_PROC;
      double partial = dot(inext-i, x+i, y+i);
      s += partial;
    }
    return s;
}</code></pre>
  <aside class="notes">
    Well, let's be a little more explicit about how we might partition
    the work.  The idea is that we are going to split the big dot product
    into smaller dot products, each about size n/p.  Then we take the
    partial dot products, and accumulate them into the total sum.
    I've summarized the logic in the pdot code above, while leaving
    vague how we would actually parallelize the main loop.
  </aside>
</section>


<section>
  <h3>Simple example</h3>
  <p>How can we parallelize?</p>
  <ul>
    <li>Where do arrays <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> live? One CPU? Partitioned?</li>
    <li>Who does what work?</li>
    <li>How do we combine to get a single final result?</li>
  </ul>
  <aside class="notes">
    Actually, there is a lot that goes into thinking about parallel
    implementation for even this simple example.  We've referred to
    the arrays x and y, but in a parallel setting, there is a question
    of where they live.  Are they on the memory of a particular
    processor?  Does that concept even make sense?  Then, of course,
    there's the issue of who does what work.  We suggested one way for
    splitting up the sum, but there are other ways to do the split-up
    as well.  Finally, once each processor has done its work, we need
    to combine the partial sums, which means some type of
    communication.

    OK.  Let's turn now to a couple of different ways we might do this.
  </aside>
</section>


<section>


  <section>
    <h2>Shared memory model</h2>

    <aside class="notes">
      The first programming model we'll discuss is shared memory.
    </aside>
  </section>


  <section>
    <h3>Shared memory model</h3>
    <p>Program consists of <em>threads</em> of control.</p>
    <ul>
      <li>Can be created dynamically</li>
      <li>Each has private variables (e.g. local)</li>
      <li>Each has shared variables (e.g. heap)</li>
      <li>Communication through shared variables</li>
      <li>Coordinate by synchronizing on variables</li>
      <li>Examples: OpenMP, pthreads</li>
    </ul>

    <aside class="notes">
      In shared memory programming systems, like OpenMP or pthreads,
      there are independent "threads" of execution that communicate
      through a common memory space.  A thread has its own program
      counter and call stack, so typically would have some private
      (stack) variables as well as accessing shared space.  Threads
      may correspond to physical processors, but they don't strictly
      need to.  In some systems, threads can be dynamically added or
      removed; in others, there is a fixed pool of threads.  The
      tricky part of shared memory programming is synchronizing the
      access to the shared space.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot product</h3>
    <p>Dot product of two <span class="math inline">\(n\)</span> vectors on <span class="math inline">\(p \ll n\)</span> processors:</p>
    <ol>
      <li>Each CPU: partial sum (<span class="math inline">\(n/p\)</span> elements, local)</li>
      <li>Everyone tallies partial sums</li>
    </ol>
    <p>Can we go home now?</p>

    <aside class="notes">
      In words, one shared memory or thread-based approach to dot
      products involves each CPU taking a partial dot product, and
      then adding that partial sum into a shared accumulator.
      Of course, it can't be that easy...
    </aside>
  </section>


  <section>
    <h3>Race condition</h3>
    <p>A <em>race condition</em>:</p>
    <ul>
      <li>Two threads access same variable</li>
      <li>At least one write.</li>
      <li>Access are concurrent – no ordering guarantees
        <ul>
          <li>Could happen simultaneously!</li>
      </ul></li>
    </ul>

    <aside class="notes">
      The problem here is that each thread is reading and writing to
      the same shared sum variable, but so far we haven't said
      anything about synchronization.  That's dangerous because it
      sets us up for what is called a race condition.  This is when
      two threads are accessing the same variable, with at least one
      write, concurrent access, and no ordering guarantees.  Race
      conditions lead to unpredictable variations in results.
    </aside>
  </section>


  <section>
    <h3>Race to the dot</h3>

    <p>Consider <tt>S += partial</tt> on two CPUs</p>

    <aside class="notes">
      Let's consider what could go wrong with two processors trying to
      simultaneously update the shared sum without synchronization.
    </aside>
  </section>
  

  <section>
    <h3>Race to the dot</h3>

    <table>
      <thead>
        <tr>
          <th><h4>P1</h4></th>
          <th><h4>P2</h4></th>
        </tr>
      </thead>
      <tbody>
        <tr><td>load S</td><td></td></tr>
        <tr><td>add partial</td><td></td></tr>
        <tr><td></td><td>load S</td></tr>
        <tr><td>store S</td><td></td></tr>
        <tr><td></td><td>add partial</td></tr>
        <tr><td></td><td>store S</td></tr>
      </tbody>
    </table>

    <aside class="notes">
      Let's consider what happens at a pseudo-assembly language
      update.  An update operation consists of three parts: first, we
      load the current sum into a register; then we update with the
      partial sum; and then we store back.  If both processors read
      before either one writes back the update, then part of the sum
      will be lost.  The thing that's really awful about this problem
      is that it won't always happen!  Whether we get the full sum, or
      one part, or the other depends on how the reads and writes
      interleave with teach other, which is pretty unpredictable and
      will vary from run to run.
    </aside>
  </section>


  <section>
    <h3>Sequential consistency</h3>

    <ul>
      <li>Idea: Looks like processors take turns, in order</li>
      <li>Convenient for thinking through correctness</li>
      <li>Really hard for performance!</li>
      <li>Will talk about <q>memory models</q> later</li>
    </ul>

    <aside class="notes">
      You might think the explanation of the race condition in the
      previous slide is unintuitive.  But even then, it's worse than
      you think.  The explanation that we gave implicitly involves the
      idea of sequential consistency: that is, the state of memory is
      consistent with some serial execution of interleaving of
      instructions from the different threads.  In reality, though,
      it's really hard to have sequential consistency and still get
      good performance on modern machines.  Some computer architects
      still try, but for the most part we have to live with weaker
      models of consistency, where even stranger things can happen
      than what we described.  We'll talk about these alternate models
      of memory later in the class.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Solution: consider S += partial_sum a <em>critical section</em></p>
    <ul>
      <li>Only one CPU at a time allowed in critical section</li>
      <li>Can violate invariants locally</li>
      <li>Enforce via a lock or mutex</li>
    </ul>
    <aside class="notes">
      How do we avoid this type of race condition?  The problem we saw
      really had to do with the fact that load, add, and store from
      one thread could interleave with the same operations from
      another thread.  We can avoid that by requiring that each thread
      get a lock, also called a mutual exclusion variable (or mutex),
      before applying the update.  This establishes what is called a
      critical section - critical sections are parts of the program
      that only one thread can enter at a time.  We will talk about
      locks and critical sections in more detail later.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Dot product with mutex:</p>
    <ol>
      <li>Create global mutex l</li>
      <li>Compute partial_sum</li>
      <li>Lock l</li>
      <li>S += partial_sum</li>
      <li>Unlock l</li>
    </ol>

    <aside class="notes">
      OK, let's sketch how this works.  We start by creating a lock or
      mutex variable.  Only one thread can "hold" or "acquire" the
      lock at a time.  So for the dot product, we compute our partial
      sum, acquire the lock, update the global sum, and release the
      lock.  This means each update is applied in sequence, without
      interleaving.  Of course, we don't know in what order the
      partial sums will be accumulated, and that matters in floating
      point, though only at the level of roundoff.  So this code,
      though correct, doesn't provide bitwise reproducibility of
      results.  As I might have already said, parallel execution is
      subtle stuff!
    </aside>
  </section>


  <section>
    <h3>A problem</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Acquire lock 1</li>
          <li>Acquire lock 2</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Acquire lock 2</li>
          <li>Acquire lock 1</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
    </div>
    <p>What if both processors execute step 1 simultaneously?</p>

    <aside class="notes">
      In the dot product example, we only need one lock, which we use
      to protect accesses to the global sum.  But what happens if we
      need more than one lock because we want to compute more than one
      thing?  Let's consider the example above.  If both threads are
      able to execute the first step simultaneously, then we run into
      trouble at the second step.  The first thread holds lock 1 and
      wants lock 2; and the second thread holds lock 2 and wants lock 1.
      Nobody can make progress!  This situation is called deadlock.
      We'll talk about this more later; it turns out that there are
      ways that we can ensure that we avoid deadlock, which those of
      you who took an OS class probably studied already (and maybe
      forgot!).  But let's now briefly mention a synchronization approach
      that will definitely not deadlock, and is really useful for lots
      of scientific codes.
    </aside>
  </section>


  <section>
    <h3>Shared memory with barriers</h3>
    <ul>
      <li>Lots of sci codes have phases (e.g. time steps)</li>
      <li>Communication only needed at end of phases</li>
      <li>Idea: synchronize on end of phase with <em>barrier</em>
        <ul>
          <li>More restrictive (less efficient?) than small locks</li>
          <li>But easier to think through! (e.g. less chance of deadlocks)</li>
        </ul>
      </li>
      <li>Sometimes called <em>bulk synchronous programming</em></li>
    </ul>

    <aside class="notes">
      In many scientific codes, and really in many codes in general,
      the computation has natural phases.  Within each phase, we can
      do independent work; for correctness, we just need to ensure
      that one phase is completely done before the next can start.  A
      barrier is a synchronization construct that does exactly this:
      every computation on every thread before the barrier has to
      finish before we start computations after the barrier.  This is
      not as flexible as fine-grain locking, but it's also less
      difficult to reason about correctness of code written with
      barriers.  This style of programming, where we have phases
      separated by barriers, is sometimes called bulk synchronous
      programming (or BSP for short).      
    </aside>
  </section>


  <section>
    <h3>Dot with barriers</h3>
    <ol>
      <li><tt>partial[threadid]</tt> = local partial sum</li>
      <li>barrier</li>
      <li>sum = sum(partial)</li>
    </ol>

    <aside class="notes">
      What does the dot product look like with barriers?  A typical
      organization might involve each thread writing a partial sum
      into an array, then a barrier, and then each thread summing the
      array to get a sum (in a private variable).  If we wanted the sum to go
      into a global space, we might have a designated thread copy the
      result out.
    </aside>
  </section>


  <section>
    <h3>Punchline</h3>
    <p>Shared memory <em>correctness</em> is hard</p>
    <ul>
      <li>Too little synchronization: races</li>
      <li>Too much synchronization: deadlock</li>
    </ul>
    <p>And this is before we talk performance!</p>

    <aside class="notes">
      So far, we've talked mostly about correctness in the shared
      memory model.  And it's not that simple!  We have to
      synchronize to avoid data races, but too much synchronization
      (done without care) might lead to deadlock.  And none of this
      has even touched on performance yet!  But to say anything about
      performance, we need to say a bit more about hardware.
    </aside>
  </section>

</section>

