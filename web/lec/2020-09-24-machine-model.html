---
title: Parallel machines and models
layout: slides
audio: 2020-09-24-machine-model
---


<style>
.container{
    display: flex;
}
.col{
    flex: 1;
}
</style>


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Parallel Machines and Models</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
  </aside>
</section>


<section>
  <h3>Parallel computer hardware</h3>
  <p>Have <em>processors</em>, <em>memory</em>, <em>interconnect</em>.</p>
  <ul>
    <li>Where is memory physically?</li>
    <li>Is it attached to processors?</li>
    <li>What is the network connectivity?</li>
  </ul>
</section>


<section>
  <h3>Parallel programming model</h3>
  <p>Programming <em>model</em> through languages, libraries.</p>
  <ul>
    <li>What are the control mechanisms?</li>
    <li>What data semantics?  Private, shared?</li>
    <li>What synchronization constructs?</li>
  </ul>
  <p>For performance, need cost models!</p>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double dot(int n, double* x, double* y)
{
    double s = 0;
    for (int i = 0; i < n; ++i)
        s += x[i] * y[i];
    return s;
}</code></pre>
</section>

<section>
  <h3>Simple example</h3>
  <p>How can we parallelize?</p>
  <ul>
    <li>Where do arrays <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> live? One CPU? Partitioned?</li>
    <li>Who does what work?</li>
    <li>How do we combine to get a single final result?</li>
  </ul>
</section>


<section>
  <h3>Shared memory model</h3>
  <p>Program consists of <em>threads</em> of control.</p>
  <ul>
    <li>Can be created dynamically</li>
    <li>Each has private variables (e.g. local)</li>
    <li>Each has shared variables (e.g. heap)</li>
    <li>Communication through shared variables</li>
    <li>Coordinate by synchronizing on variables</li>
    <li>Examples: OpenMP, pthreads</li>
  </ul>
</section>


<section>
  <h3>Shared memory dot product</h3>
  <p>Dot product of two <span class="math inline">\(n\)</span> vectors on <span class="math inline">\(p \ll n\)</span> processors:</p>
  <ol type="1">
    <li><p>Each CPU: partial sum (<span class="math inline">\(n/p\)</span> elements, local)</p></li>
    <li><p>Everyone tallies partial sums</p></li>
  </ol>
  <p>Can we go home now?</p>
</section>


<section>
  <h3>Race condition</h3>
  <p>A <em>race condition</em>:</p>
  <ul>
    <li>Two threads access same variable</li>
    <li>At least one write.</li>
    <li>Access are concurrent – no ordering guarantees
      <ul>
        <li>Could happen simultaneously!</li>
    </ul></li>
  </ul>
  <p>Need synchronization via lock or barrier.</p>
</section>


<section>
  <h3>Race to the dot</h3>

  <table>
    <thead>
      <tr>
        <th><h4>P1</h4></th>
        <th><h4>P2</h4></th>
      </tr>
    </thead>
    <tbody>
      <tr><td>load S</td><td></td></tr>
      <tr><td>add partial</td><td></td></tr>
      <tr><td></td><td>load S</td></tr>
      <tr><td>store S</td><td></td></tr>
      <tr><td></td><td>add partial</td></tr>
      <tr><td></td><td>store S</td></tr>
    </tbody>
  </table>
</section>


<section>
  <h3>Shared memory dot with locks</h3>
  <p>Solution: consider S += partial_sum a <em>critical section</em></p>
  <ul>
    <li><p>Only one CPU at a time allowed in critical section</p></li>
    <li><p>Can violate invariants locally</p></li>
    <li><p>Enforce via a lock or mutex (mutual exclusion variable)</p></li>
  </ul>

  <p>Dot product with mutex:</p>
  <ol type="1">
    <li><p>Create global mutex l</p></li>
    <li><p>Compute partial_sum</p></li>
    <li><p>Lock l</p></li>
    <li><p>S += partial_sum</p></li>
    <li><p>Unlock l</p></li>
  </ol>
</section>


<section>
  <h3>Shared memory with barriers</h3>
  <ul>
    <li><p>Lots of sci codes have phases (e.g. time steps)</p></li>
    <li><p>Communication only needed at end of phases</p></li>
    <li><p>Idea: synchronize on end of phase with <em>barrier</em></p>
      <ul>
        <li><p>More restrictive (less efficient?) than small locks</p></li>
        <li><p>But easier to think through! (e.g. less chance of deadlocks)</p></li>
    </ul></li>
    <li><p>Sometimes called <em>bulk synchronous programming</em></p></li>
  </ul>
</section>


<section>
  <h3>Shared memory machine model</h3>
  <ul>
    <li><p>Processors and memories talk through a bus</p></li>
    <li><p>Symmetric Multiprocessor (SMP)</p></li>
    <li><p>Hard to scale to lots of processors (think <span class="math inline">\(\leq 32\)</span>)</p>
      <ul>
        <li><p>Bus becomes bottleneck</p></li>
        <li><p><em>Cache coherence</em> is a pain</p></li>
    </ul></li>
    <li><p>Example: Six-core chips on cluster</p></li>
  </ul>
</section>


<section>
  <h3>Multithreaded processor machine</h3>
  <ul>
    <li><p>Maybe threads &gt; processors!</p></li>
    <li><p>Idea: Switch threads on long latency ops.</p></li>
    <li><p>Called <em>hyperthreading</em> by Intel</p></li>
    <li><p>Cray MTA was an extreme example</p></li>
  </ul>
</section>


<section>
  <h3>Distributed shared memory</h3>
  <ul>
    <li><p>Non-Uniform Memory Access (NUMA)</p></li>
    <li><p>Can <em>logically</em> share memory while <em>physically</em> distributing</p></li>
    <li><p>Any processor can access any address</p></li>
    <li><p>Cache coherence is still a pain</p></li>
    <li><p>Example: SGI Origin (or multiprocessor nodes on cluster)</p></li>
    <li><p>Many-core accelerators tend to be NUMA as well</p></li>
  </ul>
</section>


<section>
  <h3>Message-passing programming model</h3>
  <ul>
    <li><p>Collection of named processes</p></li>
    <li><p>Data is <em>partitioned</em></p></li>
    <li><p>Communication by send/receive of explicit message</p></li>
    <li><p>Lingua franca: MPI (Message Passing Interface)</p></li>
  </ul>
</section>


<section>
  <h3>Message passing dot product: v1</h3>
  <p>Processor 1:</p>
  <ol type="1">
    <li><p>Partial sum s1</p></li>
    <li><p>Send s1 to P2</p></li>
    <li><p>Receive s2 from P2</p></li>
    <li><p>s = s1 + s2</p></li>
  </ol>
  <p>Processor 2:</p>
  <ol type="1">
    <li><p>Partial sum s2</p></li>
    <li><p>Send s2 to P1</p></li>
    <li><p>Receive s1 from P1</p></li>
    <li><p>s = s1 + s2</p></li>
  </ol>

  <p>What could go wrong? Think of phones vs letters...</p>
</section>


<section>
  <h3>Message passing dot product: v1</h3>
  <p>Processor 1:</p>
  <ol type="1">
    <li><p>Partial sum s1</p></li>
    <li><p>Send s1 to P2</p></li>
    <li><p>Receive s2 from P2</p></li>
    <li><p>s = s1 + s2</p></li>
  </ol>
  <p>Processor 2:</p>
  <ol type="1">
    <li><p>Partial sum s2</p></li>
    <li><p>Receive s1 from P1</p></li>
    <li><p>Send s2 to P1</p></li>
    <li><p>s = s1 + s2</p></li>
  </ol>

  <p>Better, but what if more than two processors?</p>
</section>


<section>
  <h3>MPI: the de facto standard</h3>
  <ul>
    <li><p>Pro: <em>Portability</em></p></li>
    <li><p>Con: least-common-denominator for mid 80s</p></li>
  </ul>
  <p>The “assembly language” (or C?) of parallelism...<br />
    but, alas, assembly language can be high performance.</p>
</section>


<section>
  <h3>Distributed memory machines</h3>
  <ul>
    <li><p>Each node has local memory</p>
      <ul>
        <li>... and no direct access to memory on other nodes</li>
    </ul></li>
    <li><p>Nodes communicate via network interface</p></li>
    <li><p>Example: our cluster!</p></li>
    <li><p>Other examples: IBM SP, Cray T3E</p></li>
  </ul>
</section>


<section>
  <h3>The story so far</h3>
  <ul>
    <li><p>Even <em>serial</em> performance is a complicated function of the underlying architecture and memory system. We need to understand these effects in order to design data structures and algorithms that are fast on modern machines. Good serial performance is the basis for good parallel performance.</p></li>
    <li><p><em>Parallel</em> performance is additionally complicated by communication and synchronization overheads, and by how much parallel work is available. If a small fraction of the work is completely serial, Amdahl’s law bounds the speedup, independent of the number of processors.</p></li>
    <li><p>We have discussed serial architecture and some of the basics of parallel machine models and programming models.</p></li>
    <li><p>Now we want to describe how to think about the shape of parallel algorithms for some scientific applications.</p></li>
  </ul>
</section>


<section>
  <h3>Reminder: what do we want?</h3>
  <ul>
    <li><p>High-level: solve big problems fast</p></li>
    <li><p>Start with good <em>serial</em> performance</p></li>
    <li><p>Given <span class="math inline">\(p\)</span> processors, could then ask for</p>
      <ul>
        <li><p>Good <em>speedup</em>: <span class="math inline">\(p^{-1}\)</span> times serial time</p></li>
        <li><p>Good <em>scaled speedup</em>: <span class="math inline">\(p\)</span> times the work in same time</p></li>
    </ul></li>
    <li><p>Easiest to get good speedup from cruddy serial code!</p></li>
  </ul>
</section>


<section>
  <h3>Parallelism and locality</h3>
  <ul>
    <li><p>Real world exhibits <em>parallelism</em> and <em>locality</em></p>
      <ul>
        <li><p>Particles, people, etc function independently</p></li>
        <li><p>Nearby objects interact more strongly than distant ones</p></li>
        <li><p>Can often simplify dependence on distant objects</p></li>
    </ul></li>
    <li><p>Can get more parallelism / locality through model</p>
      <ul>
        <li><p>Limited range of dependency between adjacent time steps</p></li>
        <li><p>Can neglect or approximate far-field effects</p></li>
    </ul></li>
    <li><p>Often get parallism at multiple levels</p>
      <ul>
        <li><p>Hierarchical circuit simulation</p></li>
        <li><p>Interacting models for climate</p></li>
        <li><p>Parallelizing individual experiments in MC or optimization</p></li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>Basic styles of simulation</h3>
  <ul>
    <li><p>Discrete event systems (continuous or discrete time)</p>
      <ul>
        <li><p>Game of life, logic-level circuit simulation</p></li>
        <li><p>Network simulation</p></li>
    </ul></li>
    <li><p>Particle systems</p>
      <ul>
        <li><p>Billiards, electrons, galaxies, ...</p></li>
        <li><p>Ants, cars, ...?</p></li>
    </ul></li>
    <li><p>Lumped parameter models (ODEs)</p>
      <ul>
        <li>Circuits (SPICE), structures, chemical kinetics</li>
    </ul></li>
    <li><p>Distributed parameter models (PDEs / integral equations)</p>
      <ul>
        <li>Heat, elasticity, electrostatics, ...</li>
    </ul></li>
  </ul>
  <p>Often more than one type of simulation appropriate.<br />
    Sometimes more than one at a time!</p>
</section>
