---
title: Parallel machines and models
layout: slides
audio: 2020-09-24-machine-model
---


<style>
.container{
    display: flex;
}
.col{
    flex: 1;
}
</style>


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Parallel Machines and Models</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>

  <aside class="notes">
    Welcome to another edition of 5220.  For the last few slide dekcs,
    we've been talking about single core performance, and maybe you
    thought "when do we get to the parallel case"?  Well, we start
    today.  This is an overview lecture to set the stage; we'll get
    into much more detail on the topics presented today in later
    slide decks.
  </aside>
</section>


<section>
  <h3>Parallel computer hardware</h3>
  <p>Have <em>processors</em>, <em>memory</em>, <em>interconnect</em>.</p>
  <ul>
    <li>Where is memory physically?</li>
    <li>Is it attached to processors?</li>
    <li>What is the network connectivity?</li>
  </ul>

  <aside class="notes">
    So far, our discussions of parallel machines have mostly focused
    on hardware.  We've talked about peak flop rates and power usage,
    and of course we've mentioned what types of processors these
    machines use.  But after our discussion of memory, maybe it won't
    surprise you to learn that the positioning of memory and the way
    we communicate with memory -- and with other processors -- plays
    just as big a role in performance.  In general, parallel
    architecture is about dealing with these three components: the
    individual processors, the memory, and the networks used to
    connect everything.  I'll sometimes say "network fabric" or
    "interconnect" when referring to the network; these terms
    basically all mean the same thing.  The network is often the thing
    that most distinguishes a supercomputer from a small cluster.
  </aside>
</section>


<section>
  <h3>Parallel programming model</h3>
  <p>Programming <em>model</em> through languages, libraries.</p>
  <ul>
    <li>What are the control mechanisms?</li>
    <li>What data semantics?  Private, shared?</li>
    <li>What synchronization constructs?</li>
  </ul>
  <p>For performance, need cost models!</p>

  <aside class="notes">
    This class is pretty low-level, but it's still worth
    distinguishing between the hardware and the programming model --
    that is, the abstractions we use for writing our parallel codes.
    The model tells us how we initiate and control parallel jobs,
    share data between processors, and synchronize the efforts of
    different processors.  The parallel programming models we'll
    discuss are pretty close to the way that we think about certain
    types of hardware, but they aren't identical.  We can implement
    shared memory programming abstractions even if we only have
    hardware support for passing messages around, and we can implement
    message passing on top of shared memory hardware.  Indeed, these
    can be really useful things to do!  So it is worthwhile keeping
    the programming abstraction distinct from the hardware in our
    minds.  Of course, if we want to think about performance as well
    as correctness of our parallel codes, we need to have some
    understanding of the hardware, too.  At the very least, we need to
    know enough to build cost models that will help us predict
    performance and guide us toward good implementations.
  </aside>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double dot(int n, double* x, double* y)
{
    double s = 0;
    for (int i = 0; i < n; ++i)
        s += x[i] * y[i];
    return s;
}</code></pre>
  <aside class="notes">
    Examples always help to make things concrete. For this lecture,
    our running example will be dot products. From our centroid
    example, we know this naïve implementation of the dot product
    might not be optimal on a single core; but this doesn't really
    matter for our discussion today.  What matters is that it is
    pretty obvious how to split the dot product into independent
    pieces of work that we could assign to different processors.
  </aside>
</section>


<section>
  <h3>Simple example</h3>

  <pre><code>double pdot(int n, double* x, double* y)
{
    double s = 0;
    for (int p = 0; p < NUM_PROC; ++p) { // Loop to parallelize
      int i = p*n/NUM_PROC;
      int inext = (p+1)*n/NUM_PROC;
      double partial = dot(inext-i, x+i, y+i);
      s += partial;
    }
    return s;
}</code></pre>
  <aside class="notes">
    Well, let's be a little more explicit about how we might partition
    the work.  The idea is that we are going to split the big dot product
    into smaller dot products, each about size n/p.  Then we take the
    partial dot products, and accumulate them into the total sum.
    I've summarized the logic in the pdot code above, while leaving
    vague how we would actually parallelize the main loop.
  </aside>
</section>


<section>
  <h3>Simple example</h3>
  <p>How can we parallelize?</p>
  <ul>
    <li>Where do arrays <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> live? One CPU? Partitioned?</li>
    <li>Who does what work?</li>
    <li>How do we combine to get a single final result?</li>
  </ul>
  <aside class="notes">
    Actually, there is a lot that goes into thinking about parallel
    implementation for even this simple example.  We've referred to
    the arrays x and y, but in a parallel setting, there is a question
    of where they live.  Are they on the memory of a particular
    processor?  Does that concept even make sense?  Then, of course,
    there's the issue of who does what work.  We suggested one way for
    splitting up the sum, but there are other ways to do the split-up
    as well.  Finally, once each processor has done its work, we need
    to combine the partial sums, which means some type of
    communication.

    OK.  Let's turn now to a couple of different ways we might do this.
  </aside>
</section>


<section>


  <section>
    <h2>Shared memory model</h2>

    <aside class="notes">
      The first programming model we'll discuss is shared memory.
    </aside>
  </section>


  <section>
    <h3>Shared memory model</h3>
    <p>Program consists of <em>threads</em> of control.</p>
    <ul>
      <li>Can be created dynamically</li>
      <li>Each has private variables (e.g. local)</li>
      <li>Each has shared variables (e.g. heap)</li>
      <li>Communication through shared variables</li>
      <li>Coordinate by synchronizing on variables</li>
      <li>Examples: OpenMP, pthreads</li>
    </ul>

    <aside class="notes">
      In shared memory programming systems, like OpenMP or pthreads,
      there are independent "threads" of execution that communicate
      through a common memory space.  A thread has its own program
      counter and call stack, so typically would have some private
      (stack) variables as well as accessing shared space.  Threads
      may correspond to physical processors, but they don't strictly
      need to.  In some systems, threads can be dynamically added or
      removed; in others, there is a fixed pool of threads.  The
      tricky part of shared memory programming is synchronizing the
      access to the shared space.
    </aside>
  </section>


  <section>
    <h3>Shared memory dot product</h3>
    <p>Dot product of two <span class="math inline">\(n\)</span> vectors on <span class="math inline">\(p \ll n\)</span> processors:</p>
    <ol type="1">
      <li>Each CPU: partial sum (<span class="math inline">\(n/p\)</span> elements, local)</li>
      <li>Everyone tallies partial sums</li>
    </ol>
    <p>Can we go home now?</p>
  </section>


  <section>
    <h3>Race condition</h3>
    <p>A <em>race condition</em>:</p>
    <ul>
      <li>Two threads access same variable</li>
      <li>At least one write.</li>
      <li>Access are concurrent – no ordering guarantees
        <ul>
          <li>Could happen simultaneously!</li>
      </ul></li>
    </ul>
  </section>


  <section>
    <h3>Race to the dot</h3>

    <p>Consider <tt>S += partial</tt> on two CPUs</p>
  </section>


  <section>
    <h3>Race to the dot</h3>

    <table>
      <thead>
        <tr>
          <th><h4>P1</h4></th>
          <th><h4>P2</h4></th>
        </tr>
      </thead>
      <tbody>
        <tr><td>load S</td><td></td></tr>
        <tr><td>add partial</td><td></td></tr>
        <tr><td></td><td>load S</td></tr>
        <tr><td>store S</td><td></td></tr>
        <tr><td></td><td>add partial</td></tr>
        <tr><td></td><td>store S</td></tr>
      </tbody>
    </table>
  </section>


  <section>
    <h3>Sequential consistency</h3>

    <ul>
      <li>Idea: Looks like processors take turns, in order</li>
      <li>Convenient for thinking through correctness</li>
      <li>Really hard for performance!</li>
      <li>Will talk about <q>memory models</q> later</li>
    </ul>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Solution: consider S += partial_sum a <em>critical section</em></p>
    <ul>
      <li>Only one CPU at a time allowed in critical section</li>
      <li>Can violate invariants locally</li>
      <li>Enforce via a lock or mutex</li>
    </ul>
  </section>


  <section>
    <h3>Shared memory dot with locks</h3>
    <p>Dot product with mutex:</p>
    <ol>
      <li>Create global mutex l</li>
      <li>Compute partial_sum</li>
      <li>Lock l</li>
      <li>S += partial_sum</li>
      <li>Unlock l</li>
    </ol>
  </section>


  <section>
    <h3>A problem</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Acquire lock 1</li>
          <li>Acquire lock 2</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Acquire lock 2</li>
          <li>Acquire lock 1</li>
          <li>Do something</li>
          <li>Release locks</li>
        </ol>
      </div>
    </div>
    <p>What if both processors execute step 1 simultaneously?</p>
  </section>


  <section>
    <h3>Shared memory with barriers</h3>
    <ul>
      <li>Lots of sci codes have phases (e.g. time steps)</li>
      <li>Communication only needed at end of phases</li>
      <li>Idea: synchronize on end of phase with <em>barrier</em></p>
        <ul>
          <li>More restrictive (less efficient?) than small locks</li>
          <li>But easier to think through! (e.g. less chance of deadlocks)</li>
      </ul></li>
      <li>Sometimes called <em>bulk synchronous programming</em></li>
    </ul>
  </section>


  <section>
    <h3>Dot with barriers</h3>
    <ol>
      <li><tt>partial[threadid]</tt> = local partial sum</li>
      <li>barrier</li>
      <li>sum = sum(partial)</li>
    </ol>
  </section>


  <section>
    <h3>Punchline</h3>
    <p>Shared memory <em>correctness</em> is hard</p>
    <ul>
      <li>Too little synchronization: races</li>
      <li>Too much synchronization: deadlock</li>
    </ul>
    <p>And this is before we talk performance!</p>
  </section>

</section>

<section>

  <section><h2>Shared memory machines</h2></section>

  <section>
    <h3>Uniform shared memory</h3>
    <ul>
      <li>Processors and memories talk through a bus</li>
      <li>Symmetric Multiprocessor (SMP)</li>
      <li>Hard to scale to lots of processors (think <span class="math inline">\(\leq 32\)</span>)</p>
        <ul>
          <li>Bus becomes bottleneck</li>
          <li><em>Cache coherence</em> is a pain</li>
      </ul></li>
    </ul>
  </section>


  <section>
    <h3>Multithreaded processor machine</h3>
    <ul>
      <li>Maybe threads &gt; processors!</li>
      <li>Idea: Switch threads on long latency ops.</li>
      <li>Called <em>hyperthreading</em> by Intel</li>
      <li>Cray MTA was an extreme example</li>
    </ul>
  </section>


  <section>
    <h3>Distributed shared memory</h3>
    <ul>
      <li>Non-Uniform Memory Access (NUMA)</li>
      <li>Memory <em>logically</em> shared, <em>physically</em> distributed</li>
      <li>Any processor can access any address</li>
      <li>Close accesses are faster than far accesses</li>
      <li>Cache coherence is still a pain</li>
      <li>Most big modern chips are NUMA</li>
      <li>Many-core accelerators tend to be NUMA as well</li>
    </ul>
  </section>


  <section>
    <h3>Punchline</h3>
    <p>Shared memory is expensive!</p>
    <ul>
      <li>Uniform access means bus contention</li>
      <li>Non-uniform access scales better<br/>
        (but now access costs vary)</li>
      <li>Cache coherence is tricky</li>
      <li>May forgo sequential consistency for performance</li>
    </ul>
  </section>

</section>


<section>
  <section><h2>Message passing model</h2></section>


  <section>
    <h3>Message-passing programming model</h3>
    <ul>
      <li>Collection of named processes</li>
      <li>Data is <em>partitioned</em></li>
      <li>Communication by send/receive of explicit message</li>
      <li>Lingua franca: MPI (Message Passing Interface)</li>
    </ul>
  </section>


  <section>
    <h3>Message passing dot product: v1</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Partial sum s1</li>
          <li>Send s1 to P2</li>
          <li>Receive s2 from P2</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Partial sum s2</li>
          <li>Send s2 to P1</li>
          <li>Receive s1 from P1</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
    </div>
    <p>What could go wrong? Think of phones vs letters...</p>
  </section>


  <section>
    <h3>Message passing dot product: v1</h3>
    <div class="container">
      <div class="col">
        <p>Processor 1:</p>
        <ol type="1">
          <li>Partial sum s1</li>
          <li>Send s1 to P2</li>
          <li>Receive s2 from P2</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
      <div class="col">
        <p>Processor 2:</p>
        <ol type="1">
          <li>Partial sum s2</li>
          <li>Receive s1 from P1</li>
          <li>Send s2 to P1</li>
          <li>s = s1 + s2</li>
        </ol>
      </div>
    </div>
    <p>Better, but what if more than two processors?</p>
  </section>


  <section>
    <h3>MPI: the de facto standard</h3>
    <ul>
      <li>Pro: <em>Portability</em></li>
      <li>Con: least-common-denominator for mid 80s</li>
    </ul>
    <p>The “assembly language” (or C?) of parallelism...<br />
      but, alas, assembly language can be high performance.</p>
  </section>


  <section>
    <h3>Punchline</h3>
    <ul>
      <li>Message passing hides less than shared memory</li>
      <li>But correctness is still subtle</li>
    </ul>
  </section>

</section>


<section>
  <section><h2>Distributed memory machines</h2></section>

  <section>
    <h3>Distributed memory machines</h3>
    <ul>
      <li>Each node has local memory</p>
        <ul>
          <li>... and no direct access to memory on other nodes</li>
      </ul></li>
      <li>Nodes communicate via network interface</li>
      <li>Example: most modern clusters!</li>
    </ul>
  </section>


  <section>
    <h3>Back of the envelope</h3>

    <ul>
      <li>c is 3 billion m/s.</li>
      <li>One light-ns is about 0.3 m<br/>(about a foot)</li>
      <li>A big machine is often over 300 feet across</li>
      <li>May still be dominated by NIC latency (microseconds)</li>
      <li>Across a big machine will always be order(s)-of-magnitude slower
        than local memory accesses</li>
      <li>Another reason locality matters!</li>
    </ul>
  </section>

</section>


<section>

  <section><h2>Paths to Parallel Performance</h2></section>


  <section>
    <h3>Reminder: what do we want?</h3>
    <ul>
      <li>High-level: solve big problems fast</li>
      <li>Start with good <em>serial</em> performance</li>
      <li>Given <span class="math inline">\(p\)</span> processors, could then ask for
        <ul>
          <li>Good <em>speedup</em>: <span class="math inline">\(p^{-1}\)</span> times serial time</li>
          <li>Good <em>scaled speedup</em>: <span class="math inline">\(p \times\)</span> the work, same time</li>
      </ul></li>
      <li>Easiest to get speedup from bad serial code!</li>
    </ul>
  </section>


  <section>
    <h3>The story so far</h3>

    <p>Parallel performance is limited by:</p>
    <ul>
      <li>Single-core performance</li>
      <li>Communication and synchronization costs</li>
      <li>Non-parallel work (Amdahl)</li>
    </ul>
    <p>Plan now: talk about how to overcome these limits for some types
      of scientific applications</p>
  </section>


  <section>
    <h3>Parallelism and locality</h3>
    <p>Can get more parallelism / locality through model</p>
    <ul>
      <li>Limited range of dependency between time steps</li>
      <li>Can neglect or approximate far-field effects</li>
    </ul>
  </section>


  <section>
    <h3>Parallelism and locality</h3>
    <p>Often get parallism at multiple levels</p>
    <ul>
      <li>Hierarchical circuit simulation</li>
      <li>Interacting models for climate</li>
      <li>Parallelizing individual experiments in MC or optimization</li>
    </ul>
  </section>

</section>


<section>
  <h3>Next up</h3>

  <p>Parallel patterns in simulation</p>
  <ul>
    <li>Discrete events and particle systems</li>
    <li>Differential equations (ODEs and PDEs)</li>
  </ul>
</section>
