---
title: Overview
layout: slides
audio: 2020-09-03-overview
---


<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>
  
  <aside class="notes">
  </aside>
</section>


<section>
  <h2>The Computational Science &amp; Engineering Picture</h2>
</section>


<section>
  <h2>Applications Everywhere!</h2>
  <ul>
    <li><p>Climate modeling</a></p></li>
    <li><p>CAD tools (computers, buildings, airplanes, ...)</p></li>
    <li><p>Computational biology</p></li>
    <li><p>Computational finance</p></li>
    <li><p>Machine learning and statistical models</p></li>
    <li><p>Game physics and movie special effects</p></li>
    <li><p>Medical imaging</p></li>
  </ul>
</section>


<section>
  <h2>Question for Discussion</h2>

  <p>
    Take a minute to Google "HPC X" where X is your favorite
    application.  What comes up?
  </p>

  <p>
    If you have no favorite applications, you might poke through
    <a href="https://www.hpcwire.com/">the front page of HPCWire</a>
    to see some things that others care about!
  </p>
</section>


<section>
  <h2>Why Parallel Computing?</h2>
  
  <p>Scientific computing went parallel long ago</p>
  <ul>
    <li><p>Want an answer that is right enough, fast enough</p></li>
    <li><p>Either of those might imply a lot of work!</p></li>
    <li><p>We like to ask for more as machines get bigger</p></li>
    <li><p>We have a lot of data, too</p></li>
  </ul>
</section>


<section>
  <h2>Why Parallel Computing?</h2>

  <p>Today: Hard to get a non-parallel computer!</p>
  <ul>
    <li><p>How many cores are in your laptop?</p></li>
    <li><p>How many in NVidia's latest accelerator?</p></li>
    <li><p>What's the biggest single node EC2 instance?</p></li>
  </ul>
</section>


<section>
  <h2>Lecture Plan</h2>
  <ol>
    <li><p><span><strong>Basics:</strong></span> architecture, parallel concepts, locality and parallelism in scientific codes</p></li>
    <li><p><span><strong>Technology:</strong></span> OpenMP, MPI, CUDA/OpenCL, cloud systems, compilers and tools</p></li>
    <li><p><span><strong>Patterns:</strong></span> Monte Carlo, dense and sparse linear algebra and PDEs, graph partitioning and load balancing, fast multipole, fast transforms</p></li>
  </ol>
</section>


<section>
  <h2>Objectives</h2>
  <p>Reason about code performance</p>
  <ul>
    <li><p>Many factors: HW, SW, algorithms</p></li>
    <li><p>Want simple “good enough” models</p></li>
  </ul>
</section>


<section>
  <h2>Objectives</h2>
  <p>Learn about high-performance computing (HPC)</p>
  <ul>
    <li><p>Learn parallel concepts and vocabulary</p></li>
    <li><p>Experience parallel platforms (HW and SW)</p></li>
    <li><p>Read/judge HPC literature</p></li>
    <li><p>Apply model numerical HPC patterns</p></li>
    <li><p>Tune existing codes for modern HW</p></li>
  </ul>
</section>


<section>
  <h2>Objectives</h2>
  <p>Apply good software practices</p>
</section>


<section>
  <h2>How Fast Can We Go?</h2>
  <p>Speed records for the Linpack benchmark:</p>
  <p><span><a href="http://www.top500.org" class="uri">http://www.top500.org</a></span></p>

  <p>Speed measured in flop/s (floating point ops / second):</p>
  <ul>
    <li><p>Giga (<span class="math inline">\(10^9\)</span>) – a single core</p></li>
    <li><p>Tera (<span class="math inline">\(10^{12}\)</span>) – a big machine</p></li>
    <li><p>Peta (<span class="math inline">\(10^{15}\)</span>) – current top 10 machines</p></li>
    <li><p>Exa (<span class="math inline">\(10^{18}\)</span>) – favorite of funding agencies</p></li>
  </ul>
</section>


<section>
  <h2>Fujitsu Fugaku</h2>

  <p>
    Look at
    the <a href="https://top500.org/news/report-fujitsu-fugaku-system-jack-dongarra/">report</a>.
    What does it say about:
  </p>
  <ul>
    <li><p>Peak flop rate, Linpack rate, HPCG rate?</p></li>
    <li><p>Energy use and cooling?</p></li>
    <li><p>Individual processor architecture?</p></li>
    <li><p>Network organization?</p></li>
    <li><p>Software stack?</p></li>
  </ul>
</section>


<section>
  <h2>Alternate: <a href="https://graph500.org/">Graph 500</a></h2>
  
  <p>Graph processing benchmark (data-intensive)</p>
  <ul>
    <li><p>Metric: traversed edges per second (TEPS)</p></li>
    <li><p>What is Fujutsu Fugaku in GTEPS?</p></li>
    <li><p>How do the top machines compare between
        <a href="https://top500.org/">Top 500</a> and
        <a href="https://graph500.org/">Graph 500?</a></p></li>
  </ul>
</section>


<section>
  <h2>Punchline</h2>
  <ul>
    <li><p>Some high-end machines look like high-end clusters</p>
      <ul>
        <li><p>Except custom networks.</p></li>
        <li><p>And then some machines look very different.</p></li>
    </ul></li>
    <li><p>Achievable performance is</p>
      <ul>
        <li><p><span class="math inline">\(\ll\)</span> peak performance</p></li>
        <li><p>Application-dependent</p></li>
    </ul></li>
    <li><p>Peak is hard on more modest platforms, too!</p></li>
  </ul>
</section>


<section>
  <h2>Practical Performance</h2>
  <p>So how fast can I make my computation?</p>
  <p>Peak <span class="math inline">\(&gt;\)</span>
    Linpack <span class="math inline">\(&gt;\)</span> Gordon
    Bell <span class="math inline">\(&gt;\)</span> Typical</p>
</section>


<section>
  <h2>Practical Performance</h2>
  <p>Measuring performance of real applications is hard</p>
  <ul>
    <li><p>What figure of merit (flops, TEPS, ...?)</p></li>
    <li><p>Typically a few bottlenecks slow things down</p></li>
    <li><p>Why they slow down can be tricky!</p></li>
  </ul>
</section>


<section>
  <h2>Practical Performance</h2>
  
  <p><em>Really</em> care about time-to-solution</p>
  <ul>
    <li><p>Sophisticated methods get answer in fewer flops</p></li>
    <li><p>... but may look bad in benchmarks (lower flop rates!)</p></li>
  </ul>
</section>


<section>
  <h2>Practical Performance</h2>
  
  <p>See also David Bailey’s comments:</p>
  <ul>
    <li><p><a href="http://crd.lbl.gov/~dhbailey/dhbpapers/twelve-ways.pdf">Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Computers</a> (1991)</p></li>
    <li><p><a href="http://crd.lbl.gov/~dhbailey/dhbtalks/dhb-12ways.pdf">Twelve Ways to Fool the Masses: Fast Forward to 2011</a> (2011)</p></li>
  </ul>
</section>


<section>
  <h2>Quantifying Performance</h2>

  <p>
    Starting point: good <em>serial</em> performance.
  </p>
</section>


<section>
  <h2>Quantifying Performance</h2>

  <p>
    Strong scaling: compare parallel to serial time on the same
    problem instance as a function of number of processors
    (<span class="math inline">\(p\)</span>) <span class="math display">\[\begin{aligned}
      \mbox{Speedup} &amp;= \frac{\mbox{Serial time}}{\mbox{Parallel time}} \\[2mm]
      \mbox{Efficiency} &amp;= \frac{\mbox{Speedup}}{p}
      \end{aligned}\]</span>
  </p>
</section>


<section>
  <h2>Quantifying Performance</h2>
  <p>
    Ideally, speedup = <span class="math inline">\(p\)</span>.
    Usually, speedup <span class="math inline">\(&lt; p\)</span>.
  </p>
  <p>Barriers to perfect speedup</p>
  <ul>
    <li><p>Serial work (Amdahl’s law)</p></li>
    <li><p>Parallel overheads (communication, synchronization)</p></li>
  </ul>
</section>

<section>
  <h2>Amdahl’s Law</h2>
  <p><span class="math display">\[\begin{aligned}
      p = &amp; \mbox{ number of processors} \\
      s = &amp; \mbox{ fraction of work that is serial} \\
      t_s = &amp; \mbox{ serial time} \\
      t_p = &amp; \mbox{ parallel time} \geq s t_s + (1-s) t_s / p
      \end{aligned}\]</span></p>
  
  <p><span class="math display">\[\mbox{Speedup} = 
      \frac{t_s}{t_p} = \frac{1}{s + (1-s) / p} &gt; \frac{1}{s}\]</span></p>

  <p>So <span class="math inline">\(1\%\)</span> serial work <span class="math inline">\(\implies\)</span> max speedup &lt; <span class="math inline">\(100 \times\)</span>, regardless of <span class="math inline">\(p\)</span>.</p>
</section>


<section>
  <h2>A Thought Experiment</h2>

  <p>Let’s try a simple parallel attendance count:</p>
  <ul>
    <li><p><span><strong>Parallel computation:</strong></span> Rightmost person in each row counts number in row.</p></li>
<li><p><span><strong>Synchronization:</strong></span> Raise your hand when you have a count</p></li>
<li><p><span><strong>Communication:</strong></span> When all hands are raised, each row representative adds their count to a tally and says the sum (going front to back).</p></li>
</ul>
</section>


<section>
  <h2>A Toy Analysis</h2>
  
  <p>Parameters: <span class="math display">\[\begin{aligned}
    n = &amp; \mbox{ number of students (80)} \\
    r = &amp; \mbox{ number of rows} \\
    t_c = &amp; \mbox{ time to count one student (0.3)} \\
    t_t = &amp; \mbox{ time to say tally (1)} \\
    t_s \approx &amp; ~n t_c \\
    t_p \approx &amp; ~n t_c / r + r t_t
  \end{aligned}\]</span></p>
  <p>How much could I possibly speed up?</p>
</section>

<section>
  <h2>Modeling Speedup</h2>
  <p><span class="math display">\[\mathrm{speedup} &lt; 
      \frac{1}{2} \sqrt{\frac{n t_c}{t_t}}\]</span></p>
  <ul>
    <li><p>The problem size <span class="math inline">\(n\)</span> is small</p></li>
    <li><p>The communication cost is relatively large</p></li>
    <li><p>The serial computation cost is relatively large</p></li>
  </ul>
  <p>Some of the usual suspects for parallel performance problems!<br />
</section>


<section>
  <h2>Summary: Thinking about Parallel Performance</h2>
  <ul>
    <li><p>We have (arguably) <em>exaflop</em> machines</p></li>
    <li><p>But codes rarely get peak performance</p></li>
    <li><p>Better comparison: tuned serial performance</p></li>
    <li><p>Common measures: <span><em>speedup</em></span>
        and <em>efficiency</em></p></li>
  </ul>
</section>


<section>
  <h2>Summary: Thinking about Parallel Performance</h2>
  <ul>
    <li><p>Strong scaling: study speedup with increasing <span class="math inline">\(p\)</span></p></li>
    <li><p>Weak scaling: increase both <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span></p></li>
    <li><p>Serial overheads, communication kill speedup</p></li>
    <li><p>Simple models help us understand scaling</p></li>
  </ul>
</section>
