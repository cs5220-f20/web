---
title: Stationary iterations
layout: slides
audio: 2020-10-27-stationary
---

<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>Stationary iterations</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>
</section>

<section>
<h3>Fixed Point Iteration</h3>
<p><img data-src="figs/fixedp.svg" title="Fixed point iteration" style="width:100.0%" /></p>
<p><span class="math inline">\(x_{k+1} = f(x_k) \rightarrow x_* = f(x_*)\)</span></p>
<aside class="notes">
<p>Stationary iterations for linear systems are an example of a <em>fixed point</em> iteration. A fixed point of a function <span class="math inline">\(f\)</span> is a point <span class="math inline">\(x_*\)</span> that <span class="math inline">\(f\)</span> maps to itself. Fixed point iteration involves getting a new guess <span class="math inline">\(x_{k+1}\)</span> by applying <span class="math inline">\(f\)</span> to the previous guess <span class="math inline">\(x_k\)</span>. And sometimes this actually converges to a fixed point!</p>
</aside>
</section>

<section>
<h3>Iterative Idea</h3>
<ul>
<li><span class="math inline">\(f\)</span> is a <em>contraction</em> if <span class="math inline">\(\|f(x)-f(y)\| &lt; \|x-y\|\)</span>, <span class="math inline">\(x \neq y\)</span>.</li>
<li><span class="math inline">\(f\)</span> has a unique <em>fixed point</em> <span class="math inline">\(x_* = f(x_*)\)</span>.</li>
<li>For <span class="math inline">\(x_{k+1} = f(x_k)\)</span>, <span class="math inline">\(x_k \rightarrow x_*\)</span>.</li>
<li>If <span class="math inline">\(\|f(x)-f(y)\| &lt; \alpha \|x-y\|\)</span>, <span class="math inline">\(\alpha &lt; 1\)</span>, for all <span class="math inline">\(x, y\)</span>, then <span class="math display">\[\|x_k-x_*\| &lt; \alpha^k \|x-x_*\|\]</span></li>
<li>Looks good <em>if</em> <span class="math inline">\(\alpha\)</span> not too near 1...</li>
</ul>
<aside class="notes">
<p>More specifically, we usually apply fixed point iteration to <em>contraction mappings</em>. A function <span class="math inline">\(f\)</span> is a contraction if it takes distinct points and moves them strictly closer to each other. A contraction that maps some closed, bounded domain into itself is guaranteed to have a unique fixed point, and fixed point iteration converges to that point.</p>
<p>Things get even better if the function is Lipschitz with constant <span class="math inline">\(\alpha &lt; 1\)</span> – meaning, if the distance between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(y)\)</span> is at most <span class="math inline">\(\alpha\)</span> times the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. In this case, we have what a numerical analyst would call linear convergence, and others might call geometric or exponential convergence: the error at step <span class="math inline">\(k\)</span> is at most <span class="math inline">\(\alpha^k\)</span> times the initial error. This sounds great if <span class="math inline">\(\alpha\)</span> is enough less than one!</p>
</aside>
</section>

<section>
<h3>Stationary Iterations</h3>
<p>Write <span class="math inline">\(Ax = b\)</span> as <span class="math inline">\(A = M-K\)</span>; get fixed point of <span class="math display">\[M x_{k+1} = K x_k + b\]</span> or <span class="math display">\[x_{k+1} = (M^{-1} K) x_k + M^{-1} b.\]</span></p>
<ul>
<li>Convergence if <span class="math inline">\(\rho(M^{-1} K) &lt; 1\)</span></li>
<li>Best case for convergence: <span class="math inline">\(M = A\)</span></li>
<li>Cheapest case: <span class="math inline">\(M = I\)</span></li>
</ul>
<aside class="notes">
<p>Stationary iterations for linear systems are linear fixed point iterations. The idea is that we write the matrix as <span class="math inline">\(A = M-K\)</span>. This is sometimes called a splitting of the matrix. We want the first part, <span class="math inline">\(M\)</span> to somehow capture all of the important features of <span class="math inline">\(A\)</span>, and yet be cheap to invert. If we put all of <span class="math inline">\(A\)</span> into <span class="math inline">\(M\)</span>, the resulting iteration converges in one step. But that step involves solving a linear system with <span class="math inline">\(A\)</span>, which puts us right back where we started! Or we can use <span class="math inline">\(M = I\)</span>, the so-called Richardson’s iteration; but that often gives us painfully slow convergence.</p>
</aside>
</section>

<section>
<h3>Stationary Iterations</h3>
<ul>
<li><p>Realistic: choose something between<br />
</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">Jacobi</td>
<td style="text-align: left;"><span class="math inline">\(M = \operatorname{diag}(A)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Gauss-Seidel</td>
<td style="text-align: left;"><span class="math inline">\(M = \operatorname{tril}(A)\)</span></td>
</tr>
</tbody>
</table></li>
</ul>
<aside class="notes">
<p>Of course, there are many points between the extremes of Richardson iteration and fully solving the linear system. The two standard starting points are Jacobi iteration, which involves taking <span class="math inline">\(M\)</span> to be the diagonal of the matrix <span class="math inline">\(A\)</span>; and Gauss-Seidel, which takes <span class="math inline">\(M\)</span> to be the lower triangular part of <span class="math inline">\(A\)</span> in some ordering.</p>
<p>What I’ve said so far is true, but it sort of misses the point of how these two methods are used in practice. Let’s make everything more concrete with an example.</p>
</aside>
</section>

<section>
<h3>Reminder: Discretized 2D Poisson Problem</h3>
<p><img data-src="figs/poisson2d-stencil.svg" title="Second-order finite difference stencil for 2D Poisson" style="width:40.0%" /></p>
<p><span class="math inline">\((Lu)_{i,j} = h^{-2} \left( 4u_{i,j}-u_{i-1,j}-u_{i+1,j}-u_{i,j-1}-u_{i,j+1} \right)\)</span></p>
<aside class="notes">
<p>We’ve talked about this equation before – Poisson’s equation on a 2D mesh. The estimated Laplacian at a point – that is, the sum of the second derivatives in the x and y directions – is computed by looking at the neighbors to the north, south, east, and west of that point.</p>
<p>So what does it look like to run Jacobi iteration on a Poisson problem?</p>
</aside>
</section>

<section>
<h3>Jacobi on 2D Poisson</h3>
<p>Assuming homogeneous Dirichlet boundary conditions</p>
<pre><code>for step = 1:nsteps

  for i = 2:n-1
    for j = 2:n-1
      u_next(i,j) = ...
        ( u(i,j+1) + u(i,j-1) + ...
          u(i-1,j) + u(i+1,j) )/4 - ...
        h^2*f(i,j)/4;
    end
  end
  u = u_next;

end</code></pre>
<p>Basically do some averaging at each step.</p>
<aside class="notes">
<p>The Jacobi iteration says we are going to get a new guess for the ith unknown by solving the ith equation, assuming all the other unknowns are correct. In the case of the Poisson equation, this looks a lot like just an averaging over neighbors.</p>
</aside>
</section>

<section>
<h3>Parallel version (5 point stencil)</h3>
<p><img data-src="figs/poisson2d-5pt-block.svg" title="Sketch of data dependencies in a 2D Poisson scheme with 5-point stencil" style="width:30.0%" /></p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">Boundary values:</td>
<td style="text-align: left;">white</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data on P0:</td>
<td style="text-align: left;">green</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ghost cell data:</td>
<td style="text-align: left;">blue</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Suppose we discretize the Poisson problem this way, and partition our domain into subdomains, one per processor. What does the dependency pattern for updating the unknowns on a given subdomain look like? Essentially, we need to get neighbor data (indicated in white) or ghost cell data (indicated in blue) off each edge of the processor domains.</p>
</aside>
</section>

<section>
<h3>Parallel version (9 point stencil)</h3>
<p><img data-src="figs/poisson2d-9pt-block.svg" title="Sketch of data dependencies in a 2D Poisson scheme with 5-point stencil" style="width:30.0%" /></p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">Boundary values:</td>
<td style="text-align: left;">white</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data on P0:</td>
<td style="text-align: left;">green</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ghost cell data:</td>
<td style="text-align: left;">blue</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>What if I had a nine-point stencil instead of a five-point stencil? That is, what if I depended on diagonal neighbors, too? In that case, it looks like maybe I need to communicate with up to four more processors, right?</p>
<p>Of course, this would also be an issue for the five-point stencil if we were batching steps of the Jacobi iteration. Either way, there’s a trick.</p>
</aside>
</section>

<section>
<h3>Parallel version (5 point stencil)</h3>
<p><img data-src="figs/poisson2d-swap5.svg" title="Communication pattern for 5 point stencil" style="width:30.0%" /></p>
<p>Communicate ghost cells before each step.</p>
<aside class="notes">
<p>In the five-point stencil case, assuming we were not batching steps, we would need to communicate data across processor boundaries at every step. That means that each processor would communicate with processors to its north, south, east, and west.</p>
<p>It turns out, we don’t need to have more messages in the nine point case. We just need some synchronization.</p>
</aside>
</section>

<section>
<h3>Parallel version (9 point stencil)</h3>
<p><img data-src="figs/poisson2d-swap9ew.svg" title="Communication pattern for 9 point stencil" style="width:30.0%" /></p>
<p>Communicate in two phases (<span class="alert">EW</span>, NS) to get corners.</p>
<aside class="notes">
<p>To do one step of Jacobi in the nine-point case, we would usually communicate in two phases. In the first phase, we would have each processor exchange ghost cells with the neighboring processors to the east and west.</p>
</aside>
</section>

<section>
<h3>Parallel version (9 point stencil)</h3>
<p><img data-src="figs/poisson2d-swap9ns.svg" title="Communication pattern for 9 point stencil" style="width:30.0%" /></p>
<p>Communicate in two phases (EW, <span class="alert">NS</span>) to get corners.</p>
<aside class="notes">
<p>Then, in the second phase, we would have each processor exchange chost cells with the neighboring processors to the north and south – including ghost cells for the corner data just communicated in the east-west exchange! If you stare at this for a while, you can convince yourself that at the end of these two phases, everyone gets all the data they need to proceed, including the corners.</p>
<p>The same trick works in 3D, of course. We would just need three phases instead of two.</p>
</aside>
</section>

<section>
<h3>Gauss-Seidel on 2D Poisson</h3>
<pre><code>for step = 1:nsteps

  for i = 2:n-1
    for j = 2:n-1
      u(i,j) = ...
        ( u(i,j+1) + u(i,j-1) + ...
          u(i-1,j) + u(i+1,j) )/4 - ...
        h^2*f(i,j)/4;
    end
  end

end</code></pre>
<p>Bottom values depend on top; how to parallelize?</p>
<aside class="notes">
<p>All right, so much for Jacobi on this model problem. Let’s turn now to Gauss-Seidel. The Gauss-Seidel update is identical to the Jacobi update, except that instead of always using the guess from a previous step, we do our relaxation step at point i based on the best current estimate we have for the neighbors of point i. If some of those neighbors got updated in the current phase, all the better! Of course, now the order in which updates occur matters, and that seems like it’s bad news for parallelism.</p>
</aside>
</section>

<section>
<h3>Red-Black Gauss-Seidel</h3>
<p><img data-src="figs/checker6x6.svg" title="Checkerboard pattern for red-black Gauss-Seidel" style="width:30.0%" /></p>
<p>Red depends only on black, and vice-versa.<br />
Generalization: multi-color orderings</p>
<aside class="notes">
<p>There’s a trick here, which is that we aren’t really told from on high that we have to update the nodes in any particular order. Going through the grid in row-major or column-major order seems like it might be natural, but going through in checkerboard order reveals a lot more parallelism. In a checkerboard or chessboard, the neighbors of a red square are all black, and the neighbors of a black square are all red. So a Gauss-Seidel sweep can update all the black squares in whatever order it wants, then all the red squares in whatever order, provided that there is enough synchronization so that the black updates come before the red in any given phase.</p>
<p>Red-black ordering is a special case of the more general idea of multi-color ordering.</p>
</aside>
</section>

<section>
<h3>Red black Gauss-Seidel step</h3>
<pre><code>  for i = 2:n-1
    for j = 2:n-1
      if mod(i+j,2) == 0
        u(i,j) = ...
      end
    end
  end

  for i = 2:n-1
    for j = 2:n-1
      if mod(i+j,2) == 1
        u(i,j) = ...
      end
    end
  end</code></pre>
<aside class="notes">
<p>Here’s how I would code a rec-black Gauss-Seidel step for the 2D Poisson problem, by the way. At least, this is how I would do it in MATLAB.</p>
</aside>
</section>

<section>
<h3>Parallel red-black Gauss-Seidel sketch</h3>
<p>At each step</p>
<ul>
<li>Send black ghost cells</li>
<li>Update red cells</li>
<li>Send red ghost cells</li>
<li>Update black ghost cells</li>
</ul>
<aside class="notes">
<p>If I wanted to parallelize, my code wouldn’t change that much. I’d just have messages or a barrier at the halfway step for each iteration, as well as at the full step. Put differently: I would alternate back and forth between updating just black cells and just red cells until I decided I’d had enough or I found that the problem converged.</p>
</aside>
</section>

<section>
<h3>More Sophistication</h3>
<ul>
<li>Successive over-relaxation (SOR): extrapolate G-S</li>
<li>Block Jacobi: <span class="math inline">\(M\)</span> a block diagonal matrix from <span class="math inline">\(A\)</span>
<ul>
<li>Other block variants similar</li>
</ul></li>
<li>Alternating Direction Implicit (ADI): alternately solve on vertical lines and horizontal lines</li>
<li>Multigrid</li>
</ul>
<p>Mostly the opening act for <em>Krylov methods</em>.</p>
<aside class="notes">
<p>And there’s a lot more beyond just Jacobi and Gauss-Seidel. The SOR method, for example, takes the Gauss-Seidel step and moves a bit farther in that direction. How best to scale the step is determined by spectral properties of the matrix, and the optimal step is usually hard to compute. Then there are block Jacobi and block Gauss-Seidel, which look exactly like Jacobi and Gauss-Seidel on a blocked version of the matrix. Methods like ADI solve a series of 1D problems, but alternate between relaxing on vertical stripes and horizontal stripes through the domain. This works surprisingly well, and something like this is the approach used in optimization methods like ADMM. At the extreme case, we have iterations like multigrid that can – in principle – reduce the error by a fixed fraction in a fixed number of iterations (independent of mesh size) for problems like 2D Poisson.</p>
<p>But even multigrid is often not used on its own any more, and even more so with the other methods. Instead, these stationary iterations are used as the basis of <em>preconditioners</em> for Krylov subspace methods. But we will take up that thread in Thursday’s slide decks.</p>
</aside>
</section>

