---
title: MPI programming
layout: slides
audio: 2020-10-06-mpi
---

<section>
  <h1><a href="https://www.cs.cornell.edu/courses/cs5220/2020fa/">CS 5220</a></h1>
  <h2>Applications of Parallel Computers</h2>
  <h3>MPI Programming</h3>
  <p>
    <small>Prof <a href="http://www.cs.cornell.edu/~bindel">David Bindel</a></small>
  </p>
  <p>Please click the play button below.</p>
</section>

  
<section>
  <h3>Message passing programming</h3>
  <p>Basic operations:</p>
  <ul>
    <li>Pairwise messaging: send/receive</li>
    <li>Collective messaging: broadcast, scatter/gather</li>
    <li>Collective computation: parallel prefix (sum, max)</li>
    <li>Barriers (no need for locks!)</li>
    <li>Environmental inquiries (who am I? do I have mail?)</li>
  </ul>
  <p>(Much of what follows is adapted from Bill Gropp.)</p>
</section>


<section>
  <h3>MPI</h3>
  <ul>
    <li>Message Passing Interface</li>
    <li>An interface spec — many implementations<br/>
      (OpenMPI, MPICH, MVAPICH, Intel, ...)
    </li>
    <li>Bindings to C, C++, Fortran</li>
  </ul>
</section>


<section>
  <h3>MPI</h3>

  <ul>
    <li>Version 1.0 in 1994, 2.2 in 2009, 3.0 in 2012</li>
    <li>MPI 3 goodies:
      <ul>
        <li>Nonblocking collectives</li>
        <li>Neighborhood collectives</li>
        <li>RMA and one-sided comm</li>
      </ul>
    </li>
    <li>Will stick to MPI-2 today</li>
  </ul>
</section>


<section>
<h3>Hello world</h3>
<pre><code>#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;

int main(int argc, char** argv) {
    int rank, size;
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
    printf(&quot;Hello from %d of %d\n&quot;, rank, size);
    MPI_Finalize();
    return 0;
}</code></pre>
</section>


<section>
  <h3>Building, queueing, running</h3>

  <p>Several steps to actually run</p>
  <pre>
mpicc -o foo.x foo.c   # Compile the program
sbatch foo.sub         # Submit to queue (SLURM)
# mpirun -n 2 ./foo.x  # (in foo.sub) Run on 2 procs
  </pre>
  <p>This is all platform-specific.</p>
</section>


<section>
  <h3>Communicators</h3>
  <ul>
    <li>Processes form <em>groups</em></li>
    <li>Messages sent in <em>contexts</em>
      <ul>
        <li>Separate communication for libraries</li>
    </ul></li>
    <li>Group + context = communicator</li>
    <li>Identify process by rank in group</li>
    <li>Default is <tt>MPI_COMM_WORLD</tt></li>
  </ul>
</section>


<section>
  <h3>Sending and receiving</h3>
  <p>Need to specify:</p>
  <ul>
    <li>What’s the data?
      <ul>
        <li>Different machines use different encodings (e.g. endian-ness)</li>
        <li>$\implies$ “bag o’ bytes” model is inadequate</li>
    </ul></li>
    <li>How do we identify processes?</li>
    <li>How does receiver identify messages?</li>
    <li>What does it mean to “complete” a send/recv?</li>
  </ul>
</section>


<section>
  <h3>MPI datatypes</h3>
  <p>Message is (address, count, datatype). Allow:</p>
  <ul>
    <li>Basic types (<tt>MPI_INT</tt>, <tt>MPI_DOUBLE</tt>)</li>
    <li>Contiguous arrays</li>
    <li>Strided arrays</li>
    <li>Indexed arrays</li>
    <li>Arbitrary structures</li>
  </ul>
  <p>Complex data types may hurt performance?</p>
</section>


<section>
  <h3>MPI tags</h3>
  <p>Use an integer <em>tag</em> to label messages</p>
  <ul>
    <li>Help distinguish different message types</li>
    <li>Can screen messages with wrong tag</li>
    <li><tt>MPI_ANY_TAG</tt> is a wildcard</li>
  </ul>
</section>


<section>
<h3>MPI Send/Recv</h3>
<p>Basic blocking point-to-point communication:</p>
<pre><code>int
MPI_Send(void *buf, int count,
         MPI_Datatype datatype,
         int dest, int tag, MPI_Comm comm);

int
MPI_Recv(void *buf, int count,
         MPI_Datatype datatype,
         int source, int tag, MPI_Comm comm,
         MPI_Status *status);</code></pre>
</section>


<section>
  <h3>MPI send/recv semantics</h3>
  <ul>
    <li>Send returns when data gets to <em>system</em>
      <ul>
        <li>... might not yet arrive at destination!</li>
    </ul></li>
    <li>Recv ignores messages that mismatch source/tag
      <ul>
        <li><tt>MPI_ANY_SOURCE</tt> and <tt>MPI_ANY_TAG</tt> wildcards</li>
    </ul></li>
    <li>Recv status contains more info (tag, source, size)</li>
  </ul>
</section>


<section>
<h3>Ping-pong pseudocode</h3>
<p>Process 0:</p>
<pre><code>for i = 1:ntrials
  send b bytes to 1
  recv b bytes from 1
end</code></pre>
<p>Process 1:</p>
<pre><code>for i = 1:ntrials
  recv b bytes from 0
  send b bytes to 0
end</code></pre>
</section>


<section>
<h3>Ping-pong MPI</h3>
<pre><code>void ping(char* buf, int n, int ntrials, int p)
{
    for (int i = 0; i &lt; ntrials; ++i) {
        MPI_Send(buf, n, MPI_CHAR, p, 0,
                 MPI_COMM_WORLD);
        MPI_Recv(buf, n, MPI_CHAR, p, 0,
                 MPI_COMM_WORLD, NULL);
    }
}</code></pre>
<p>(Pong is similar)</p>
</section>


<section>
<h3>Ping-pong MPI</h3>
<pre><code>for (int sz = 1; sz &lt;= MAX_SZ; sz += 1000) {
    if (rank == 0) {
        clock_t t1, t2;
        t1 = clock();
        ping(buf, sz, NTRIALS, 1);
        t2 = clock();
        printf(&quot;%d %g\n&quot;, sz,
               (double) (t2-t1)/CLOCKS_PER_SEC);
    } else if (rank == 1) {
        pong(buf, sz, NTRIALS, 0);
    }
}</code></pre>
</section>


<section>
  <h3>Running the code</h3>
  <p>On my laptop (OpenMPI)</p>
  <pre><code>mpicc -std=c99 pingpong.c -o pingpong.x
      mpirun -np 2 ./pingpong.x</code></pre>
  <p>Details vary, but this is pretty normal.</p>
</section>


<section>
  <h3>Blocking and buffering</h3>
  <img src="{{ "lec/figs/mpi-send-buf.svg" | relative_url }}"
       alt="Diagram of buffered message send"
       width="80%"/>
  <p>Block until data “in system” — maybe in a buffer?</p>
</section>


<section>
  <h3>Blocking and buffering</h3>
  <img src="{{ "lec/figs/mpi-send-nbuf.svg" | relative_url }}"
       alt="Diagram of unbuffered message send"
       width="80%"/>
  <p>Alternative: don’t copy, block until done.</p>
</section>


<section>
  <h3>Problem 1: Potential deadlock</h3>
  <img src="{{ "lec/figs/mpi-send-dead.svg" | relative_url }}"
       alt="Diagram of deadlock scenario"
       width="30%"/>
  <p>Both processors wait to send before they receive!<br />
    May not happen if lots of buffering on both sides.</p>
</section>


<section>
  <h3>Solution 1: Alternating order</h3>
  <img src="{{ "lec/figs/mpi-send-dead.svg" | relative_url }}"
       alt="Diagram of breaking deadlock scenario by alternate send/recv"
       width="30%"/>
  <p>Could alternate who sends and who receives.</p>
</section>


<section>
  <h3>Solution 2: Combined send/recv</h3>
  <img src="{{ "lec/figs/mpi-sendrecv.svg" | relative_url }}"
       alt="Diagram of send-receive primitive"
       width="60%"/>
  <p>Common operations deserve explicit support!</p>
</section>


<section>
<h3>Combined sendrecv</h3>
<pre><code>MPI_Sendrecv(sendbuf, sendcount, sendtype,
             dest, sendtag,
             recvbuf, recvcount, recvtype,
             source, recvtag,
             comm, status);</code></pre>
<p>Blocking operation, combines send and recv to avoid deadlock.</p>
</section>


<section>
  <h3>Problem 2: Communication overhead</h3>
  <img src="{{ "lec/figs/mpi-sendrecv-waiting.svg" | relative_url }}"
       alt="Diagram of nonblocking send primitive"
       width="60%"/>
  <p>Partial solution: nonblocking communication</p>
</section>


<section>
  <h3>Blocking vs non-blocking</h3>
  <ul>
    <li><tt>MPI_Send</tt> and <tt>MPI_Recv</tt> are <em>blocking</em>
      <ul>
        <li>Send does not return until data is in system</li>
        <li>Recv does not return until data is ready</li>
        <li>Cons: possible deadlock, time wasted waiting</li>
    </ul></li>
    <li>Why blocking?
      <ul>
        <li>Overwrite buffer during send $\implies$ evil!</li>
        <li>Read buffer before data ready $\implies$ evil!</li>
    </ul></li>
  </ul>
</section>

<section>
  <h3>Blocking vs non-blocking</h3>
  <p>Alternative: <em>nonblocking</em> communication</p>
  <ul>
    <li>Split into distinct initiation/completion phases</li>
    <li>Initiate send/recv and promise not to touch buffer</li>
    <li>Check later for operation completion</li>
  </ul>
</section>


<section>
  <h3>Overlap communication and computation</h3>
  <img src="{{ "lec/figs/mpi-isendrecv.svg" | relative_url }}"
       alt="Diagram of nonblocking send/receive primitive"
       width="80%"/>
</section>


<section>
<h3>Nonblocking operations</h3>
<p>Initiate message:</p>
<pre><code>MPI_Isend(start, count, datatype, dest
          tag, comm, request);
MPI_Irecv(start, count, datatype, dest
          tag, comm, request);</code></pre>
<p>Wait for message completion:</p>
<pre><code>MPI_Wait(request, status);</code></pre>
<p>Test for message completion:</p>
<pre><code>MPI_Test(request, status);</code></pre>
</section>


<section>
<h3>Multiple outstanding requests</h3>
<p>Sometimes useful to have multiple outstanding messages:</p>
<pre><code>MPI_Waitall(count, requests, statuses);
MPI_Waitany(count, requests, index, status);
MPI_Waitsome(count, requests, indices, statuses);</code></pre>
<p>Multiple versions of test as well.</p>
</section>


<section>
  <h3>Other send/recv variants</h3>
  <p>Other variants of <tt>MPI_Send</tt></p>
  <ul>
    <li><tt>MPI_Ssend</tt> (synchronous) – do not complete until receive has begun</li>
    <li><tt>MPI_Bsend</tt> (buffered) – user provides buffer (via <tt>MPI_Buffer_attach</tt>)</li>
    <li><tt>MPI_Rsend</tt> (ready) – user guarantees receive has already been posted</li>
    <li>Can combine modes (e.g. <tt>MPI_Issend</tt>)</li>
  </ul>
  <p><tt>MPI_Recv</tt> receives anything.</p>
</section>


<section>
  <h3>Another approach</h3>
  <ul>
    <li>Send/recv is one-to-one communication</li>
    <li>An alternative is one-to-many (and vice-versa):
      <ul>
        <li><em>Broadcast</em> to distribute data from one process</li>
        <li><em>Reduce</em> to combine data from all processors</li>
        <li>Operations are called by all processes in communicator</li>
    </ul></li>
  </ul>
</section>


<section>
<h3>Broadcast and reduce</h3>
<pre><code>MPI_Bcast(buffer, count, datatype,
          root, comm);
MPI_Reduce(sendbuf, recvbuf, count, datatype,
           op, root, comm);</code></pre>
<ul>
<li>buffer is copied from root to others</li>
<li>recvbuf receives result only at root</li>
<li>op $\in \{$ <tt>MPI_MAX</tt>, <tt>MPI_SUM</tt>, …$\}$</li>
</ul>
</section>


<section>
<h3>Example: basic Monte Carlo</h3>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;mpi.h&gt;
int main(int argc, char** argv) {
    int nproc, myid, ntrials = atoi(argv[1]);
    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;nproc);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_id);
    MPI_Bcast(&amp;ntrials, 1, MPI_INT,
              0, MPI_COMM_WORLD);
    run_trials(myid, nproc, ntrials);
    MPI_Finalize();
    return 0;
}</code></pre>
</section>


<section>
<h3>Example: basic Monte Carlo</h3>
<p>Let sum[0] = $\sum_i X_i$ and sum[1] = $\sum_i X_i^2$.</p>
<pre><code>void run_mc(int myid, int nproc, int ntrials) {
    double sums[2] = {0,0};
    double my_sums[2] = {0,0};
    /* ... run ntrials local experiments ... */
    MPI_Reduce(my_sums, sums, 2, MPI_DOUBLE,
               MPI_SUM, 0, MPI_COMM_WORLD);
    if (myid == 0) {
        int N = nproc*ntrials;
        double EX = sums[0]/N;
        double EX2 = sums[1]/N;
        printf(&quot;Mean: %g; err: %g\n&quot;,
               EX, sqrt((EX*EX-EX2)/N));
    }
}</code></pre>
</section>


<section>
  <h3>Collective operations</h3>
  <ul>
    <li>Involve all processes in communicator</li>
    <li>Basic classes:
      <ul>
        <li>Synchronization (e.g. barrier)</li>
        <li>Data movement (e.g. broadcast)</li>
        <li>Computation (e.g. reduce)</li>
    </ul></li>
  </ul>
</section>


<section>
  <h3>Barrier</h3>
  <pre><code>MPI_Barrier(comm);</code></pre>
  <p>Not much more to say. Not needed that often.</p>
</section>


<section>
  <h3>Broadcast</h3>
  <img src="{{ "lec/figs/mpi-bcast.svg" | relative_url }}"
       alt="Diagram of MPI broadcast"
       width="80%"/>
</section>


<section>
  <h3>Scatter/gather</h3>
  <img src="{{ "lec/figs/mpi-gather.svg" | relative_url }}"
       alt="Diagram of MPI gather"
       width="80%"/>
</section>


<section>
  <h3>Allgather</h3>
  <img src="{{ "lec/figs/mpi-allgather.svg" | relative_url }}"
       alt="Diagram of MPI all-gather"
       width="80%"/>
</section>


<section>
  <h3>Alltoall</h3>
  <img src="{{ "lec/figs/mpi-alltoall.svg" | relative_url }}"
       alt="Diagram of MPI all-to-all"
       width="80%"/>
</section>


<section>
  <h3>Reduce</h3>
  <img src="{{ "lec/figs/mpi-reduce.svg" | relative_url }}"
       alt="Diagram of MPI reduce"
       width="80%"/>
</section>


<section>
  <h3>Scan</h3>
  <img src="{{ "lec/figs/mpi-scan.svg" | relative_url }}"
       alt="Diagram of MPI scan"
       width="80%"/>
</section>


<section>
  <h3>The kitchen sink</h3>
  <ul>
    <li>In addition to above, have vector variants (v suffix), more All variants (Allreduce), Reduce_scatter, ...</li>
    <li>MPI3 adds one-sided communication (put/get)</li>
    <li>MPI is <em>not</em> a small library!</li>
    <li>But a small number of calls goes a long way
      <ul>
        <li>Init/Finalize</li>
        <li>Get_comm_rank, Get_comm_size</li>
        <li>Send/Recv variants and Wait</li>
        <li>Allreduce, Allgather, Bcast</li>
    </ul></li>
  </ul>
</section>


