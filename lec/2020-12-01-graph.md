% CS 5220: Parallel Graph Algorithms
% David Bindel

### Plan

- Some background on graphs
- Applications and building blocks
- Basic parallel graph algorithms
- Representations and performance
- Graphs and LA
- Frameworks

::: notes
We have a bit of a potpourri today.  After reminding you about
different types of graphs and their applications to various problems,
we'll talk about basic parallel graph algorithms.  This is different
from our earlier discussion of graph theory for load balancing, in
that this time we are talking about actually parallelizing the graph
computations instead of using them to reason about another parallel
computation!  We'll do a couple examples here to highlight some common
ideas that show up when transitioning from the serial to the parallel
settings.  In particular, randomization shows up often.

In the back half of the latter, we'll switch from talking about
algorithms to talking about some of the nuts-and-bolts of making
things run fast on modern machines.  That means understanding
representations of graphs that we might want to use, and also
frameworks that allow us to program graph algorithms fast and at scale
(or that people think allow us to do so).
:::

### Graphs

Mathematically: $G = (V,E)$ where $E \subset V \times V$

-   Convention: $|V| = n$ and $|E| = m$
-   May be directed or undirected
-   May have weights $w_V : V \rightarrow \mathbb{R}$ or
    $w_E : E : \rightarrow \mathbb{R}$
-   May have other node or edge attributes as well
-   Path is $\left[ \, (u_i,u_{i+1}) \, \right]_{i=1}^\ell \in E^*$, sum
    of weights is length
-   Diameter is $\max_{s, t \in V} d(s, t)$

::: notes
First, a reminder: a graph consists of vertices (also called nodes)
and edges (which are just pairs of vertices).  If the edge order
matters, we call the graph directed; otherwise, it is undirected.  We
can attach weights or other attributes to either the vertices or edges.
A path through the graph is just a sequence of edges that share
endpoints.  The length of the path is the sum of the edge weights, and
the distance between two vertices in a graph is the length of the
shortest path that connects them.  The longest shortest path in the
graph is called the graph diameter.
:::

### Generalizations

-   Hypergraph (edges in $V^d$)
-   Multigraph (multiple copies of edges)

::: notes
There are some natural generalizations of graphs that come up often.
In particular, a hypergraph is like a graph, but with edges (pairs of
vertices) generalized to hyper-edges (longer tuples of vertices).
We mentioned hypergraphs very briefly when we talked about graph
partitioning.

A multigraph still just has normal edges, but we're allowed to have
more than one copy of an edge.

We aren't going to discuss hypergraphs or multigraphs further in this
class, though if any of you want to hear more about hypergraphs and
tensors, you should consider taking 6241 with me in the spring!
:::

### Types of graphs

![](figs/pg-chain.svg){width=50%}

::: notes
Maybe the simplest graph is a chain.  The diameter here is one minus
the number of vertices -- longest diameter we can get without having a
disconnected graph.  Also, there's only one path between any two vertices.
:::

### Types of graphs

![](figs/pg-tree.svg){width=50%}

::: notes
The property that there's only one path between any two vertices
characterizes a tree.  The 1D chain was a tree; this binary tree
is probably a more specific example.
:::

### Types of graphs

![](figs/pg-mesh.svg){width=50%}

::: notes
One step up from a tree or chain, we have very regular
"low-dimensional" graphs like this mesh.  This type of mesh has
diameter at least proportional to sqrt(n), and can be cut into pieces
with separators of size sqrt(n).
:::

### Types of graphs

![](figs/pg-Lmesh.svg){width=50%}

::: notes
Of course, we can get a little fancier than a completely regular mesh
and still maintain lots of the important properties.
:::

### Types of graphs

![](figs/part_esep_spectral.svg){width=50%}

::: notes
Graphs don't have to be regular meshes to still "look" fundamentally
2D.  Planar graphs with nearest neighbor connectivity, like this one,
have the same types of properties, like (relatively) small separators,
relatively high diameters, and (often) bounded vertex degrees.
:::

### Types of graphs

![](figs/Karate_new.svg){width=50%}

::: notes
In contrast to these fundamentally low-dimensional graphs are the
types of things that we see with social networks.  This is a rendering
of the so-called Zachary karate club network, often used as a
demonstration problem for social network analysis.  The graph
describes the friendship relations within a karate club that split in
half.  As you can see, there is a lot of variation in the degree, from
the two super-connected leaders to that one poor lonely guy who only
ever talked to his teacher (bottom of the cluster on the right).
Social networks tend to not have highly variable node connectivity;
they often don't have good separators; and they are pretty low-dimensional.
:::

### Types of graphs

Many possible structures:

-   Lines and trees
-   Completely regular grids
-   Planar graphs (no edges need cross)
-   Low-dimensional Euclidean
-   Power law graphs
-   \...

Algorithms are not one-size-fits-all!

::: notes
So, we have a lot of possible structures, running from the line graph
to these super-connected graphs that show up in social network
analysis.  The coarse scale properties of these graphs are pretty
different, so it shouldn't surprise you that they deserve different
algorithms!
:::

### Ends of a spectrum

                      Planar               Power law
  ------------------- -------------------- ----------------------------------------
        Vertex degree Uniformly small      $P(\mathrm{deg} = k) \sim k^{-\gamma}$
               Radius $\Omega(\sqrt{n})$   Small
             Edge sep $O(\sqrt{n})$        nothing small
         Linear solve Direct OK            Iterative
                 Apps PDEs                 Social networks

Calls for different methods!

::: notes
Over-simplifying a bit, we might say that most of the world sits
between the extremes represented by nice regular planar graphs and
power-law graphs.  For planar graphs, we have small degree,
diameters no smaller than sqrt(n), and separators no bigger than
sqrt(n).  For power law graphs, we have a highly variable degree
distribution, with bigger graphs showing higher degree nodes;
the diameters are small; and there are no small edge separators.

When we're doing linear algebra, a direct solver works pretty well for
2D PDE discretizations, where the matrix has the structure of a
regular planar graph; for matrices associated with graphs at the other
extreme, we usually use iterative methods.  It turns out that we
generally use different graph algorithms in the two cases, too.
:::

### Applications: Routing and shortest paths

![image](figs/ithaca2nyc.png){width="50%"}

::: notes
All right.  I've rattled on about this range of different types of
graphs enough without saying what we want to *do* with them.
So what do we want to do?

One thing that we might want to do is to find shortest paths,
or at least good-enough paths.  The standard example might be
navigation systems, where we want directions for the shortest path
between two points.  The road network is a very two-dimensional
sort of graph, which we generally use in algorithms.
:::

### Applications: Traversal, ranking, clustering

-   Web crawl / traversal
-   PageRank, HITS
-   Clustering similar documents

::: notes
Ranking and clustering in social and information networks generally
involves computing over graphs.  Often, the computations are linear
algebraic in nature: linear solves and eigenvalue computations.
We often use these for small-world types of graphs.
:::

### Applications: Sparse solvers

[![image](figs/Pothen-barth5.jpg){width="50%"}](http://yifanhu.net/GALLERY/GRAPHS/GIF_SMALL/Pothen@barth5.html)

-   Ordering for sparse factorization
-   Partitioning
-   Coarsening for AMG

::: notes
We've already had a couple lectures as well in which we discussed the
role of graph algorithms for graph partitioning and for finding small
separators for sparse direct solvers.  We usually apply these
techniques when we're dealing with graphs that have good separators,
often coming from physics problems.
:::

### Applications: Dimensionality reduction

[![image](figs/web1-isomap.jpg){width="80%"}](http://web.mit.edu/cocosci/isomap/isomap.html)

::: notes
And then there is dimensionality reduction, where we might try to use
a graph to uncover low-dimensional structure in high-dimensional
geometric data.  If you want to see lots of tricks of this sort, I
again encourage you to take my spring course!
:::

### Common building blocks

-   Traversals
-   Shortest paths
-   Spanning tree
-   Flow computations
-   Topological sort
-   Coloring
-   \...

\... and most of sparse linear algebra.

::: notes
The building blocks for all of these applications are linear algebra
(of course) and the types of things that you would typically see in
early courses on algorithms: graph traversals, shortest path finding,
spanning tree computations, flow computations, topological sorting,
graph coloring, and so on.

We've talked a lot about parallelizing linear algebra.  Let's talk a
little about parallelizing some of these other building blocks for a
while.
:::

### Over-simple models

Let $t_p =$ idealized time on $p$ processors

-   $t_1 =$ work
-   $t_\infty =$ span (or depth, or critical path length)

Don't bother with parallel DFS! Span is $\Omega(n)$.\
Let's spend a few minutes on more productive algorithms\...

::: notes
An overly simplified way of thinking about the potential parallelism
in an algorithm is to think what would happen if you had an infinite
number of processors and communication was free.  In that case, the
time to complete a computation would be determined by the longest
dependency chain within the computation.  This is sometimes called the
span, or depth, or critical path length.

As an example: things that look like depth first search tend to be
terrible for parallelism, because the span is proportional to the
problem size!  So we won't spend time thinking about depth first
search.  Let's instead think about breadth first search.
:::

### Serial BFS

- Push seed node onto queue and mark
- While Q nonempty
  - Pop node from queue
  - Visit node
  - Push unmarked neighbors on queue
  - Mark all neighbors

::: notes
Breadth-first search visits nodes in an unweighted graph in ascending
order by distance from a start node.  The serial version of
breadth-first search usually involves a queue that contains a
"frontier" of nodes that have been seen, but not yet visited.  In the
process of visiting all nodes at distance d from the start, we queue
up all nodes at distance d+1 to be visited next.  Hence, the algorithm
proceeds layer by layer.
:::

### Parallel BFS

Simple idea: parallelize across frontiers

-   Pro: Simple to think about
-   Pro: Lots of parallelism with small radius?
-   Con: What if frontiers are small?

::: notes
The simplest way to think about parallelizing BFS is to parallelize
exploration from a frontier at depth d to one at depth d+1.  This is
simple enough to think about, except for the part where we have to
deal with a parallel data structure of some sort to accumulate the
nodes at step d+1 without redundancy.  Unfortunately, if we are
dealing with a large-diameter graph with lots of good separators, the
frontiers might never get all that big, and so we might not have
enough parallel work to make us happy.  What should we do then?
:::

### Parallel BFS: Ullman-Yannakakis

Assuming a high-diameter graph:

-   Form set $S$ with start + random nodes, $|S| = \Theta(\sqrt{n} \log n)$ 
    - long shortest paths go through $S$ w.h.p.
-   Take $\sqrt{n}$ steps of BFS from each seed in $S$
-   Form aux graph for distances between seeds
-   Run all-pairs shortest path on aux graph

OK, but what if diameter is not large?

::: notes
An idea due to Ullman and Yanakakkis involves parallel exploration of
different parts of the graph, followed by a method for connecting
things together.  The idea is to take the start node together with a
lot of other randomly selected nodes, and expand a small BFS of length
sqrt(n) from each of those seeds.  Then we show that, with high
probability, any shortest path that is longer than sqrt(n) must go
through one of the seeds.  From there, we only need to consider the
graph of seed-to-seed distances, which we can compute with an
all-pairs shortest path computation on an auxiliary graph.

The good thing about this algorithm is that it has lots of parallelism
and is relatively simple to code.  The bad thing is that the analysis
is not so intuitive, and it only works for graphs with large diameter.

So what should we do if we want to do breadth-first search in a "small
world" graph?
:::

### Serial BFS: Bottom-up

- Set $d[v] = \infty$ for all vertices
- Set $d[s] = 0$ for seed $s$
- Until $d$ stops changing
  - For each $u \in V$
    - $d[u] = \min(d[u], \min_{w \in N(u)} d[w]+1)$

::: notes
Let's go back to the serial setting, and talk about another way of
doing breadth-first search.  Before, we did a "top-down" exploration
of a search tree, pushing from each node out to its children.  But we
can also do a "bottom-up" exploration, checking at every step whether
a node could find a closer neighbor.  The algorithm is shown here:
until the distance stops changing, we sweep over all the nodes,
updating their distance to the min of the old distances and one more
than the min of their neighbor distances.  The sweep is embarassingly
parallel, and the number of iterations to converge is just the graph
diameter.
:::

### Parallel BFS

Key ideas:

-   At some point, switch from top-down expanding frontier ("are you my
    child?") to bottom-up checking for parents ("are you my parent?")
-   Use 2D blocking of adjacency

::: notes
The fastest parallel BFS approaches out there combine the top-down
approach with the bottom-up approach, switching at some point in the
process.  They also do a linear-algebra-style tiling of the adjacency
matrix.  We'll revisit this later.
:::

### Single-source shortest path

Classic algorithm: Dijkstra

-   Dequeue closest point to frontier, expand frontier
-   Update priority queue of distances (in parallel)
-   Repeat

Or run serial Dijkstra from different sources for APSP.

::: notes
All right. Breadth-first search explores in order of distance from a
start node in an *unweighted* graph. What about the same order in a
*weighted* graph?  For that, we would usually use Dijkstra's
algorithm, which is very similar to BFS, except with a priority queue
ordered by distance to the already-visited points.  At each step, we
pull off the closest unvisited point, and update the distances for all
its (unprocessed) neighbors.  Like BFS, this is a very "top-down"
organization.
:::

### Alternate idea: label correcting

Initialize $d[u]$ with distance over-estimates to source

-   $d[s] = 0$
-   Repeatedly relax $d[u] := \min_{(v,u) \in E} d[v] + w(v,u)$

Converges (eventually) as long as all nodes visited repeatedly, updates
are atomic. If serial sweep in a consistent order, call it Bellman-Ford.

::: notes
The same "bottom-up" organization we described for BFS works here,
too.  In this general setting, it is called a label correcting method.
The classic label correcting method that repeatedly sweeps the
vertices in a consistent order is called Bellman-Ford; but for
convergence, we really only need to manage to visit all the nodes repeatedly.
:::

### Single-source shortest path: $\Delta$-stepping

Alternate approach: *hybrid* algorithm

-   Process a "bucket" at a time
-   Relax "light" edges (wt \< $\Delta$), might add to bucket
-   When bucket empties, relax "heavy" edges a la Dijkstra

::: notes
And, as in breadth first search, we can make a lot of progress with
parallel methods that combine the bottom-up and top-down approaches.
So called delta-stepping methods do label correcting on a bucket of
nodes connected by very lightweight edges, and alternates that with
Dijkstra-style handling of longer edges.

OK, I admit that isn't the best description of delta-stepping, but it
gets across the basic idea.
:::

### Maximal independent sets (MIS)

-   $S \subset V$ *independent* if none are neighbors.
-   *Maximal* if no others can be added and remain independent.
-   *Maximum* if no other MIS is bigger.
-   Maximum is NP-hard; maximal is easy (serial)

::: notes
Let's talk about another graph primitive that doesn't involve a
traversal.  An independent set in a graph is a subset of the vertices
in which no vertex is a neighbor.  A maximal independent set is one to
which no node can be added without violating independence.  The
maximum independent set is a largest maximal independent set, and
finding maximum independent sets is generally hard.  But maximal
independent sets are relatively easy.

Maximal independent sets are a good setup for other problems, too,
like graph coloring.
:::

### Simple greedy MIS

-   Start with $S$ empty
-   For each $v \in V$ *sequentially*, add $v$ to $S$ if possible.

::: notes
How would we find a maximal independent set in a serial setting?  Keep
adding vertices until we can't anymore!  This greedy method works just
fine --- but how could you parallelize it?  Doesn't seem so obvious.
:::

### Luby's algorithm

-   Init $S := \emptyset$
-   Init candidates $C := V$
-   While $C \neq \emptyset$
    -   Label each $v$ with a random $r(v)$
    -   For each $v \in C$ in parallel, if $r(v) <
              \min_{\mathcal{N}(v)} r(u)$
        -   Move $v$ from $C$ to $S$
        -   Remove neighbors from $v$ to $C$

Very probably finishes in $O(\log n)$ rounds.

::: notes
The clever trick here -- as in the Ullman-Yannakakis BFS algorithm --
is to introduce randomization.  This algorithm for maximal independent
sets, due to Luby, generally finishes in O(log n) rounds, where each
round involves running over the graph once (in parallel).  In each
round, we give all the nodes in a graph a random label.  Then we add
every node with a locally smallest label to the independent set, and
remove all the neighbors of every such node from the candidates to
join the independent set.  We finish when the set of candidates is empty.
:::

### Luby's algorithm (round 1)

![](figs/mis_luby1.svg){width=80%}

::: notes
Let's look at how this plays out with our potato graph.  In the first
round, each of the red nodes joins the independent set -- they have
labels smaller than the labels of their neighbors.  The gray nodes,
neighbors of the nodes in the independent set, are taken off the list
of candidates.  At the end of the round, only the two blue nodes in
the corner remain as candidates.
:::

### Luby's algorithm (round 2)

![](figs/mis_luby2.svg){width=80%}

::: notes
Now, in round two, we add one of the corner nodes and finish the
algorithm.  The four indicated red nodes are a maximal independent
set in this graph.
:::

### A fundamental problem

Many graph ops are

-   Computationally cheap (per node or edge)
-   Bad for locality

**Memory bandwidth** as a limiting factor.

::: notes
At this point in the lecture, perhaps you're starting to get
suspicious.  All the graph theory and randomization ideas and
bottom-up-vs-top-down stuff sounds very much like what you'd see in an
algorithms class -- or maybe a parallel algorithms class -- but it's
missing a lot of what we often focus on when we do HPC.
What about the data structures?  Issues of memory locality and
computational intensity?

Well, we'll talk about the data structures in a moment.  In terms of
memory locality and computational intensity, though, the news is
mostly bad.  Many graph operations are computationally cheap per node
or edge, requiring only one or a small number of visits before
completing (and with each of the visits only involving cheap
operations).  So the main bottleneck is generally not computing on the
graph, but getting the graph out of memory.
:::

### Big data?

Consider:

-   323 million in US (fits in 32-bit int)
-   About 350 Facebook friends each
-   Compressed sparse row: about 450 GB

Topology (no metadata) on one big cloud node\...

::: notes
There's good news here, though, too.  Compared to many of the problems
we're used to dealing with even "big" graphs may not be that big.
For example, consider the Facebook social graph in the US (or the
graph for a comparable network -- I don't mean to be old-fashioned
here, I just happen to know more numbers in this case).  We can
identify everyone in the US by a 32 bit int, and if we just store
connectivity (assuming about 350 friends per person), the entire
topology can be represented in compressed sparse row form with less
than half a terabyte of memory.  That may seem like a lot, but it
easily fits on a single big machine; maybe not your laptop, but a
cloud node that you can easily get time on.

So -- is CSR the Right Way to represent things?  The answer,
of course, is "it depends."  Let's talk about a few options.
:::

### Graph rep: Adj matrix

![](figs/part_matrix.svg){width=40%}

Pro: efficient for dense graphs\
Con: wasteful for sparse case\...

::: notes
One possible representation of a graph would be as a dense matrix.  In
fact, if more than a couple percent of the edges are present, this
probably is *the* right way to do the representation!  In the
unweighted case, we only need one bit per edge, though we need more in
the weighted case.  The dense adjacency matrix representation is also
cheap to update, and there are sometimes when that property is useful.

In the sparse case, storing a lot of explicit zeros is wasteful -- we
just want to store where the edges are, not a marker for the absence
of other edges.
:::

### Graph rep: Coordinate

-   Tuples: $(i,j,w_{ij})$
-   Pro: Easy to update
-   Con: Slow for multiply

::: notes
Another possibility is to store a graph as an edge list.  That is,
we keep tuples $(i,j)$ (and possibly a weight) for each edge in the
graph.  This is again easy to update, and it's more compact than a
dense adjacency matrix in the sparse case.  But it's not great for
things that look like matrix-vector products, nor is it necessarily
great for iterating through all the neighbors of a particular node.
:::

### Graph rep: Adj list

-   Linked lists of adjacent nodes
-   Pro: Still easy to update
-   Con: May cost more to store than coord?

::: notes
Another standard representation is the adjacency list.  Here, each
list has a linked list of neighbors in the graph.  This is still easy
to update, and it's easy to make nearest-neighbor queries with this
structure, but it will generally cost more to store than the simple coordinate
form.
:::

### Graph rep: CSR

![](figs/csr-demo.svg){width=80%}

Pro: traversal? Con: updates

::: notes
And then, of course, there is the compressed sparse row representation
that we've discussed before in the context of sparse matrices.  This
is a pretty nice data structure when it comes to fast traversal;
the Achilles heel is that it is painful if we ever have to add edges.
:::

### Graph rep: implicit

-   Idea: Never materialize a graph data structure
-   Key: Provide traversal primitives
-   Pro: Explicit rep'n sometimes overkill for one-off graphs?
-   Con: Hard to use canned software (except NLA?)

::: notes
Finally, there's the option of not keeping any data structure at all!
As long as we provide some primitives that let us traverse edges, it
doesn't matter if we have an explicit representation in memory.
Of course, this type of limited functional interface might be less
than we want for some algorithms -- except, perhaps, if we just need a
matrix-vector product.
:::

### Graph algorithms and LA

-   Really is standard LA
    -   Spectral partitioning and clustering
    -   PageRank and some other centralities
    -   "Laplacian Paradigm" (Spielman, Teng, others\...)
-   Looks like LA
    -   Floyd-Warshall
    -   Breadth-first search?

::: notes
We have been flirting throughout the lecture with the relationship
between graph theory and linear algebra.  Let's take a moment now to
clarify this relationship.

Lots of graph operations that we care about really are based on
standard linear algebra over the real numbers.  This includes various
algorithms for partitioning, clustering, and ranking, as well as
anything that can be built on the "graph Laplacian paradigm"
popularized by Spielman, Teng, and others.

At the same time, there are also operations that end up looking
suspiciously like linear algebra in structure, even if the details
aren't quite right.  Examples include the Floyd-Warshall algorithm for
all-pairs shortest path, or the bottom-up breadth-first search
algorithm.  Well, it turns out that these may not exactly be linear
algebra, but they aren't exactly not linear algebra, either.

Perhaps that deserves some more explanation...
:::

### Graph algorithms and LA

*Semirings* have $\oplus$ and $\otimes$ s.t.

-   Addition is commutative+associative with a 0
-   Multiplication is associative with identity 1
-   Both are distributive
-   $a \otimes 0 = 0 \otimes a = 0$
-   But no subtraction or division

Technically *modules* over semirings

::: notes
In abstract algebra, we have vector spaces over fields.  The most
frequently-used fields are the real and complex numbers or subsets
thereof (the rationals, the algebraic numbers, etc), but there are
also plenty of applications of finite field arithmetic.  But we can
still do a lot with a weaker abstraction than a field called a
semi-ring.  A semi-ring has addition and multiplication, each with the
usual identity element.  Addition is commutative and associative,
multiplication is associative (but maybe not commutative), and the two
together satisfy the usual distributive law.  But we have no
subtraction (unlike a ring), and no division (unlike a division ring
or a field).

We have something like a vector space in this setting, too.  It just
happens to be called a module rather than a vector space in the case
when you don't have an underlying field.

Have I jumped off the deep end and started talking pure math when I
should be telling you about HPC?  Well, bear with me for one more
slide, and perhaps things will become more clear.
:::

### Graph algorithms and LA

Example: min-plus

-   $\oplus = \min$ and additive identity $0 \equiv \infty$
-   $\otimes = +$ and multiplicative identity $1 \equiv 0$
-   Useful for shortest distance: $d = A \otimes d$

::: notes
An example of a semiring is the min-plus semiring (a "tropical"
semiring).  The addition operation here takes a min, and the
multiplication operation is ordinary addition.  Shortest distance
computations can be seen as fixed-point iterations for the equation
d = Ad under this semi-ring.  Other semi-rings can be used to express
many other graph algorithms in matrix-vector language, too.
:::

### Graph BLAS

<http://www.graphblas.org/>

-   Version 1.3.0 (final) as of 2019-09-25
-   (Opaque) internal sparse matrix data structure
-   Allows operations over misc semirings

::: notes
The GraphBLAS interfaces formalize this "graph algorithms as linear
algebra" way of thinking.  There are lots of advantages to this:
one can get different algorithms by playing with associativity and
distributivity, and build on top of high-performance
linar-algebra-style building blocks that have been tuned for different
types of architectures.  For someone like me, who likes to think about
things in terms of linear algebra, this is a very tempting intervace
indeed, particularly since I can take advantage of the work that
friends of mine have done to make this interface fast on various platforms.
:::

### Graph frameworks

Several to choose from!

-   Pregel, Apache Giraph, Stanford GPS, \...
-   GraphLab family
    -   GraphLab: Original distributed memory
    -   PowerGraph: For "natural" (power law) networks
    -   GraphChi: *Chi*huahua -- shared mem vs distributed
-   Outperformed by Galois, Ligra, BlockGRACE, others
-   But\... programming model was easy
-   GraphIt - best of both worlds?

::: notes
Alas, not everyone thinks that linear algebra is the right way to
think about graph algorithms!  Over the past decade or so, a lot of
work has been done on other frameworks for programming graph
algorithms "at scale."  Maybe the earliest such was Google's Pregel
system, followed by the GraphLab family of systems from CMU.  Later
frameworks out-performed these earlier systems.  Indeed, I was involved
in some of this work -- the GRACE system was Cornell's entry into this
world, back when Johannes Gehrke was still on the faculty, and I was
involved in work that added blocked algorithms into GRACE.  But some
of the performance improvements came at the cost of the so-called
"think like a vertex" abstraction that the original designers of
Pregel and GraphLab liked so much.

As a brief aside: there's a project at MIT called GraphIt that
involves a domain-specific language for programming graph algorithms
that have high performance.  It seems like the logical successor to
the Pregel/GraphLab/etc line of work, but it has much higher
performance than those systems did.
:::

### Graph frameworks

-   "Think as a vertex"
    -   Each vertex updates locally
    -   Exchanges messages with neighbors
    -   Runtime actually schedules updates/messages
-   Message sent at super-step $S$ arrives at $S+1$
-   Looks like BSP

::: notes
The programming framework used by Pregel, GraphLab, and so forth
involved phased independent message exchanges between vertices.  In
each phase, a vertex could update locally and then send messages to
neighbors; those messages would arrive in the next "super-step."  So
this looks a lot like a bulk-synchronous programming abstraction with
a parallel loop over the vertices and the messages serving as the
intermediate data structure that carries information from one phase ot
the next.  But I suppose "think like a vertex" is a catchier way of
putting it.
:::

### At what COST?

"Scalability! But at what COST?"\
McSherry, Isard, Murray, HotOS 15

> You can have a second computer once you've shown you know how to use
> the first one.\
> -- Paul Barham (quoted in intro)

-   Configuration that Outperforms a Single Thread
-   Observation: many systems have unbounded COST!

::: notes
One of my very favorite things to come out of some of the craze in the
early 2010s for simple-and-scalable analytics frameworks --- things
like Pregel and MapReduce and the like --- is watching people
eventually realize that there's more to performance than parallelism,
and that often a well-written code on a laptop could do what industry
players were attempting to do with a framework code distributed across
a giant cluster or cloud environment.  McSherry, Isard, and Murray
made the point very nicely in this 2015 paper, where they talked about
Configuration that Outperforms a Single Thread.  The punchline for the
paper was that in many cases, there was no configuration that
outperforms a single thread!  This is not to be too dismissive of the
framework work, which often was pretty good about parallelizing disk
head use (which is not an entirely trivial matter).  But it is good
for perspective.

My personal punchline to all this: if I were trying to do high-performance
combinatorial graph operations for something these days, I would
probably reach for GraphBLAS before reaching for any of the graph
processing engine frameworks.
:::
