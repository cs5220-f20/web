% Load balancing
% David Bindel

### Inefficiencies in parallel code

![](figs/lb-red-serial.svg){width=40%}

Poor single processor performance

-   Typically in the memory system
-   Saw this in matrix multiply assignment

::: notes
Before we talk about load balance, let's talk more generally about how
parallel code might be inefficient.

At the start of the semester, we talked about inefficiencies in
single-core code.  Often this comes because of bad use of caches and
the memory system; or perhaps the code isn't properly vectorized.
We saw a lot of this in the matrix multiply assignment.

The key to good parallel performance is good single-core performance.
:::

### Inefficiencies in parallel code

![](figs/lb-red-comm.svg){width=40%}

Overhead for parallelism

-   Thread creation, synchronization, communication
-   Saw this in shallow water assignment

::: notes
Once we have tuned our serial performance, the time spent on
parallel overheads looks relatively larger.  This includes all types
of parallel overheads: thread creation, synchronization, or explicit
message passing.  We saw this in the shallow water assignment, for
example.
:::

### Inefficiencies in parallel code

![](figs/lb-red-imbalance.svg){width=40%}

Load imbalance

-   Different amounts of work across processors
-   Different speeds / available resources
-   Insufficient parallel work
-   All this can change over phases

::: notes
Once we have tuned both the single core performance and the
communication costs, we still have to worry about assigning
different amounts of work to different processors.  This is what
we call load imbalance.  Maybe we have different amounts of work,
maybe we have different resources at different processors, or maybe
we just have more processors than we have parallelism.

If our computations has phases, all of these things can change from
one phase to the next.
:::

### Where does the time go?

-   Load balance looks like large sync cost
-   \... maybe so does ordinary sync overhead!
-   And spin-locks may make sync look like useful work
-   And ordinary time sharing can confuse things more
-   Can get some help from profiling tools

::: notes
When you look at wallclock time, load imbalance may look like a long
time spent in a barrier or other synchronization construct.  But maybe
so does ordinary synchronization overhead!  Or, depending on the
profiler, synchronization techniques like spin locks may make
synchronization overhead appear like useful work, even though they're
note.  Ordinary time-sharing on a modern operating system can confuse
things even more.  So while we can get some help from timers and
profiling tools, really separating out overheads associated with
synchronization or with OS-based sharing of a processor from those
associated with load imbalance may not be as easy as it seems.
:::

### Many independent tasks

![](figs/lb-task-circles.svg){width=40%}

-   Simplest strategy: partition by task index
    -   What if task costs are inhomogeneous?
    -   Worse: all expensive tasks on one thread?
-   Potential fixes
    -   Many small tasks, randomly assigned
    -   Dynamic task assignment
-   Issue: what about scheduling overhead?

::: notes
Scheduling is easiest when we have independent tasks.  So let's
consider this case first.  A natural approach is to partition
statically, say by index, giving each processor one pth of the
original tasks.  But... if task costs are inhomogeneous, we could end
up with a bad situation where we lump all the expensive tasks on one
thread.

If there are lots of small tasks, we could randomly assign them in the
hopes that things will balance out.  If you remember your law of large
numbers, though, you'll realize that for a fixed distribution of task
costs, the load imbalance relative to the total time cost decays like
the square root of the number of tasks.  So for this to make sense,
either you want fairly homogeneous task costs or you want a large
number of tasks.

We could also dynamically assign tasks in a way that events out the
load across processprs.  But if we're going to do that type of dynamic
task assignment, what about the overhead of scheduling?
:::

### Variations on a theme

How to avoid overhead? Chunks!\
(Think OpenMP loops)

-   Small chunks: good balance, large overhead
-   Large chunks: poor balance, low overhead

::: notes
One of our common tools for defeating overhead is to amortize it by
doing work in big chunks.  This is an option for OpenMP loop
scheduling, for example.  But in the load balancing case, there is a
tension to how we size the chunks of work.  If we have very small
chunks of work, that's probably good for balance, but the overhead of
scheduling may be very high.  In contrast, if we have big chunks of
work, the overhead of scheduling is relatively low, but our load
balance might be worse.
:::

### Variations on a theme

-   Fixed chunk size (requires good cost estimates)
-   Guided self-scheduling (take $\lceil (\mbox{tasks left})/p \rceil$
    work)
-   Tapering (size chunks based on variance)
-   Weighted factoring (GSS with heterogeneity)

::: notes
So how do we resolve the tension between large chunks good for
overhead and small chunks good for load balancing?

The simplest approach is to statically partition the work into fixed
size chunks, but this requires good cost estimates a priori.  The
other extreme, sometimes called self-scheduling, involves querying a
work queue for every new task; this is good for load balance under
uncertain costs, but the overhead is pretty high.

An alternate approach that has some of the advantages of
self-scheduling without quite so much scheduling overhead is called
guided self scheduling (GSS).  In GSS, the scheduler decreases
the chunk size over time: each time a processor requests work, it gets
one pth of the remaining work (or one work item, whichever is larger).

Tapering is based on GSS, but it takes the mean and standard deviation
of the work costs into account.  If the standard deviation is zero,
tapering is just GSS.  Otherwise, tapering takes somewhat smaller
chunks than GSS according to soem complicated formula.

Weighted factoring is a little like GSS, but it can take into
account heterogeneity in processor speeds as well.

These are not the only scheduling protocols out there!  This stuff has
been studied since the 1980s, and there are still new ideas proposed
every year.  But our goal is not to consider only independent tasks,
so let's move on.
:::

### Static dependency

![](figs/part_esep.svg){width=40%}

-   Graph $G = (V,E)$ with vertex and edge weights
-   Goal: even partition, small cut (comm volume)
-   Optimal partitioning is NP complete -- use heuristics
-   Tradeoff quality vs speed
-   Good software exists (e.g. METIS)

::: notes
The next step up in complexity from independent tasks is when we have
tasks with a fixed dependency.  Often, these dependencies can be
captured in terms of a graph, where nodes represent variables that we
have to update and edges are dependencies between them.  We talked in
the last deck about using graph partitioning to cut up these problems
in a way that evenly partitions the variables while minimizing the
edge cut (which corresponds to the communication volume).

Of course, we have to pay attention to how expensive graph
partitioning might be.  Different methods represent different
tradeoffs between speed and quality of the partition.  Fortunately,
there are good software packages for the static partitioning problem,
like METIS and ParMETIS.
:::

### The limits of graph partitioning

What if

-   We don't know task costs?
-   We don't know the comm/dependency pattern?
-   These things change over time?

May want *dynamic* load balancing?

Even in regular case: not every problem looks like an undirected graph!

::: notes
Static graph partitioning has its limits. Graph partitioning mostly
solves load-balancing when we tasks costs and dependency patterns are
fixed and known. But there are problems that have none of these
properties! So what do we do if the task costs are unknown, the
dependency pattern is unknown, and everything is constantly changing?
This calls for a more dynamic approach!
:::

### Dependency graphs

So far: Graphs for dependencies between *unknowns*.

For dependency between tasks or computations:

-   Arrow from $A$ to $B$ means that $B$ depends on $A$
-   Result is a *directed acyclic graph* (DAG)

::: notes
When we talk about static graph partitioning, we're often -- though
not always -- thinking about the graph as representing symmetric
dependencies between unknowns.  In contrast, at the heart of a lot of
dynamic scheduling is the notion of a task graph, a directed acyclic
graph (DAG) that represents how data flows out of one task and into
others.

This sounds a little abstract, so let's walk through a concrete example.
:::

### Longest Common Substring

Goal: Longest sequence of (not necessarily contiguous) characters common
to strings $S$ and $T$.

Recursive formulation: 
$$\begin{aligned}
& \mathrm{LCS}[i,j] = \\
& \begin{cases}
    \max(\mathrm{LCS}[i-1,j], \mathrm{LCS}[j,i-1]), & S[i] \neq T[j] \\
    1 + \mathrm{LCS}[i-1,j-1], & S[i] = T[j]
  \end{cases}
\end{aligned}$$
Dynamic programming: Form a table of $\mathrm{LCS}[i,j]$

::: notes
The longest common substring problem is one of those classic CS
problems that comes up in algorithms classes where people talk about
dynamic programming (though it also comes up in some other situations,
like in genomics studies).  The goal is just to find the longest
subsequence of characters that two strings S and T have in common.

We can write down the length of the longest common subsequence via a
recursion.  Let LCS[i,j] represent the longest common subsequence of
characters 1 through i in string S and 1 through j in string T.
When i or j is zero, the LCS is zero.  That's our base case.
Otherwise, we could have the last characters in the substrings of S
and T be the same, or they could be different.  If they're the same,
the LCS is going to be one longer than the LCS where we leave the ith
character off S and the jth off T.  Otherwise, we take the max of
the LCS where we either leave the ith character off S or the jth off T.

It's OK to stare at this if you need a moment.

The usual dynamic programming approach to solving this problem
involves computing a table of LCS[i,j] for every i and j in the
range from zero to the string lengths.
:::

### Dependency graphs

![](figs/lb-lcs-dependency.svg){width=40%}

Process in any order consistent with dependencies.\
Limits to available parallel work early on or late!

::: notes
Here's a plot of the dependencies in the longest substring recurrence.
Each entry depends on the entries below it, to the left of it, and on
the diagonal to the left and below.  We can process the entries in
this graph in any order consistent with the dependencies, and there
are various ways to do this.

The coloring denotes one order, a sweep starting at the lower left
corner (position 1,1) and moving to the top right corner.  If we tilt
our heads so that the diagonals of constant color run side-to-side,
we might notice that the pattern of arrows is very similar to the one
that we've seen in save problems, and this suggests that we can think
about what's happening in similar ways (e.g. we can think of the
analog of "batching steps", doing some redundant computation in order
to reduce communication between neighboring processors.

But one of the things that is different between this problem and our
wave problems is that the diagonals are not all the same size!
So there isn't much work (or parallelism) available in the early
phases, nor at the very end.
:::

### Dependency graphs

![](figs/lb-lcs-coarsen.svg){width=40%}

Partition into coarser-grain tasks for locality?

::: notes
We can potentially reduce the synchronization overheads and improve
locality of reference by tiling the dynamic programming table, and
handling each tile as a chunk.  Of course, we then have the same issue
we had in the case of independent tasks: how big should the chunks be?
:::

### Dependency graphs

![](figs/lb-lcs-coarse3x3.svg){width=40%}

Dependence between coarse tasks limits parallelism.

::: notes
Having coarse tasks may reduce synchronization overheads, but it also
reduces the available parallelization.  For the version shown in this
picture (corresponding to the coarse blocking in the previous slide),
we can never process more than three blocks concurrently!
:::

### Alternate perspective

Two approaches to LCS:

-   Solve subproblems from bottom up
-   Solve top down, *memoize* common subproblems

Parallel question: shared memoization (and synchronize) or independent
memoization (and redundant computation)?

::: notes
Everything we've discussed so far is from the "bottom up" perspective:
we start with short prefixes of S and T and build up to the full
strings.  But there's another way of approaching this type of dynamic
program, too, sometimes known as "memo-ization," where we use a data
structure to keep track of previously-solved subproblems.  The
memoization perspective gives maybe a slightly different way of
thinking about parallelism for LCS, where we could be more or less
strict about synchronized access to a shared memoization data
structure, depending on whether we think it's more important to reduce
synchronization costs or more important to avoid all redundant
computations.  Of course, we see the same type of tradeoff in the
bottom-up approach.
:::

### Load balancing and task-based parallelism

![](figs/lb-task-dag.svg){width=40%}

-   Task DAG captures data dependencies
-   May be known at outset or dynamically generated
-   Topological sort reveals parallelism opportunities

::: notes
Going back to the general picture: a task graph is a directed acyclic
graph that captures the data dependencies between different tasks in
our computation.  The task graph might be known from the start, as in
our dynamic programming example, or it might be something that we
generate on the fly.  Either way, any DAG admits a "topological sort"
of the nodes: that is, we can always come up with a linear ordering of
the tasks so that all dependencies between tasks are satisfied.  In
fact, there are many such orderings -- if there weren't, we would have
no room for parallelism!  A variant of one of the earliest topological
sort algorithms (Kahn's algorithm -- no relation to the Star Trek
villan!) decomposes a task graph into layers as shown here, where all
the tasks in each layer are independent, and can be computed when the
tasks in each previous layer are done.  Hence, topological sort -- or
at least this layered variant of topological sort -- shows us the
opportunities for parallelism that exist in the computation.
:::

### Basic parameters

-   Task costs
    -   Do all tasks have equal costs?
    -   Known statically, at creation, at completion?
-   Task dependencies
    -   Can tasks be run in any order?
    -   If not, when are dependencies known?
-   Locality
    -   Tasks co-located to reduce communication?
    -   When is this information known?

::: notes
We now have a handle on the main parameters we need to consider when
thinking about dynamic load balancing and parallelism.  Somehow, we
have to start with decomposing our problem into tasks, and look for
parallelism between those tasks.  In the easiest case, the tasks have
equal costs, known statically; but there are certainly problems where
we don't know how much it will cost to execute a task until we start
working on it (or even until we finish working on it!).  We also may
have dependencies between tasks that keep us from executing them in
arbitrary order and with arbitrary parallelism; the fewer the
dependencies and the earlier we know them, the easier the task of
scheduling for parallelism.  Finally, we always look for ways to keep
locality of reference, and in a task-based problem decomposition, that
often means co-locating the execution of tasks that depend on each
other as much as possible.
:::

### Task costs

![Easy: equal unit cost tasks (branch-free loops)](figs/lb-task-eq-circles.svg){width=80%}

![Harder: different, known times (sparse MVM)](figs/lb-task-circles.svg){width=80%}

![Hardest: costs unknown until completed (search)](figs/lb-task-unk-circles.svg){width=80%}

::: notes
Breaking it down a bit more: in terms of task costs, the easiest case
is lots of tasks that cost the same amount.

An example of a harder case might be partitioning the row-times-vector
products in a sparse matrix-vector product.  The cost of handling each
row is proportional to the number of nonzeros in that row; this varies
from row to row, but we know the counts in advance.  If we want to
partition the matrix into sets of rows so that each processor is doing
the same amount of work in a matrix-vector product, we have to take
this heterogeneity into account.  But it's something that we know at
the start, and it doesn't change over time.

The hardest case, which is common in search, is when we don't know how
much time it will take to complete a given task until the task is
actually done!
:::

### Dependencies

![Easy: dependency-free loop (Jacobi sweep)](figs/lb-deps-easy.svg){width=60%}

![Harder: tasks have predictable structure (some DAG)](figs/lb-deps-harder.svg){width=25%}

![Hardest: structure is dynamic (search, sparse LU)](figs/lb-tree-search.svg){width=15%}

::: notes
Similarly, there are easy, harder, and hardest cases for dependencies.

The easiest case is independent tasks.  This is what happens in our
parallel for loops in OpenMP, for example.

A harder case is when tasks have a predictable structure that is known
statically.  This is what happened in our wave propagation codes, for
example, where the "task" in question corresponded to computing a
value for a given mesh point at a given time step.

The hardest case, again, is search.  In this case, we might not know
what depends on what until we've started exploring.  This also happens
in problems like sparse LU factorization.
:::

### Locality/communication

When do you communicate?

-   Easy: Only at start/end (embarrassingly parallel)
-   Harder: In a predictable pattern (PDE solver)
-   Hardest: Unpredictable (discrete event simulation)

::: notes
And then there's the question of when we communicate, which is closely
coupled to the nature of the dependency pattern.  The easy case is
when there are no dependencies except maybe at the start or the end;
this gives us a pleasingly parallel problem.  Harder is the case of a
predictable pattern like we see in PDE solvers.  And in the interest
of not constantly complaining about search, let's give discrete event
simulations as an instance of the hardest case, where you don't
necessarily know when you'll need to send a message.  We talked about
this earlier in the class when we talked about playing the Game of
Life on a dilute board, for example.
:::

### A spectrum of solutions

Depending on cost, dependency, locality:

- Static scheduling
- Semi-static scheduling
- Dynamic scheduling

::: notes
Depending on what we know about the cost of the tasks, how they depend
on each other, and what type of locality or communication minimization
concerns we might have, we might try different strategies for
scheduling our work and communication.  In particular, we can arrange
these strategies along a continuum, with completely static
pre-computed strategies at one extreme and completely dynamic
strategies at the other.
:::

### Static scheduling

-   Everything known in advance
-   Can schedule offline (e.g. graph partitioning)
-   Example: Shallow water solver

::: notes
When we know the whole shape of the computation in advance, we can
plan things out in advance and just execute our plan, with no need
for communication to update the plans as we go.  Often this involves
some form of graph partitioning.  An example of a completely static
schedule is what we did with the shallow water solver (or at least
our version of the solver).  We know exactly
what depends on what, and probably decide in advance how many steps we
should take in a bach.

Of course, you might recall that the stable step size depends on the
water height, so there is some room to do something dynamic here,
advancing some parts of the domain with longer time steps and other
parts with shorter steps.  Load balancing gets trickier if we want to
do something like that, since we would get a bad load imbalance if the
regions with long time steps were the same size as the regions
requiring short time steps.
:::

### Semi-static scheduling

-   Everything known at start of step (for example)
-   Use offline ideas (e.g. Kernighan-Lin refinement)
-   Example: Particle-based methods

::: notes
In other problems, dependencies or task costs might change over time,
but slowly enough that we can create a static plan that is useful over
several steps of the algorithm.  We might then re-compute the plan
from scratch, or we might do something to refine the plan in the face
of changes, like applying a few sweeps of Kernighan-Lin to update a
partition after changing some edges around.  An example where this
comes in handy is in particle simulations where particles interact
with all other particles in some local neighborhood.  As the particles
move around, who they interact with slowly changes.  But these changes
are slow enough relative to the time step size that we can re-use the
same (slightly conservative) interaction graph for scheduling
interaction computations over several consecutive time steps.
:::

### Dynamic scheduling

-   Don't know what we're doing until we've started
-   Have to use online algorithms
-   Example: most search problems

::: notes
In cases where we have less advance knowledge of how things will
unfold, we're forced to schedule dynamically.  As I've said, this is
usually what happens with search problems.
:::

### Search problems

-   Different set of strategies from physics sims!
-   Usually require dynamic load balance
-   Example:
    -   Optimal VLSI layout
    -   Robot motion planning
    -   Game playing
    -   Speech processing
    -   Reconstructing phylogeny
    -   \...

::: notes
I keep complaining about search problems!  So what's the story with
these?  They're pretty different from the types of structured
simulations that we've seen so far.  Examples include finding optimal
layouts for integrated circuits, path planning for robots, game
playing, speech processing, reconstructing phylogenetic trees, and
many more.  For those of you in CS, these likely feel like much more
comfortable problems than wave simulations!  But they're a lot harder
from the perspective of parallel performance.
:::

### Example: Tree search

![](figs/lb-tree-search.svg){width=40%}

-   Tree unfolds dynamically during search
-   Common problems on different paths (graph)?
-   Graph may or may not be explicit in advance

::: notes
A canonical example is tree search, where the tree unfolds dynamically
as part of the search process.  A lot of graph problems end up looking
like this.  Tree traversals sometimes kick up different problems along
different paths in the tree, so it can in principal be effective to
look for opportunities to treat them like DAGs -- but the tree case is
usually easier.
:::

### Search algorithms

Generic search:

- Put root in stack/queue
- while stack/queue has work
  - remove node $n$ from queue
  - if $n$ satisfies goal, return
  - mark $n$ as searched
  - queue viable unsearched children\
    (Can branch-and-bound)

DFS (stack), BFS (queue), A$^*$ (priority queue), \...

::: notes
Let's take a generic graph search as a working example of something
that's building a search tree. We start with a root node of the tree,
which we put into some type of queue data structure.  Then,
while there are nodes in the queue, we remove one node, check whether
it matches our goal (returning if it does), and otherwise mark the
node as returned and queue up any viable unsearched children.

Here I uses "queue" to denote not only a first-in-first-out (FIFO) data
structure, but a more generic collection that we can add to and remove
from.  With a FIFO queue, we've just described breadth-first search;
with a stack, it would be depth-first search; and with a priority
queue, it would be A-star search.
:::

### Simple parallel search

![](figs/lb-static-tree.svg){width=40%}

Static load balancing:

-   Each new task on a proc until all have a subtree
-   Ineffective without work estimates for subtrees!
-   How can we do better?

::: notes
It is, in fact, possible to consider a mostly-static partitioning scheme for
this type of tree search algorithm.  Start unfolding the search tree,
assigning each new node to a different processors until we have used
all the processors; then let each processor handle its subtree.  Of
course, in the case of something like our graph search example, we
might have to do some synchronization around marking already-searched
nodes, but there are a lot of situations where different subtrees
really can be processed completely independently.  The problem is,
unless we know in advance how big each of those subtrees is, this
strategy might assign massively more work to some processors than to
others!  So what do we need to do in order to do better?
:::

### Centralized scheduling

![](figs/lb-centralq.svg){width=50%}

Idea: obvious parallelization of standard search

-   Locks on shared data structure (stack, queue, etc)
-   Or might be a manager task

::: notes
The obvious thing to do is to organize our work around some
centralized queue.  For example, in our graph search picture, we might
try to just use an appropriately-synchronized shared stack or FIFO
queue.  Or we might set up a manager to parcel out work on request.
:::

### Centralized scheduling

Teaser: What could go wrong with this parallel BFS?

- Queue root and fork
  - obtain queue lock
  - while queue has work
    - remove node $n$ from queue
    - release queue lock
    - process $n$, mark as searched
    - obtain queue lock
    - enqueue unsearched children
  - release queue lock
- join

::: notes
Let's think through how we might do this.  It's constructive to do the
most straightforward thing first, and then think about what goes
wrong, rather than jumping immediately to the right answer!

In this code, we start off by putting the root node into the queue,
and then we fork a bunch of worker processes to handle nodes as they
appear in the queue.  We use a lock to ensure that only one worker is
modifying a queue at any given time.

Can you see what goes wrong with this code?  I'll give you a hint: the
code produces a correct result, but is potentially subject to a
massive load imbalance.
:::

### Centralized scheduling

- Put root in queue; **workers active = 0**; fork
  - obtain queue lock
  - while queue has work **or workers active \> 0**
    - remove node $n$ from queue; **workers active ++**
    - release queue lock
    - process $n$, mark as searched
    - obtain queue lock
    - enqueue unsearched children; **workers active --**
  - release queue lock
- join

::: notes
The problem with our code on the previous slide is that if P0 dequeues
the root node at the start, all the other processors might look at the
empty work queue and decide to quit before P0 enqueues the searched
children!  To avoid this problem, we need to know not only when the
queue is empty, but also when there is the *potential* for new work to
be added to the queue.  We can do this by keeping track of the number
of workers that are processing nodes -- if anyone is processing a
node, they might generate new nodes in the not-too-distant future.

If I was implementing this in real code, by the way, I would probably
use a condition variable to signal when the queue had work available.
But explaining that would have made this longer than one slide, too.
:::

### Centralized task queue

-   Called *self-scheduling* when applied to loops
    -   Tasks might be range of loop indices
    -   Assume independent iterations
    -   Loop body has unpredictable time (or do it statically)
-   Pro: dynamic, online scheduling
-   Con: centralized, so doesn't scale
-   Con: high overhead if tasks are small

::: notes
We've already seen this type of centralized queue data structure in
our OpenMP discussion, though we didn't describe it that way at the
time.  In a parallel for loop, tasks correspond to indices (or chunks
of indices), all of which are assumed to be independent.  The
"dynamic" scheduling option (aka self-scheduling) in an OpenMP loop
does this.

This type of centralized task queue is great for load balance, but it
also requires a potentially-expensive synchronized global data
structure.  Imagine we were in a distributed memory environment and
had to send an MPI message every time we wanted to process a point!
This is great for small numbers of processors, but we probably want
something else as we scale up to larger machines.
:::

### Beyond centralized task queue

![](figs/lb-distq.svg){width=80%}

::: notes
Maybe unsurprisingly, the obvious competitor to a centralized task
queue is a decentralized task queue!  The idea here is that each
worker maintains its own local task queue, but there is a mechanism to
transfer work from one worker to another in order to balance load out.
For example, in the diagram shown here, worker 1 might see that it has
an empty work queue, and decide to steal a work item from worker 2
(yoink!).  This is a "work-stealing" arrangement.

It's worth thinking for a moment: in an environment like this, how do
you decide when you're done?
:::

### Beyond centralized task queue

Basic *distributed* task queue idea:

-   Each processor works on part of a tree
-   When done, get work from a peer
-   *Or* if busy, push work to a peer
-   Asynch communication useful

Also goes by work stealing, work crews\...

::: notes
In words, here's the distributed task queue idea.  Each processor
works on its queue of tasks; if the queue empties, it might steal work
from a peer, or it might push work to a peer if the queue gets too
full.  Either way, this is a place where one-sided communication is
probably useful, though not totally critical.
:::

### Picking a donor

Could use:

-   Asynchronous round-robin
-   Global round-robin (current donor ptr at P0)
-   Randomized -- optimal with high probability!

::: notes
Let's consider the work-stealing variant.  My queue is empty, and I
want to take work from a donor.  How do I decide who to go to?

One approach would be for me to ask each of the processors in turn.
The problem with this is that if I use the same ordering as everyone
else, we'll all end up swamping the first few nodes with requests for
work (and probably running them completely dry so that they then have
to beg).  This is not great for spreading around the wealth.

A second approach, much more equitable in how it steals work, would
involve a global round-robin ordering.  But to do a global ordering of
work stealing, we would need to keep a synchronized pointer to the
next in the list, maybe at processor 0.  That pointer is now a source
of contention and communication overhead.

It turns out that a somewhat stupid-sounding strategy is nearly as
good as global round robin, but involves no communication.  This
strategy is to choose a donor at random.
:::

### Diffusion-based balancing

-   Problem with random polling: communication cost!
    -   But not all connections are equal
    -   Idea: prefer to poll more local neighbors
-   Average out load with neighbors $\implies$ diffusion!

::: notes
Randomly targeting who we beg for work doesn't require any
sophisticated communication around who to ask.  But we do have to
communicate to make the ask, and not all communication messages are
equal!  In an MPI job with multiple ranks per node, messages that are
within the node are probably going to be much faster than inter-node
messages.  So maybe instead of choosing a donor uniformly at random,
we should choose a donor according to a distribution that favors
asking nearby nodes first!  This is sometimes called diffusion-based
load balancing.  This type of diffusion-based load balancing doesn't
"spread out" developing load imbalances as fast as uniform donor
selection does, but it does spread the load eventually, and with less
expensive inter-node communication.
:::

### Mixed parallelism

-   Today: mostly coarse-grain *task* parallelism
-   Other times: fine-grain *data* parallelism
-   Why not do both? *Switched* parallelism.

::: notes
You may feel a bit uneasy that the type of task-based parallelism
we've discussed today feels very different from the type of
data-parallel strategies that we've discussed previously.  But there's
no need to just one or the other.  A big part of this class involves
aggregating our computation into chunks of work that are big enough to
amortize the cost of synchronization and communication, but small
enough to play nicely with caches and provide some opportunities for
parallelism and load balance.  Switched parallelism is really just an
instance of this approach: do fine-grained data parallelism at one
level, where that data parallelism lives inside of a coarse-grained task.
:::

### Takeaway

-   Lots of ideas, not one size fits all!
-   Axes: task size, task dependence, communication
-   Dynamic tree search is a particularly hard case!
-   Fundamental tradeoffs
    -   Overdecompose (load balance) vs\
        keep tasks big (overhead, locality)
    -   Steal work globally (balance) vs\
        steal from neighbors (comm.Â overhead)
-   Sometimes hard to know when code should stop!

::: notes
Wrapping up for today: there is no uber-algorithm for finding
parallelism and balancing load across processors.  While it's often
useful to think about these problems in terms of tasks and their
interdependencies, the exact nature of the task costs and dependencies
(and when we find out about those costs and dependencies) has a huge
impact on what's most appropriate.  The right solution in any given
situation represents a particular answer to how we should make some
fundamental tradeoffs.  For example, should we overdecompose a
project, getting lots of small tasks that are easier to spread across
processors in a balanced way?  Or should we keep the tasks big in
order to amortize scheduling overhead and potentially improve locality
of reference?  And if we're in distributed memory environments, should
we do work stealing with donors chosen uniformly at random, which is
better for evening out load quickly in very imbalanced settings?  Or
should we use a diffusion scheme that favors nearby donors, which may
not spread out load as quickly but will tend to do less expensive
communication when we stay around an equilibrium?

As a final theme that has come up a few times: for as cool as some of
the more dynamic load balancing mechanisms are, they often make it
really hard to decide when a program should be done.

Fortunately, I have no such difficulty with deciding when to end this
slide deck!  Until next time...
:::
